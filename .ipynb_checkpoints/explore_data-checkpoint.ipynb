{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "# fetch and load training data\n",
    "trainset = datasets.KMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # fetch and load test data\n",
    "testset = datasets.KMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this means we have 64 images of 28x28 pixels in grayscale (1 channel)\n",
    "\n",
    "example_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.view(example_data.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape\n",
    "x = example_data\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(x.shape[0], -1)#flatten image into 784x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spirals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.50000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.31380</td>\n",
       "      <td>1.25590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.31380</td>\n",
       "      <td>-1.25590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.88973</td>\n",
       "      <td>2.43961</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-5.88973</td>\n",
       "      <td>-2.43961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.24865</td>\n",
       "      <td>3.50704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-5.24865</td>\n",
       "      <td>-3.50704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.41941</td>\n",
       "      <td>4.41943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-4.41941</td>\n",
       "      <td>-4.41943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_coord   y_coord  class\n",
       "0  6.50000   0.00000      0\n",
       "1 -6.50000  -0.00000      1\n",
       "2  6.31380   1.25590      0\n",
       "3 -6.31380  -1.25590      1\n",
       "4  5.88973   2.43961      0\n",
       "5 -5.88973  -2.43961      1\n",
       "6  5.24865   3.50704      0\n",
       "7 -5.24865  -3.50704      1\n",
       "8  4.41941   4.41943      0\n",
       "9 -4.41941  -4.41943      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(df.values,dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-6.5000e+00, -0.0000e+00,  1.0000e+00],\n",
       "        [ 6.3138e+00,  1.2559e+00,  0.0000e+00],\n",
       "        [-6.3138e+00, -1.2559e+00,  1.0000e+00],\n",
       "        [ 5.8897e+00,  2.4396e+00,  0.0000e+00],\n",
       "        [-5.8897e+00, -2.4396e+00,  1.0000e+00],\n",
       "        [ 5.2487e+00,  3.5070e+00,  0.0000e+00],\n",
       "        [-5.2487e+00, -3.5070e+00,  1.0000e+00],\n",
       "        [ 4.4194e+00,  4.4194e+00,  0.0000e+00],\n",
       "        [-4.4194e+00, -4.4194e+00,  1.0000e+00],\n",
       "        [ 3.4376e+00,  5.1447e+00,  0.0000e+00],\n",
       "        [-3.4376e+00, -5.1447e+00,  1.0000e+00],\n",
       "        [ 2.3439e+00,  5.6588e+00,  0.0000e+00],\n",
       "        [-2.3439e+00, -5.6588e+00,  1.0000e+00],\n",
       "        [ 1.1827e+00,  5.9460e+00,  0.0000e+00],\n",
       "        [-1.1827e+00, -5.9460e+00,  1.0000e+00],\n",
       "        [-2.0000e-05,  6.0000e+00,  0.0000e+00],\n",
       "        [ 2.0000e-05, -6.0000e+00,  1.0000e+00],\n",
       "        [-1.1584e+00,  5.8234e+00,  0.0000e+00],\n",
       "        [ 1.1584e+00, -5.8234e+00,  1.0000e+00],\n",
       "        [-2.2483e+00,  5.4278e+00,  0.0000e+00],\n",
       "        [ 2.2483e+00, -5.4278e+00,  1.0000e+00],\n",
       "        [-3.2293e+00,  4.8329e+00,  0.0000e+00],\n",
       "        [ 3.2293e+00, -4.8329e+00,  1.0000e+00],\n",
       "        [-4.0659e+00,  4.0658e+00,  0.0000e+00],\n",
       "        [ 4.0659e+00, -4.0658e+00,  1.0000e+00],\n",
       "        [-4.7290e+00,  3.1598e+00,  0.0000e+00],\n",
       "        [ 4.7290e+00, -3.1598e+00,  1.0000e+00],\n",
       "        [-5.1968e+00,  2.1526e+00,  0.0000e+00],\n",
       "        [ 5.1968e+00, -2.1526e+00,  1.0000e+00],\n",
       "        [-5.4556e+00,  1.0852e+00,  0.0000e+00],\n",
       "        [ 5.4556e+00, -1.0852e+00,  1.0000e+00],\n",
       "        [-5.5000e+00, -4.0000e-05,  0.0000e+00],\n",
       "        [ 5.5000e+00,  4.0000e-05,  1.0000e+00],\n",
       "        [-5.3330e+00, -1.0609e+00,  0.0000e+00],\n",
       "        [ 5.3330e+00,  1.0609e+00,  1.0000e+00],\n",
       "        [-4.9658e+00, -2.0570e+00,  0.0000e+00],\n",
       "        [ 4.9658e+00,  2.0570e+00,  1.0000e+00],\n",
       "        [-4.4172e+00, -2.9515e+00,  0.0000e+00],\n",
       "        [ 4.4172e+00,  2.9515e+00,  1.0000e+00],\n",
       "        [-3.7123e+00, -3.7123e+00,  0.0000e+00],\n",
       "        [ 3.7123e+00,  3.7123e+00,  1.0000e+00],\n",
       "        [-2.8820e+00, -4.3133e+00,  0.0000e+00],\n",
       "        [ 2.8820e+00,  4.3133e+00,  1.0000e+00],\n",
       "        [-1.9612e+00, -4.7349e+00,  0.0000e+00],\n",
       "        [ 1.9612e+00,  4.7349e+00,  1.0000e+00],\n",
       "        [-9.8759e-01, -4.9652e+00,  0.0000e+00],\n",
       "        [ 9.8759e-01,  4.9652e+00,  1.0000e+00],\n",
       "        [ 6.0000e-05, -5.0000e+00,  0.0000e+00],\n",
       "        [-6.0000e-05,  5.0000e+00,  1.0000e+00],\n",
       "        [ 9.6331e-01, -4.8426e+00,  0.0000e+00],\n",
       "        [-9.6331e-01,  4.8426e+00,  1.0000e+00],\n",
       "        [ 1.8656e+00, -4.5039e+00,  0.0000e+00],\n",
       "        [-1.8656e+00,  4.5039e+00,  1.0000e+00],\n",
       "        [ 2.6737e+00, -4.0014e+00,  0.0000e+00],\n",
       "        [-2.6737e+00,  4.0014e+00,  1.0000e+00],\n",
       "        [ 3.3588e+00, -3.3587e+00,  0.0000e+00],\n",
       "        [-3.3588e+00,  3.3587e+00,  1.0000e+00],\n",
       "        [ 3.8976e+00, -2.6042e+00,  0.0000e+00],\n",
       "        [-3.8976e+00,  2.6042e+00,  1.0000e+00],\n",
       "        [ 4.2730e+00, -1.7699e+00,  0.0000e+00],\n",
       "        [-4.2730e+00,  1.7699e+00,  1.0000e+00],\n",
       "        [ 4.4749e+00, -8.9004e-01,  0.0000e+00],\n",
       "        [-4.4749e+00,  8.9004e-01,  1.0000e+00],\n",
       "        [ 4.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-4.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 4.3522e+00,  8.6578e-01,  0.0000e+00],\n",
       "        [-4.3522e+00, -8.6578e-01,  1.0000e+00],\n",
       "        [ 4.0420e+00,  1.6743e+00,  0.0000e+00],\n",
       "        [-4.0420e+00, -1.6743e+00,  1.0000e+00],\n",
       "        [ 3.5857e+00,  2.3960e+00,  0.0000e+00],\n",
       "        [-3.5857e+00, -2.3960e+00,  1.0000e+00],\n",
       "        [ 3.0052e+00,  3.0052e+00,  0.0000e+00],\n",
       "        [-3.0052e+00, -3.0052e+00,  1.0000e+00],\n",
       "        [ 2.3264e+00,  3.4818e+00,  0.0000e+00],\n",
       "        [-2.3264e+00, -3.4818e+00,  1.0000e+00],\n",
       "        [ 1.5785e+00,  3.8110e+00,  0.0000e+00],\n",
       "        [-1.5785e+00, -3.8110e+00,  1.0000e+00],\n",
       "        [ 7.9248e-01,  3.9845e+00,  0.0000e+00],\n",
       "        [-7.9248e-01, -3.9845e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  4.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -4.0000e+00,  1.0000e+00],\n",
       "        [-7.6824e-01,  3.8618e+00,  0.0000e+00],\n",
       "        [ 7.6824e-01, -3.8618e+00,  1.0000e+00],\n",
       "        [-1.4830e+00,  3.5800e+00,  0.0000e+00],\n",
       "        [ 1.4830e+00, -3.5800e+00,  1.0000e+00],\n",
       "        [-2.1182e+00,  3.1699e+00,  0.0000e+00],\n",
       "        [ 2.1182e+00, -3.1699e+00,  1.0000e+00],\n",
       "        [-2.6517e+00,  2.6516e+00,  0.0000e+00],\n",
       "        [ 2.6517e+00, -2.6516e+00,  1.0000e+00],\n",
       "        [-3.0661e+00,  2.0486e+00,  0.0000e+00],\n",
       "        [ 3.0661e+00, -2.0486e+00,  1.0000e+00],\n",
       "        [-3.3491e+00,  1.3872e+00,  0.0000e+00],\n",
       "        [ 3.3491e+00, -1.3872e+00,  1.0000e+00],\n",
       "        [-3.4941e+00,  6.9493e-01,  0.0000e+00],\n",
       "        [ 3.4941e+00, -6.9493e-01,  1.0000e+00],\n",
       "        [-3.5000e+00, -8.0000e-05,  0.0000e+00],\n",
       "        [ 3.5000e+00,  8.0000e-05,  1.0000e+00],\n",
       "        [-3.3714e+00, -6.7070e-01,  0.0000e+00],\n",
       "        [ 3.3714e+00,  6.7070e-01,  1.0000e+00],\n",
       "        [-3.1181e+00, -1.2916e+00,  0.0000e+00],\n",
       "        [ 3.1181e+00,  1.2916e+00,  1.0000e+00],\n",
       "        [-2.7542e+00, -1.8404e+00,  0.0000e+00],\n",
       "        [ 2.7542e+00,  1.8404e+00,  1.0000e+00],\n",
       "        [-2.2980e+00, -2.2982e+00,  0.0000e+00],\n",
       "        [ 2.2980e+00,  2.2982e+00,  1.0000e+00],\n",
       "        [-1.7708e+00, -2.6504e+00,  0.0000e+00],\n",
       "        [ 1.7708e+00,  2.6504e+00,  1.0000e+00],\n",
       "        [-1.1958e+00, -2.8872e+00,  0.0000e+00],\n",
       "        [ 1.1958e+00,  2.8872e+00,  1.0000e+00],\n",
       "        [-5.9739e-01, -3.0037e+00,  0.0000e+00],\n",
       "        [ 5.9739e-01,  3.0037e+00,  1.0000e+00],\n",
       "        [ 8.0000e-05, -3.0000e+00,  0.0000e+00],\n",
       "        [-8.0000e-05,  3.0000e+00,  1.0000e+00],\n",
       "        [ 5.7315e-01, -2.8810e+00,  0.0000e+00],\n",
       "        [-5.7315e-01,  2.8810e+00,  1.0000e+00],\n",
       "        [ 1.1003e+00, -2.6561e+00,  0.0000e+00],\n",
       "        [-1.1003e+00,  2.6561e+00,  1.0000e+00],\n",
       "        [ 1.5626e+00, -2.3385e+00,  0.0000e+00],\n",
       "        [-1.5626e+00,  2.3385e+00,  1.0000e+00],\n",
       "        [ 1.9446e+00, -1.9445e+00,  0.0000e+00],\n",
       "        [-1.9446e+00,  1.9445e+00,  1.0000e+00],\n",
       "        [ 2.2346e+00, -1.4930e+00,  0.0000e+00],\n",
       "        [-2.2346e+00,  1.4930e+00,  1.0000e+00],\n",
       "        [ 2.4252e+00, -1.0045e+00,  0.0000e+00],\n",
       "        [-2.4252e+00,  1.0045e+00,  1.0000e+00],\n",
       "        [ 2.5133e+00, -4.9985e-01,  0.0000e+00],\n",
       "        [-2.5133e+00,  4.9985e-01,  1.0000e+00],\n",
       "        [ 2.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-2.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 2.3907e+00,  4.7560e-01,  0.0000e+00],\n",
       "        [-2.3907e+00, -4.7560e-01,  1.0000e+00],\n",
       "        [ 2.1942e+00,  9.0894e-01,  0.0000e+00],\n",
       "        [-2.1942e+00, -9.0894e-01,  1.0000e+00],\n",
       "        [ 1.9227e+00,  1.2848e+00,  0.0000e+00],\n",
       "        [-1.9227e+00, -1.2848e+00,  1.0000e+00],\n",
       "        [ 1.5909e+00,  1.5910e+00,  0.0000e+00],\n",
       "        [-1.5909e+00, -1.5910e+00,  1.0000e+00],\n",
       "        [ 1.2153e+00,  1.8189e+00,  0.0000e+00],\n",
       "        [-1.2153e+00, -1.8189e+00,  1.0000e+00],\n",
       "        [ 8.1314e-01,  1.9633e+00,  0.0000e+00],\n",
       "        [-8.1314e-01, -1.9633e+00,  1.0000e+00],\n",
       "        [ 4.0231e-01,  2.0229e+00,  0.0000e+00],\n",
       "        [-4.0231e-01, -2.0229e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  2.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -2.0000e+00,  1.0000e+00],\n",
       "        [-3.7805e-01,  1.9003e+00,  0.0000e+00],\n",
       "        [ 3.7805e-01, -1.9003e+00,  1.0000e+00],\n",
       "        [-7.1759e-01,  1.7322e+00,  0.0000e+00],\n",
       "        [ 7.1759e-01, -1.7322e+00,  1.0000e+00],\n",
       "        [-1.0070e+00,  1.5070e+00,  0.0000e+00],\n",
       "        [ 1.0070e+00, -1.5070e+00,  1.0000e+00],\n",
       "        [-1.2375e+00,  1.2374e+00,  0.0000e+00],\n",
       "        [ 1.2375e+00, -1.2374e+00,  1.0000e+00],\n",
       "        [-1.4031e+00,  9.3748e-01,  0.0000e+00],\n",
       "        [ 1.4031e+00, -9.3748e-01,  1.0000e+00],\n",
       "        [-1.5013e+00,  6.2181e-01,  0.0000e+00],\n",
       "        [ 1.5013e+00, -6.2181e-01,  1.0000e+00],\n",
       "        [-1.5325e+00,  3.0477e-01,  0.0000e+00],\n",
       "        [ 1.5325e+00, -3.0477e-01,  1.0000e+00],\n",
       "        [-1.5000e+00, -6.0000e-05,  0.0000e+00],\n",
       "        [ 1.5000e+00,  6.0000e-05,  1.0000e+00],\n",
       "        [-1.4099e+00, -2.8049e-01,  0.0000e+00],\n",
       "        [ 1.4099e+00,  2.8049e-01,  1.0000e+00],\n",
       "        [-1.2703e+00, -5.2624e-01,  0.0000e+00],\n",
       "        [ 1.2703e+00,  5.2624e-01,  1.0000e+00],\n",
       "        [-1.0913e+00, -7.2923e-01,  0.0000e+00],\n",
       "        [ 1.0913e+00,  7.2923e-01,  1.0000e+00],\n",
       "        [-8.8385e-01, -8.8392e-01,  0.0000e+00],\n",
       "        [ 8.8385e-01,  8.8392e-01,  1.0000e+00],\n",
       "        [-6.5970e-01, -9.8740e-01,  0.0000e+00],\n",
       "        [ 6.5970e-01,  9.8740e-01,  1.0000e+00],\n",
       "        [-4.3048e-01, -1.0394e+00,  0.0000e+00],\n",
       "        [ 4.3048e-01,  1.0394e+00,  1.0000e+00],\n",
       "        [-2.0724e-01, -1.0421e+00,  0.0000e+00],\n",
       "        [ 2.0724e-01,  1.0421e+00,  1.0000e+00],\n",
       "        [ 4.0000e-05, -1.0000e+00,  0.0000e+00],\n",
       "        [-4.0000e-05,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.8293e-01, -9.1948e-01,  0.0000e+00],\n",
       "        [-1.8293e-01,  9.1948e-01,  1.0000e+00],\n",
       "        [ 3.3488e-01, -8.0838e-01,  0.0000e+00],\n",
       "        [-3.3488e-01,  8.0838e-01,  1.0000e+00],\n",
       "        [ 4.5143e-01, -6.7555e-01,  0.0000e+00],\n",
       "        [-4.5143e-01,  6.7555e-01,  1.0000e+00],\n",
       "        [ 5.3035e-01, -5.3031e-01,  0.0000e+00],\n",
       "        [-5.3035e-01,  5.3031e-01,  1.0000e+00],\n",
       "        [ 5.7165e-01, -3.8193e-01,  0.0000e+00],\n",
       "        [-5.7165e-01,  3.8193e-01,  1.0000e+00],\n",
       "        [ 5.7744e-01, -2.3915e-01,  0.0000e+00],\n",
       "        [-5.7744e-01,  2.3915e-01,  1.0000e+00],\n",
       "        [ 5.5170e-01, -1.0971e-01,  0.0000e+00],\n",
       "        [-5.5170e-01,  1.0971e-01,  1.0000e+00],\n",
       "        [ 5.0000e-01,  2.0000e-05,  0.0000e+00],\n",
       "        [-5.0000e-01, -2.0000e-05,  1.0000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data #transform all data into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_input # number of features without the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data\n",
    "full_input  = data[:,0:num_input]\n",
    "full_target = data[:,num_input:num_input+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000e+00,  0.0000e+00],\n",
       "        [-6.5000e+00, -0.0000e+00],\n",
       "        [ 6.3138e+00,  1.2559e+00],\n",
       "        [-6.3138e+00, -1.2559e+00],\n",
       "        [ 5.8897e+00,  2.4396e+00],\n",
       "        [-5.8897e+00, -2.4396e+00],\n",
       "        [ 5.2487e+00,  3.5070e+00],\n",
       "        [-5.2487e+00, -3.5070e+00],\n",
       "        [ 4.4194e+00,  4.4194e+00],\n",
       "        [-4.4194e+00, -4.4194e+00],\n",
       "        [ 3.4376e+00,  5.1447e+00],\n",
       "        [-3.4376e+00, -5.1447e+00],\n",
       "        [ 2.3439e+00,  5.6588e+00],\n",
       "        [-2.3439e+00, -5.6588e+00],\n",
       "        [ 1.1827e+00,  5.9460e+00],\n",
       "        [-1.1827e+00, -5.9460e+00],\n",
       "        [-2.0000e-05,  6.0000e+00],\n",
       "        [ 2.0000e-05, -6.0000e+00],\n",
       "        [-1.1584e+00,  5.8234e+00],\n",
       "        [ 1.1584e+00, -5.8234e+00],\n",
       "        [-2.2483e+00,  5.4278e+00],\n",
       "        [ 2.2483e+00, -5.4278e+00],\n",
       "        [-3.2293e+00,  4.8329e+00],\n",
       "        [ 3.2293e+00, -4.8329e+00],\n",
       "        [-4.0659e+00,  4.0658e+00],\n",
       "        [ 4.0659e+00, -4.0658e+00],\n",
       "        [-4.7290e+00,  3.1598e+00],\n",
       "        [ 4.7290e+00, -3.1598e+00],\n",
       "        [-5.1968e+00,  2.1526e+00],\n",
       "        [ 5.1968e+00, -2.1526e+00],\n",
       "        [-5.4556e+00,  1.0852e+00],\n",
       "        [ 5.4556e+00, -1.0852e+00],\n",
       "        [-5.5000e+00, -4.0000e-05],\n",
       "        [ 5.5000e+00,  4.0000e-05],\n",
       "        [-5.3330e+00, -1.0609e+00],\n",
       "        [ 5.3330e+00,  1.0609e+00],\n",
       "        [-4.9658e+00, -2.0570e+00],\n",
       "        [ 4.9658e+00,  2.0570e+00],\n",
       "        [-4.4172e+00, -2.9515e+00],\n",
       "        [ 4.4172e+00,  2.9515e+00],\n",
       "        [-3.7123e+00, -3.7123e+00],\n",
       "        [ 3.7123e+00,  3.7123e+00],\n",
       "        [-2.8820e+00, -4.3133e+00],\n",
       "        [ 2.8820e+00,  4.3133e+00],\n",
       "        [-1.9612e+00, -4.7349e+00],\n",
       "        [ 1.9612e+00,  4.7349e+00],\n",
       "        [-9.8759e-01, -4.9652e+00],\n",
       "        [ 9.8759e-01,  4.9652e+00],\n",
       "        [ 6.0000e-05, -5.0000e+00],\n",
       "        [-6.0000e-05,  5.0000e+00],\n",
       "        [ 9.6331e-01, -4.8426e+00],\n",
       "        [-9.6331e-01,  4.8426e+00],\n",
       "        [ 1.8656e+00, -4.5039e+00],\n",
       "        [-1.8656e+00,  4.5039e+00],\n",
       "        [ 2.6737e+00, -4.0014e+00],\n",
       "        [-2.6737e+00,  4.0014e+00],\n",
       "        [ 3.3588e+00, -3.3587e+00],\n",
       "        [-3.3588e+00,  3.3587e+00],\n",
       "        [ 3.8976e+00, -2.6042e+00],\n",
       "        [-3.8976e+00,  2.6042e+00],\n",
       "        [ 4.2730e+00, -1.7699e+00],\n",
       "        [-4.2730e+00,  1.7699e+00],\n",
       "        [ 4.4749e+00, -8.9004e-01],\n",
       "        [-4.4749e+00,  8.9004e-01],\n",
       "        [ 4.5000e+00,  7.0000e-05],\n",
       "        [-4.5000e+00, -7.0000e-05],\n",
       "        [ 4.3522e+00,  8.6578e-01],\n",
       "        [-4.3522e+00, -8.6578e-01],\n",
       "        [ 4.0420e+00,  1.6743e+00],\n",
       "        [-4.0420e+00, -1.6743e+00],\n",
       "        [ 3.5857e+00,  2.3960e+00],\n",
       "        [-3.5857e+00, -2.3960e+00],\n",
       "        [ 3.0052e+00,  3.0052e+00],\n",
       "        [-3.0052e+00, -3.0052e+00],\n",
       "        [ 2.3264e+00,  3.4818e+00],\n",
       "        [-2.3264e+00, -3.4818e+00],\n",
       "        [ 1.5785e+00,  3.8110e+00],\n",
       "        [-1.5785e+00, -3.8110e+00],\n",
       "        [ 7.9248e-01,  3.9845e+00],\n",
       "        [-7.9248e-01, -3.9845e+00],\n",
       "        [-7.0000e-05,  4.0000e+00],\n",
       "        [ 7.0000e-05, -4.0000e+00],\n",
       "        [-7.6824e-01,  3.8618e+00],\n",
       "        [ 7.6824e-01, -3.8618e+00],\n",
       "        [-1.4830e+00,  3.5800e+00],\n",
       "        [ 1.4830e+00, -3.5800e+00],\n",
       "        [-2.1182e+00,  3.1699e+00],\n",
       "        [ 2.1182e+00, -3.1699e+00],\n",
       "        [-2.6517e+00,  2.6516e+00],\n",
       "        [ 2.6517e+00, -2.6516e+00],\n",
       "        [-3.0661e+00,  2.0486e+00],\n",
       "        [ 3.0661e+00, -2.0486e+00],\n",
       "        [-3.3491e+00,  1.3872e+00],\n",
       "        [ 3.3491e+00, -1.3872e+00],\n",
       "        [-3.4941e+00,  6.9493e-01],\n",
       "        [ 3.4941e+00, -6.9493e-01],\n",
       "        [-3.5000e+00, -8.0000e-05],\n",
       "        [ 3.5000e+00,  8.0000e-05],\n",
       "        [-3.3714e+00, -6.7070e-01],\n",
       "        [ 3.3714e+00,  6.7070e-01],\n",
       "        [-3.1181e+00, -1.2916e+00],\n",
       "        [ 3.1181e+00,  1.2916e+00],\n",
       "        [-2.7542e+00, -1.8404e+00],\n",
       "        [ 2.7542e+00,  1.8404e+00],\n",
       "        [-2.2980e+00, -2.2982e+00],\n",
       "        [ 2.2980e+00,  2.2982e+00],\n",
       "        [-1.7708e+00, -2.6504e+00],\n",
       "        [ 1.7708e+00,  2.6504e+00],\n",
       "        [-1.1958e+00, -2.8872e+00],\n",
       "        [ 1.1958e+00,  2.8872e+00],\n",
       "        [-5.9739e-01, -3.0037e+00],\n",
       "        [ 5.9739e-01,  3.0037e+00],\n",
       "        [ 8.0000e-05, -3.0000e+00],\n",
       "        [-8.0000e-05,  3.0000e+00],\n",
       "        [ 5.7315e-01, -2.8810e+00],\n",
       "        [-5.7315e-01,  2.8810e+00],\n",
       "        [ 1.1003e+00, -2.6561e+00],\n",
       "        [-1.1003e+00,  2.6561e+00],\n",
       "        [ 1.5626e+00, -2.3385e+00],\n",
       "        [-1.5626e+00,  2.3385e+00],\n",
       "        [ 1.9446e+00, -1.9445e+00],\n",
       "        [-1.9446e+00,  1.9445e+00],\n",
       "        [ 2.2346e+00, -1.4930e+00],\n",
       "        [-2.2346e+00,  1.4930e+00],\n",
       "        [ 2.4252e+00, -1.0045e+00],\n",
       "        [-2.4252e+00,  1.0045e+00],\n",
       "        [ 2.5133e+00, -4.9985e-01],\n",
       "        [-2.5133e+00,  4.9985e-01],\n",
       "        [ 2.5000e+00,  7.0000e-05],\n",
       "        [-2.5000e+00, -7.0000e-05],\n",
       "        [ 2.3907e+00,  4.7560e-01],\n",
       "        [-2.3907e+00, -4.7560e-01],\n",
       "        [ 2.1942e+00,  9.0894e-01],\n",
       "        [-2.1942e+00, -9.0894e-01],\n",
       "        [ 1.9227e+00,  1.2848e+00],\n",
       "        [-1.9227e+00, -1.2848e+00],\n",
       "        [ 1.5909e+00,  1.5910e+00],\n",
       "        [-1.5909e+00, -1.5910e+00],\n",
       "        [ 1.2153e+00,  1.8189e+00],\n",
       "        [-1.2153e+00, -1.8189e+00],\n",
       "        [ 8.1314e-01,  1.9633e+00],\n",
       "        [-8.1314e-01, -1.9633e+00],\n",
       "        [ 4.0231e-01,  2.0229e+00],\n",
       "        [-4.0231e-01, -2.0229e+00],\n",
       "        [-7.0000e-05,  2.0000e+00],\n",
       "        [ 7.0000e-05, -2.0000e+00],\n",
       "        [-3.7805e-01,  1.9003e+00],\n",
       "        [ 3.7805e-01, -1.9003e+00],\n",
       "        [-7.1759e-01,  1.7322e+00],\n",
       "        [ 7.1759e-01, -1.7322e+00],\n",
       "        [-1.0070e+00,  1.5070e+00],\n",
       "        [ 1.0070e+00, -1.5070e+00],\n",
       "        [-1.2375e+00,  1.2374e+00],\n",
       "        [ 1.2375e+00, -1.2374e+00],\n",
       "        [-1.4031e+00,  9.3748e-01],\n",
       "        [ 1.4031e+00, -9.3748e-01],\n",
       "        [-1.5013e+00,  6.2181e-01],\n",
       "        [ 1.5013e+00, -6.2181e-01],\n",
       "        [-1.5325e+00,  3.0477e-01],\n",
       "        [ 1.5325e+00, -3.0477e-01],\n",
       "        [-1.5000e+00, -6.0000e-05],\n",
       "        [ 1.5000e+00,  6.0000e-05],\n",
       "        [-1.4099e+00, -2.8049e-01],\n",
       "        [ 1.4099e+00,  2.8049e-01],\n",
       "        [-1.2703e+00, -5.2624e-01],\n",
       "        [ 1.2703e+00,  5.2624e-01],\n",
       "        [-1.0913e+00, -7.2923e-01],\n",
       "        [ 1.0913e+00,  7.2923e-01],\n",
       "        [-8.8385e-01, -8.8392e-01],\n",
       "        [ 8.8385e-01,  8.8392e-01],\n",
       "        [-6.5970e-01, -9.8740e-01],\n",
       "        [ 6.5970e-01,  9.8740e-01],\n",
       "        [-4.3048e-01, -1.0394e+00],\n",
       "        [ 4.3048e-01,  1.0394e+00],\n",
       "        [-2.0724e-01, -1.0421e+00],\n",
       "        [ 2.0724e-01,  1.0421e+00],\n",
       "        [ 4.0000e-05, -1.0000e+00],\n",
       "        [-4.0000e-05,  1.0000e+00],\n",
       "        [ 1.8293e-01, -9.1948e-01],\n",
       "        [-1.8293e-01,  9.1948e-01],\n",
       "        [ 3.3488e-01, -8.0838e-01],\n",
       "        [-3.3488e-01,  8.0838e-01],\n",
       "        [ 4.5143e-01, -6.7555e-01],\n",
       "        [-4.5143e-01,  6.7555e-01],\n",
       "        [ 5.3035e-01, -5.3031e-01],\n",
       "        [-5.3035e-01,  5.3031e-01],\n",
       "        [ 5.7165e-01, -3.8193e-01],\n",
       "        [-5.7165e-01,  3.8193e-01],\n",
       "        [ 5.7744e-01, -2.3915e-01],\n",
       "        [-5.7744e-01,  2.3915e-01],\n",
       "        [ 5.5170e-01, -1.0971e-01],\n",
       "        [-5.5170e-01,  1.0971e-01],\n",
       "        [ 5.0000e-01,  2.0000e-05],\n",
       "        [-5.0000e-01, -2.0000e-05]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_input #this will be fed in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_target #this will be used for the output loss calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(full_input,full_target)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset,batch_size=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x17b2e91e0b8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset #transforming to speed training in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x17b2e91e080>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 6.5000e+00,  0.0000e+00],\n",
      "        [-6.5000e+00, -0.0000e+00],\n",
      "        [ 6.3138e+00,  1.2559e+00],\n",
      "        [-6.3138e+00, -1.2559e+00],\n",
      "        [ 5.8897e+00,  2.4396e+00],\n",
      "        [-5.8897e+00, -2.4396e+00],\n",
      "        [ 5.2487e+00,  3.5070e+00],\n",
      "        [-5.2487e+00, -3.5070e+00],\n",
      "        [ 4.4194e+00,  4.4194e+00],\n",
      "        [-4.4194e+00, -4.4194e+00],\n",
      "        [ 3.4376e+00,  5.1447e+00],\n",
      "        [-3.4376e+00, -5.1447e+00],\n",
      "        [ 2.3439e+00,  5.6588e+00],\n",
      "        [-2.3439e+00, -5.6588e+00],\n",
      "        [ 1.1827e+00,  5.9460e+00],\n",
      "        [-1.1827e+00, -5.9460e+00],\n",
      "        [-2.0000e-05,  6.0000e+00],\n",
      "        [ 2.0000e-05, -6.0000e+00],\n",
      "        [-1.1584e+00,  5.8234e+00],\n",
      "        [ 1.1584e+00, -5.8234e+00],\n",
      "        [-2.2483e+00,  5.4278e+00],\n",
      "        [ 2.2483e+00, -5.4278e+00],\n",
      "        [-3.2293e+00,  4.8329e+00],\n",
      "        [ 3.2293e+00, -4.8329e+00],\n",
      "        [-4.0659e+00,  4.0658e+00],\n",
      "        [ 4.0659e+00, -4.0658e+00],\n",
      "        [-4.7290e+00,  3.1598e+00],\n",
      "        [ 4.7290e+00, -3.1598e+00],\n",
      "        [-5.1968e+00,  2.1526e+00],\n",
      "        [ 5.1968e+00, -2.1526e+00],\n",
      "        [-5.4556e+00,  1.0852e+00],\n",
      "        [ 5.4556e+00, -1.0852e+00],\n",
      "        [-5.5000e+00, -4.0000e-05],\n",
      "        [ 5.5000e+00,  4.0000e-05],\n",
      "        [-5.3330e+00, -1.0609e+00],\n",
      "        [ 5.3330e+00,  1.0609e+00],\n",
      "        [-4.9658e+00, -2.0570e+00],\n",
      "        [ 4.9658e+00,  2.0570e+00],\n",
      "        [-4.4172e+00, -2.9515e+00],\n",
      "        [ 4.4172e+00,  2.9515e+00],\n",
      "        [-3.7123e+00, -3.7123e+00],\n",
      "        [ 3.7123e+00,  3.7123e+00],\n",
      "        [-2.8820e+00, -4.3133e+00],\n",
      "        [ 2.8820e+00,  4.3133e+00],\n",
      "        [-1.9612e+00, -4.7349e+00],\n",
      "        [ 1.9612e+00,  4.7349e+00],\n",
      "        [-9.8759e-01, -4.9652e+00],\n",
      "        [ 9.8759e-01,  4.9652e+00],\n",
      "        [ 6.0000e-05, -5.0000e+00],\n",
      "        [-6.0000e-05,  5.0000e+00],\n",
      "        [ 9.6331e-01, -4.8426e+00],\n",
      "        [-9.6331e-01,  4.8426e+00],\n",
      "        [ 1.8656e+00, -4.5039e+00],\n",
      "        [-1.8656e+00,  4.5039e+00],\n",
      "        [ 2.6737e+00, -4.0014e+00],\n",
      "        [-2.6737e+00,  4.0014e+00],\n",
      "        [ 3.3588e+00, -3.3587e+00],\n",
      "        [-3.3588e+00,  3.3587e+00],\n",
      "        [ 3.8976e+00, -2.6042e+00],\n",
      "        [-3.8976e+00,  2.6042e+00],\n",
      "        [ 4.2730e+00, -1.7699e+00],\n",
      "        [-4.2730e+00,  1.7699e+00],\n",
      "        [ 4.4749e+00, -8.9004e-01],\n",
      "        [-4.4749e+00,  8.9004e-01],\n",
      "        [ 4.5000e+00,  7.0000e-05],\n",
      "        [-4.5000e+00, -7.0000e-05],\n",
      "        [ 4.3522e+00,  8.6578e-01],\n",
      "        [-4.3522e+00, -8.6578e-01],\n",
      "        [ 4.0420e+00,  1.6743e+00],\n",
      "        [-4.0420e+00, -1.6743e+00],\n",
      "        [ 3.5857e+00,  2.3960e+00],\n",
      "        [-3.5857e+00, -2.3960e+00],\n",
      "        [ 3.0052e+00,  3.0052e+00],\n",
      "        [-3.0052e+00, -3.0052e+00],\n",
      "        [ 2.3264e+00,  3.4818e+00],\n",
      "        [-2.3264e+00, -3.4818e+00],\n",
      "        [ 1.5785e+00,  3.8110e+00],\n",
      "        [-1.5785e+00, -3.8110e+00],\n",
      "        [ 7.9248e-01,  3.9845e+00],\n",
      "        [-7.9248e-01, -3.9845e+00],\n",
      "        [-7.0000e-05,  4.0000e+00],\n",
      "        [ 7.0000e-05, -4.0000e+00],\n",
      "        [-7.6824e-01,  3.8618e+00],\n",
      "        [ 7.6824e-01, -3.8618e+00],\n",
      "        [-1.4830e+00,  3.5800e+00],\n",
      "        [ 1.4830e+00, -3.5800e+00],\n",
      "        [-2.1182e+00,  3.1699e+00],\n",
      "        [ 2.1182e+00, -3.1699e+00],\n",
      "        [-2.6517e+00,  2.6516e+00],\n",
      "        [ 2.6517e+00, -2.6516e+00],\n",
      "        [-3.0661e+00,  2.0486e+00],\n",
      "        [ 3.0661e+00, -2.0486e+00],\n",
      "        [-3.3491e+00,  1.3872e+00],\n",
      "        [ 3.3491e+00, -1.3872e+00],\n",
      "        [-3.4941e+00,  6.9493e-01],\n",
      "        [ 3.4941e+00, -6.9493e-01],\n",
      "        [-3.5000e+00, -8.0000e-05]])\n"
     ]
    }
   ],
   "source": [
    "for _, (data_feed,target) in enumerate(train_loader):\n",
    "    print('x:',data_feed)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.5000, 0.0000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.5000e+00, -6.5000e+00,  6.3138e+00, -6.3138e+00,  5.8897e+00,\n",
       "        -5.8897e+00,  5.2487e+00, -5.2487e+00,  4.4194e+00, -4.4194e+00,\n",
       "         3.4376e+00, -3.4376e+00,  2.3439e+00, -2.3439e+00,  1.1827e+00,\n",
       "        -1.1827e+00, -2.0000e-05,  2.0000e-05, -1.1584e+00,  1.1584e+00,\n",
       "        -2.2483e+00,  2.2483e+00, -3.2293e+00,  3.2293e+00, -4.0659e+00,\n",
       "         4.0659e+00, -4.7290e+00,  4.7290e+00, -5.1968e+00,  5.1968e+00,\n",
       "        -5.4556e+00,  5.4556e+00, -5.5000e+00,  5.5000e+00, -5.3330e+00,\n",
       "         5.3330e+00, -4.9658e+00,  4.9658e+00, -4.4172e+00,  4.4172e+00,\n",
       "        -3.7123e+00,  3.7123e+00, -2.8820e+00,  2.8820e+00, -1.9612e+00,\n",
       "         1.9612e+00, -9.8759e-01,  9.8759e-01,  6.0000e-05, -6.0000e-05,\n",
       "         9.6331e-01, -9.6331e-01,  1.8656e+00, -1.8656e+00,  2.6737e+00,\n",
       "        -2.6737e+00,  3.3588e+00, -3.3588e+00,  3.8976e+00, -3.8976e+00,\n",
       "         4.2730e+00, -4.2730e+00,  4.4749e+00, -4.4749e+00,  4.5000e+00,\n",
       "        -4.5000e+00,  4.3522e+00, -4.3522e+00,  4.0420e+00, -4.0420e+00,\n",
       "         3.5857e+00, -3.5857e+00,  3.0052e+00, -3.0052e+00,  2.3264e+00,\n",
       "        -2.3264e+00,  1.5785e+00, -1.5785e+00,  7.9248e-01, -7.9248e-01,\n",
       "        -7.0000e-05,  7.0000e-05, -7.6824e-01,  7.6824e-01, -1.4830e+00,\n",
       "         1.4830e+00, -2.1182e+00,  2.1182e+00, -2.6517e+00,  2.6517e+00,\n",
       "        -3.0661e+00,  3.0661e+00, -3.3491e+00,  3.3491e+00, -3.4941e+00,\n",
       "         3.4941e+00, -3.5000e+00])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_feed[:,0] # x vector\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00, -0.0000e+00,  1.2559e+00, -1.2559e+00,  2.4396e+00,\n",
       "        -2.4396e+00,  3.5070e+00, -3.5070e+00,  4.4194e+00, -4.4194e+00,\n",
       "         5.1447e+00, -5.1447e+00,  5.6588e+00, -5.6588e+00,  5.9460e+00,\n",
       "        -5.9460e+00,  6.0000e+00, -6.0000e+00,  5.8234e+00, -5.8234e+00,\n",
       "         5.4278e+00, -5.4278e+00,  4.8329e+00, -4.8329e+00,  4.0658e+00,\n",
       "        -4.0658e+00,  3.1598e+00, -3.1598e+00,  2.1526e+00, -2.1526e+00,\n",
       "         1.0852e+00, -1.0852e+00, -4.0000e-05,  4.0000e-05, -1.0609e+00,\n",
       "         1.0609e+00, -2.0570e+00,  2.0570e+00, -2.9515e+00,  2.9515e+00,\n",
       "        -3.7123e+00,  3.7123e+00, -4.3133e+00,  4.3133e+00, -4.7349e+00,\n",
       "         4.7349e+00, -4.9652e+00,  4.9652e+00, -5.0000e+00,  5.0000e+00,\n",
       "        -4.8426e+00,  4.8426e+00, -4.5039e+00,  4.5039e+00, -4.0014e+00,\n",
       "         4.0014e+00, -3.3587e+00,  3.3587e+00, -2.6042e+00,  2.6042e+00,\n",
       "        -1.7699e+00,  1.7699e+00, -8.9004e-01,  8.9004e-01,  7.0000e-05,\n",
       "        -7.0000e-05,  8.6578e-01, -8.6578e-01,  1.6743e+00, -1.6743e+00,\n",
       "         2.3960e+00, -2.3960e+00,  3.0052e+00, -3.0052e+00,  3.4818e+00,\n",
       "        -3.4818e+00,  3.8110e+00, -3.8110e+00,  3.9845e+00, -3.9845e+00,\n",
       "         4.0000e+00, -4.0000e+00,  3.8618e+00, -3.8618e+00,  3.5800e+00,\n",
       "        -3.5800e+00,  3.1699e+00, -3.1699e+00,  2.6516e+00, -2.6516e+00,\n",
       "         2.0486e+00, -2.0486e+00,  1.3872e+00, -1.3872e+00,  6.9493e-01,\n",
       "        -6.9493e-01, -8.0000e-05])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data_feed[:,1] # y vector \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.atan2(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.sqrt(x*x + y*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.zeros(data_feed.shape,dtype=torch.float32)\n",
    "out[:,0] = a\n",
    "out[:,1] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  6.5000e+00],\n",
       "        [-3.1416e+00,  6.5000e+00],\n",
       "        [ 1.9635e-01,  6.4375e+00],\n",
       "        [-2.9452e+00,  6.4375e+00],\n",
       "        [ 3.9270e-01,  6.3750e+00],\n",
       "        [-2.7489e+00,  6.3750e+00],\n",
       "        [ 5.8905e-01,  6.3125e+00],\n",
       "        [-2.5525e+00,  6.3125e+00],\n",
       "        [ 7.8540e-01,  6.2500e+00],\n",
       "        [-2.3562e+00,  6.2500e+00],\n",
       "        [ 9.8175e-01,  6.1875e+00],\n",
       "        [-2.1598e+00,  6.1875e+00],\n",
       "        [ 1.1781e+00,  6.1250e+00],\n",
       "        [-1.9635e+00,  6.1250e+00],\n",
       "        [ 1.3744e+00,  6.0625e+00],\n",
       "        [-1.7671e+00,  6.0625e+00],\n",
       "        [ 1.5708e+00,  6.0000e+00],\n",
       "        [-1.5708e+00,  6.0000e+00],\n",
       "        [ 1.7671e+00,  5.9375e+00],\n",
       "        [-1.3744e+00,  5.9375e+00],\n",
       "        [ 1.9635e+00,  5.8750e+00],\n",
       "        [-1.1781e+00,  5.8750e+00],\n",
       "        [ 2.1599e+00,  5.8125e+00],\n",
       "        [-9.8174e-01,  5.8125e+00],\n",
       "        [ 2.3562e+00,  5.7500e+00],\n",
       "        [-7.8539e-01,  5.7500e+00],\n",
       "        [ 2.5525e+00,  5.6875e+00],\n",
       "        [-5.8904e-01,  5.6875e+00],\n",
       "        [ 2.7489e+00,  5.6250e+00],\n",
       "        [-3.9269e-01,  5.6250e+00],\n",
       "        [ 2.9453e+00,  5.5625e+00],\n",
       "        [-1.9634e-01,  5.5625e+00],\n",
       "        [-3.1416e+00,  5.5000e+00],\n",
       "        [ 7.2727e-06,  5.5000e+00],\n",
       "        [-2.9452e+00,  5.4375e+00],\n",
       "        [ 1.9636e-01,  5.4375e+00],\n",
       "        [-2.7489e+00,  5.3750e+00],\n",
       "        [ 3.9271e-01,  5.3750e+00],\n",
       "        [-2.5525e+00,  5.3125e+00],\n",
       "        [ 5.8906e-01,  5.3125e+00],\n",
       "        [-2.3562e+00,  5.2500e+00],\n",
       "        [ 7.8541e-01,  5.2500e+00],\n",
       "        [-2.1598e+00,  5.1875e+00],\n",
       "        [ 9.8176e-01,  5.1875e+00],\n",
       "        [-1.9635e+00,  5.1250e+00],\n",
       "        [ 1.1781e+00,  5.1250e+00],\n",
       "        [-1.7671e+00,  5.0625e+00],\n",
       "        [ 1.3745e+00,  5.0625e+00],\n",
       "        [-1.5708e+00,  5.0000e+00],\n",
       "        [ 1.5708e+00,  5.0000e+00],\n",
       "        [-1.3744e+00,  4.9375e+00],\n",
       "        [ 1.7672e+00,  4.9375e+00],\n",
       "        [-1.1781e+00,  4.8750e+00],\n",
       "        [ 1.9635e+00,  4.8750e+00],\n",
       "        [-9.8174e-01,  4.8125e+00],\n",
       "        [ 2.1599e+00,  4.8125e+00],\n",
       "        [-7.8538e-01,  4.7500e+00],\n",
       "        [ 2.3562e+00,  4.7500e+00],\n",
       "        [-5.8903e-01,  4.6875e+00],\n",
       "        [ 2.5526e+00,  4.6875e+00],\n",
       "        [-3.9268e-01,  4.6250e+00],\n",
       "        [ 2.7489e+00,  4.6250e+00],\n",
       "        [-1.9634e-01,  4.5625e+00],\n",
       "        [ 2.9453e+00,  4.5625e+00],\n",
       "        [ 1.5556e-05,  4.5000e+00],\n",
       "        [-3.1416e+00,  4.5000e+00],\n",
       "        [ 1.9636e-01,  4.4375e+00],\n",
       "        [-2.9452e+00,  4.4375e+00],\n",
       "        [ 3.9271e-01,  4.3750e+00],\n",
       "        [-2.7489e+00,  4.3750e+00],\n",
       "        [ 5.8906e-01,  4.3125e+00],\n",
       "        [-2.5525e+00,  4.3125e+00],\n",
       "        [ 7.8541e-01,  4.2500e+00],\n",
       "        [-2.3562e+00,  4.2500e+00],\n",
       "        [ 9.8177e-01,  4.1875e+00],\n",
       "        [-2.1598e+00,  4.1875e+00],\n",
       "        [ 1.1781e+00,  4.1250e+00],\n",
       "        [-1.9635e+00,  4.1250e+00],\n",
       "        [ 1.3745e+00,  4.0625e+00],\n",
       "        [-1.7671e+00,  4.0625e+00],\n",
       "        [ 1.5708e+00,  4.0000e+00],\n",
       "        [-1.5708e+00,  4.0000e+00],\n",
       "        [ 1.7672e+00,  3.9375e+00],\n",
       "        [-1.3744e+00,  3.9375e+00],\n",
       "        [ 1.9635e+00,  3.8750e+00],\n",
       "        [-1.1781e+00,  3.8750e+00],\n",
       "        [ 2.1599e+00,  3.8125e+00],\n",
       "        [-9.8173e-01,  3.8125e+00],\n",
       "        [ 2.3562e+00,  3.7500e+00],\n",
       "        [-7.8538e-01,  3.7500e+00],\n",
       "        [ 2.5526e+00,  3.6875e+00],\n",
       "        [-5.8903e-01,  3.6875e+00],\n",
       "        [ 2.7489e+00,  3.6250e+00],\n",
       "        [-3.9268e-01,  3.6250e+00],\n",
       "        [ 2.9453e+00,  3.5625e+00],\n",
       "        [-1.9633e-01,  3.5625e+00],\n",
       "        [-3.1416e+00,  3.5000e+00]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = torch.arange(start=-7,end=7.1,step=0.01,dtype=torch.float32)\n",
    "yrange = torch.arange(start=-6.6,end=6.7,step=0.01,dtype=torch.float32)\n",
    "xcoord = xrange.repeat(yrange.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0000, -6.9900, -6.9800,  ...,  7.0700,  7.0800,  7.0900])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.6000, -6.5900, -6.5800,  ...,  6.6700,  6.6800,  6.6900])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1330])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrange.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0000],\n",
       "        [-6.9900],\n",
       "        [-6.9800],\n",
       "        ...,\n",
       "        [ 7.0700],\n",
       "        [ 7.0800],\n",
       "        [ 7.0900]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcoord.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcoord = xrange.repeat(yrange.size()[0])\n",
    "ycoord = torch.repeat_interleave(yrange, xrange.size()[0], dim=0)\n",
    "grid = torch.cat((xcoord.unsqueeze(1),ycoord.unsqueeze(1)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0000, -6.9900, -6.9800,  ...,  7.0700,  7.0800,  7.0900])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.6000, -6.6000, -6.6000,  ...,  6.6900,  6.6900,  6.6900])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ycoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0000, -6.6000],\n",
       "        [-6.9900, -6.6000],\n",
       "        [-6.9800, -6.6000],\n",
       "        ...,\n",
       "        [ 7.0700,  6.6900],\n",
       "        [ 7.0800,  6.6900],\n",
       "        [ 7.0900,  6.6900]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hidden layer\n",
    "#first define the class\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RawNet(torch.nn.Module):\n",
    "    def __init__(self, num_hid):\n",
    "        super(RawNet, self).__init__()\n",
    "        self.feed = nn.Linear(2,num_hid)\n",
    "        self.hid = nn.Linear(num_hid,num_hid)\n",
    "        self.out = nn.Linear(num_hid,1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.active1 = torch.tanh(self.feed(x))\n",
    "        self.active2 = torch.tanh(self.hid(self.active1))\n",
    "        self.hidlayer = [self.active1,self.active2]\n",
    "        output = torch.sigmoid(self.out(self.active2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, optimizer):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for _, (data,target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()    # zero the gradients\n",
    "        output = net(data)       # apply network\n",
    "        loss = F.binary_cross_entropy(output,target)\n",
    "        loss.backward()          # compute gradients\n",
    "        optimizer.step()         # update weights\n",
    "        pred = (output >= 0.5).float()\n",
    "        correct += (pred == target).float().sum()\n",
    "        total += target.size()[0]\n",
    "        accuracy = 100*correct/total\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('ep:%5d loss: %6.4f acc: %5.2f' %\n",
    "             (epoch,loss.item(),accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-6.5000e+00, -0.0000e+00,  1.0000e+00],\n",
       "        [ 6.3138e+00,  1.2559e+00,  0.0000e+00],\n",
       "        [-6.3138e+00, -1.2559e+00,  1.0000e+00],\n",
       "        [ 5.8897e+00,  2.4396e+00,  0.0000e+00],\n",
       "        [-5.8897e+00, -2.4396e+00,  1.0000e+00],\n",
       "        [ 5.2487e+00,  3.5070e+00,  0.0000e+00],\n",
       "        [-5.2487e+00, -3.5070e+00,  1.0000e+00],\n",
       "        [ 4.4194e+00,  4.4194e+00,  0.0000e+00],\n",
       "        [-4.4194e+00, -4.4194e+00,  1.0000e+00],\n",
       "        [ 3.4376e+00,  5.1447e+00,  0.0000e+00],\n",
       "        [-3.4376e+00, -5.1447e+00,  1.0000e+00],\n",
       "        [ 2.3439e+00,  5.6588e+00,  0.0000e+00],\n",
       "        [-2.3439e+00, -5.6588e+00,  1.0000e+00],\n",
       "        [ 1.1827e+00,  5.9460e+00,  0.0000e+00],\n",
       "        [-1.1827e+00, -5.9460e+00,  1.0000e+00],\n",
       "        [-2.0000e-05,  6.0000e+00,  0.0000e+00],\n",
       "        [ 2.0000e-05, -6.0000e+00,  1.0000e+00],\n",
       "        [-1.1584e+00,  5.8234e+00,  0.0000e+00],\n",
       "        [ 1.1584e+00, -5.8234e+00,  1.0000e+00],\n",
       "        [-2.2483e+00,  5.4278e+00,  0.0000e+00],\n",
       "        [ 2.2483e+00, -5.4278e+00,  1.0000e+00],\n",
       "        [-3.2293e+00,  4.8329e+00,  0.0000e+00],\n",
       "        [ 3.2293e+00, -4.8329e+00,  1.0000e+00],\n",
       "        [-4.0659e+00,  4.0658e+00,  0.0000e+00],\n",
       "        [ 4.0659e+00, -4.0658e+00,  1.0000e+00],\n",
       "        [-4.7290e+00,  3.1598e+00,  0.0000e+00],\n",
       "        [ 4.7290e+00, -3.1598e+00,  1.0000e+00],\n",
       "        [-5.1968e+00,  2.1526e+00,  0.0000e+00],\n",
       "        [ 5.1968e+00, -2.1526e+00,  1.0000e+00],\n",
       "        [-5.4556e+00,  1.0852e+00,  0.0000e+00],\n",
       "        [ 5.4556e+00, -1.0852e+00,  1.0000e+00],\n",
       "        [-5.5000e+00, -4.0000e-05,  0.0000e+00],\n",
       "        [ 5.5000e+00,  4.0000e-05,  1.0000e+00],\n",
       "        [-5.3330e+00, -1.0609e+00,  0.0000e+00],\n",
       "        [ 5.3330e+00,  1.0609e+00,  1.0000e+00],\n",
       "        [-4.9658e+00, -2.0570e+00,  0.0000e+00],\n",
       "        [ 4.9658e+00,  2.0570e+00,  1.0000e+00],\n",
       "        [-4.4172e+00, -2.9515e+00,  0.0000e+00],\n",
       "        [ 4.4172e+00,  2.9515e+00,  1.0000e+00],\n",
       "        [-3.7123e+00, -3.7123e+00,  0.0000e+00],\n",
       "        [ 3.7123e+00,  3.7123e+00,  1.0000e+00],\n",
       "        [-2.8820e+00, -4.3133e+00,  0.0000e+00],\n",
       "        [ 2.8820e+00,  4.3133e+00,  1.0000e+00],\n",
       "        [-1.9612e+00, -4.7349e+00,  0.0000e+00],\n",
       "        [ 1.9612e+00,  4.7349e+00,  1.0000e+00],\n",
       "        [-9.8759e-01, -4.9652e+00,  0.0000e+00],\n",
       "        [ 9.8759e-01,  4.9652e+00,  1.0000e+00],\n",
       "        [ 6.0000e-05, -5.0000e+00,  0.0000e+00],\n",
       "        [-6.0000e-05,  5.0000e+00,  1.0000e+00],\n",
       "        [ 9.6331e-01, -4.8426e+00,  0.0000e+00],\n",
       "        [-9.6331e-01,  4.8426e+00,  1.0000e+00],\n",
       "        [ 1.8656e+00, -4.5039e+00,  0.0000e+00],\n",
       "        [-1.8656e+00,  4.5039e+00,  1.0000e+00],\n",
       "        [ 2.6737e+00, -4.0014e+00,  0.0000e+00],\n",
       "        [-2.6737e+00,  4.0014e+00,  1.0000e+00],\n",
       "        [ 3.3588e+00, -3.3587e+00,  0.0000e+00],\n",
       "        [-3.3588e+00,  3.3587e+00,  1.0000e+00],\n",
       "        [ 3.8976e+00, -2.6042e+00,  0.0000e+00],\n",
       "        [-3.8976e+00,  2.6042e+00,  1.0000e+00],\n",
       "        [ 4.2730e+00, -1.7699e+00,  0.0000e+00],\n",
       "        [-4.2730e+00,  1.7699e+00,  1.0000e+00],\n",
       "        [ 4.4749e+00, -8.9004e-01,  0.0000e+00],\n",
       "        [-4.4749e+00,  8.9004e-01,  1.0000e+00],\n",
       "        [ 4.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-4.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 4.3522e+00,  8.6578e-01,  0.0000e+00],\n",
       "        [-4.3522e+00, -8.6578e-01,  1.0000e+00],\n",
       "        [ 4.0420e+00,  1.6743e+00,  0.0000e+00],\n",
       "        [-4.0420e+00, -1.6743e+00,  1.0000e+00],\n",
       "        [ 3.5857e+00,  2.3960e+00,  0.0000e+00],\n",
       "        [-3.5857e+00, -2.3960e+00,  1.0000e+00],\n",
       "        [ 3.0052e+00,  3.0052e+00,  0.0000e+00],\n",
       "        [-3.0052e+00, -3.0052e+00,  1.0000e+00],\n",
       "        [ 2.3264e+00,  3.4818e+00,  0.0000e+00],\n",
       "        [-2.3264e+00, -3.4818e+00,  1.0000e+00],\n",
       "        [ 1.5785e+00,  3.8110e+00,  0.0000e+00],\n",
       "        [-1.5785e+00, -3.8110e+00,  1.0000e+00],\n",
       "        [ 7.9248e-01,  3.9845e+00,  0.0000e+00],\n",
       "        [-7.9248e-01, -3.9845e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  4.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -4.0000e+00,  1.0000e+00],\n",
       "        [-7.6824e-01,  3.8618e+00,  0.0000e+00],\n",
       "        [ 7.6824e-01, -3.8618e+00,  1.0000e+00],\n",
       "        [-1.4830e+00,  3.5800e+00,  0.0000e+00],\n",
       "        [ 1.4830e+00, -3.5800e+00,  1.0000e+00],\n",
       "        [-2.1182e+00,  3.1699e+00,  0.0000e+00],\n",
       "        [ 2.1182e+00, -3.1699e+00,  1.0000e+00],\n",
       "        [-2.6517e+00,  2.6516e+00,  0.0000e+00],\n",
       "        [ 2.6517e+00, -2.6516e+00,  1.0000e+00],\n",
       "        [-3.0661e+00,  2.0486e+00,  0.0000e+00],\n",
       "        [ 3.0661e+00, -2.0486e+00,  1.0000e+00],\n",
       "        [-3.3491e+00,  1.3872e+00,  0.0000e+00],\n",
       "        [ 3.3491e+00, -1.3872e+00,  1.0000e+00],\n",
       "        [-3.4941e+00,  6.9493e-01,  0.0000e+00],\n",
       "        [ 3.4941e+00, -6.9493e-01,  1.0000e+00],\n",
       "        [-3.5000e+00, -8.0000e-05,  0.0000e+00],\n",
       "        [ 3.5000e+00,  8.0000e-05,  1.0000e+00],\n",
       "        [-3.3714e+00, -6.7070e-01,  0.0000e+00],\n",
       "        [ 3.3714e+00,  6.7070e-01,  1.0000e+00],\n",
       "        [-3.1181e+00, -1.2916e+00,  0.0000e+00],\n",
       "        [ 3.1181e+00,  1.2916e+00,  1.0000e+00],\n",
       "        [-2.7542e+00, -1.8404e+00,  0.0000e+00],\n",
       "        [ 2.7542e+00,  1.8404e+00,  1.0000e+00],\n",
       "        [-2.2980e+00, -2.2982e+00,  0.0000e+00],\n",
       "        [ 2.2980e+00,  2.2982e+00,  1.0000e+00],\n",
       "        [-1.7708e+00, -2.6504e+00,  0.0000e+00],\n",
       "        [ 1.7708e+00,  2.6504e+00,  1.0000e+00],\n",
       "        [-1.1958e+00, -2.8872e+00,  0.0000e+00],\n",
       "        [ 1.1958e+00,  2.8872e+00,  1.0000e+00],\n",
       "        [-5.9739e-01, -3.0037e+00,  0.0000e+00],\n",
       "        [ 5.9739e-01,  3.0037e+00,  1.0000e+00],\n",
       "        [ 8.0000e-05, -3.0000e+00,  0.0000e+00],\n",
       "        [-8.0000e-05,  3.0000e+00,  1.0000e+00],\n",
       "        [ 5.7315e-01, -2.8810e+00,  0.0000e+00],\n",
       "        [-5.7315e-01,  2.8810e+00,  1.0000e+00],\n",
       "        [ 1.1003e+00, -2.6561e+00,  0.0000e+00],\n",
       "        [-1.1003e+00,  2.6561e+00,  1.0000e+00],\n",
       "        [ 1.5626e+00, -2.3385e+00,  0.0000e+00],\n",
       "        [-1.5626e+00,  2.3385e+00,  1.0000e+00],\n",
       "        [ 1.9446e+00, -1.9445e+00,  0.0000e+00],\n",
       "        [-1.9446e+00,  1.9445e+00,  1.0000e+00],\n",
       "        [ 2.2346e+00, -1.4930e+00,  0.0000e+00],\n",
       "        [-2.2346e+00,  1.4930e+00,  1.0000e+00],\n",
       "        [ 2.4252e+00, -1.0045e+00,  0.0000e+00],\n",
       "        [-2.4252e+00,  1.0045e+00,  1.0000e+00],\n",
       "        [ 2.5133e+00, -4.9985e-01,  0.0000e+00],\n",
       "        [-2.5133e+00,  4.9985e-01,  1.0000e+00],\n",
       "        [ 2.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-2.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 2.3907e+00,  4.7560e-01,  0.0000e+00],\n",
       "        [-2.3907e+00, -4.7560e-01,  1.0000e+00],\n",
       "        [ 2.1942e+00,  9.0894e-01,  0.0000e+00],\n",
       "        [-2.1942e+00, -9.0894e-01,  1.0000e+00],\n",
       "        [ 1.9227e+00,  1.2848e+00,  0.0000e+00],\n",
       "        [-1.9227e+00, -1.2848e+00,  1.0000e+00],\n",
       "        [ 1.5909e+00,  1.5910e+00,  0.0000e+00],\n",
       "        [-1.5909e+00, -1.5910e+00,  1.0000e+00],\n",
       "        [ 1.2153e+00,  1.8189e+00,  0.0000e+00],\n",
       "        [-1.2153e+00, -1.8189e+00,  1.0000e+00],\n",
       "        [ 8.1314e-01,  1.9633e+00,  0.0000e+00],\n",
       "        [-8.1314e-01, -1.9633e+00,  1.0000e+00],\n",
       "        [ 4.0231e-01,  2.0229e+00,  0.0000e+00],\n",
       "        [-4.0231e-01, -2.0229e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  2.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -2.0000e+00,  1.0000e+00],\n",
       "        [-3.7805e-01,  1.9003e+00,  0.0000e+00],\n",
       "        [ 3.7805e-01, -1.9003e+00,  1.0000e+00],\n",
       "        [-7.1759e-01,  1.7322e+00,  0.0000e+00],\n",
       "        [ 7.1759e-01, -1.7322e+00,  1.0000e+00],\n",
       "        [-1.0070e+00,  1.5070e+00,  0.0000e+00],\n",
       "        [ 1.0070e+00, -1.5070e+00,  1.0000e+00],\n",
       "        [-1.2375e+00,  1.2374e+00,  0.0000e+00],\n",
       "        [ 1.2375e+00, -1.2374e+00,  1.0000e+00],\n",
       "        [-1.4031e+00,  9.3748e-01,  0.0000e+00],\n",
       "        [ 1.4031e+00, -9.3748e-01,  1.0000e+00],\n",
       "        [-1.5013e+00,  6.2181e-01,  0.0000e+00],\n",
       "        [ 1.5013e+00, -6.2181e-01,  1.0000e+00],\n",
       "        [-1.5325e+00,  3.0477e-01,  0.0000e+00],\n",
       "        [ 1.5325e+00, -3.0477e-01,  1.0000e+00],\n",
       "        [-1.5000e+00, -6.0000e-05,  0.0000e+00],\n",
       "        [ 1.5000e+00,  6.0000e-05,  1.0000e+00],\n",
       "        [-1.4099e+00, -2.8049e-01,  0.0000e+00],\n",
       "        [ 1.4099e+00,  2.8049e-01,  1.0000e+00],\n",
       "        [-1.2703e+00, -5.2624e-01,  0.0000e+00],\n",
       "        [ 1.2703e+00,  5.2624e-01,  1.0000e+00],\n",
       "        [-1.0913e+00, -7.2923e-01,  0.0000e+00],\n",
       "        [ 1.0913e+00,  7.2923e-01,  1.0000e+00],\n",
       "        [-8.8385e-01, -8.8392e-01,  0.0000e+00],\n",
       "        [ 8.8385e-01,  8.8392e-01,  1.0000e+00],\n",
       "        [-6.5970e-01, -9.8740e-01,  0.0000e+00],\n",
       "        [ 6.5970e-01,  9.8740e-01,  1.0000e+00],\n",
       "        [-4.3048e-01, -1.0394e+00,  0.0000e+00],\n",
       "        [ 4.3048e-01,  1.0394e+00,  1.0000e+00],\n",
       "        [-2.0724e-01, -1.0421e+00,  0.0000e+00],\n",
       "        [ 2.0724e-01,  1.0421e+00,  1.0000e+00],\n",
       "        [ 4.0000e-05, -1.0000e+00,  0.0000e+00],\n",
       "        [-4.0000e-05,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.8293e-01, -9.1948e-01,  0.0000e+00],\n",
       "        [-1.8293e-01,  9.1948e-01,  1.0000e+00],\n",
       "        [ 3.3488e-01, -8.0838e-01,  0.0000e+00],\n",
       "        [-3.3488e-01,  8.0838e-01,  1.0000e+00],\n",
       "        [ 4.5143e-01, -6.7555e-01,  0.0000e+00],\n",
       "        [-4.5143e-01,  6.7555e-01,  1.0000e+00],\n",
       "        [ 5.3035e-01, -5.3031e-01,  0.0000e+00],\n",
       "        [-5.3035e-01,  5.3031e-01,  1.0000e+00],\n",
       "        [ 5.7165e-01, -3.8193e-01,  0.0000e+00],\n",
       "        [-5.7165e-01,  3.8193e-01,  1.0000e+00],\n",
       "        [ 5.7744e-01, -2.3915e-01,  0.0000e+00],\n",
       "        [-5.7744e-01,  2.3915e-01,  1.0000e+00],\n",
       "        [ 5.5170e-01, -1.0971e-01,  0.0000e+00],\n",
       "        [-5.5170e-01,  1.0971e-01,  1.0000e+00],\n",
       "        [ 5.0000e-01,  2.0000e-05,  0.0000e+00],\n",
       "        [-5.0000e-01, -2.0000e-05,  1.0000e+00]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_output(net):\n",
    "    xrange = torch.arange(start=-7,end=7.1,step=0.01,dtype=torch.float32)\n",
    "    yrange = torch.arange(start=-6.6,end=6.7,step=0.01,dtype=torch.float32)\n",
    "    xcoord = xrange.repeat(yrange.size()[0])\n",
    "    ycoord = torch.repeat_interleave(yrange, xrange.size()[0], dim=0)\n",
    "    grid = torch.cat((xcoord.unsqueeze(1),ycoord.unsqueeze(1)),1)\n",
    "    with torch.no_grad(): # suppress updating of gradients\n",
    "        net.eval()        # toggle batch norm, dropout\n",
    "        output = net(grid)\n",
    "        net.train() # toggle batch norm, dropout back again\n",
    "\n",
    "        pred = (output >= 0.5).float()\n",
    "\n",
    "        # plot function computed by model\n",
    "        plt.clf()\n",
    "        plt.pcolormesh(xrange,yrange,pred.cpu().view(yrange.size()[0],xrange.size()[0]), cmap='Wistia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hidden(net, layer, node):\n",
    "    print(\"node:\",node)\n",
    "    print(\"layer:\",layer)\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import arange\n",
    "p = [j for j in arange(.1,1,.1)]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:  100 loss: 0.6723 acc: 64.95\n",
      "ep:  200 loss: 0.6437 acc: 54.12\n",
      "ep:  300 loss: 0.4858 acc: 64.95\n",
      "ep:  400 loss: 0.2234 acc: 79.90\n",
      "ep:  500 loss: 0.1196 acc: 91.75\n",
      "ep:  600 loss: 0.0380 acc: 99.48\n",
      "hidden:100, acc:100.0 , epoch:624\n",
      "ep:  100 loss: 0.6491 acc: 55.15\n",
      "ep:  200 loss: 0.3185 acc: 77.84\n",
      "ep:  300 loss: 0.1180 acc: 97.94\n",
      "hidden:200, acc:100.0 , epoch:341\n",
      "ep:  100 loss: 0.5346 acc: 59.79\n",
      "ep:  200 loss: 0.2557 acc: 82.47\n",
      "hidden:300, acc:100.0 , epoch:258\n",
      "ep:  100 loss: 0.4698 acc: 64.43\n",
      "ep:  200 loss: 0.1926 acc: 88.66\n",
      "hidden:400, acc:99.48453521728516 , epoch:257\n",
      "ep:  100 loss: 0.4664 acc: 64.43\n",
      "ep:  200 loss: 0.1769 acc: 87.11\n",
      "hidden:500, acc:94.8453598022461 , epoch:256\n"
     ]
    }
   ],
   "source": [
    "num_input = data.shape[1] - 1\n",
    "\n",
    "full_input  = data[:,0:num_input]\n",
    "full_target = data[:,num_input:num_input+1]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(full_input,full_target)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset,batch_size=97)\n",
    "\n",
    "epoch = 20000\n",
    "y_epoch_converge = []\n",
    "x_init_vals = []\n",
    "z_accuracy = []\n",
    "for init in p:\n",
    "    net = RawNet(hid_layers)\n",
    "\n",
    "    if list(net.parameters()):\n",
    "        # initialize weight values\n",
    "        for m in list(net.parameters()):\n",
    "            m.data.normal_(0,init)\n",
    "\n",
    "        # use Adam optimizer\n",
    "        optimizer = torch.optim.Adam(net.parameters(),eps=0.000001,lr=.01,\n",
    "                                     betas=(0.9,0.999),weight_decay=0.0001)\n",
    "        # training loop\n",
    "        for epoch in range(1, epoch):\n",
    "            accuracy = train(net, train_loader, optimizer)\n",
    "            if accuracy == 100:\n",
    "                break\n",
    "\n",
    "    y_epoch_converge.append(epoch)\n",
    "    x_init_vals.append(init)\n",
    "    z_accuracy.append(accuracy)\n",
    "    print(f\"hidden:{hid_layers}, acc:{accuracy} , epoch:{epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.active1[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97])\n",
      "torch.Size([97])\n",
      "1875300\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(net.active1[:,0].shape)\n",
    "    net.eval() \n",
    "    print(net.active1[:,0].shape)\n",
    "    output = net(grid)\n",
    "    print(net.active1[:,0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # suppress updating of gradients\n",
    "    # plot function computed by model\n",
    "    net.eval()\n",
    "    net(grid)\n",
    "    hid = net.hidlayer[0]\n",
    "    pred = (hid[:,0].view(hid.shape[0],1) >= 0.5).float()\n",
    "    plt.clf()\n",
    "    plt.pcolormesh(xrange,yrange,pred.cpu().view(yrange.size()[0],xrange.size()[0]), cmap='inferno')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1875300, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6938,  0.6930,  0.6921,  ..., -0.6984, -0.6992, -0.6999])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.active1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1875300, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " with torch.no_grad(): # suppress updating of gradients\n",
    "    net.eval()        # toggle batch norm, dropout\n",
    "    output = net(grid)\n",
    "    net.train() # toggle batch norm, dropout back again\n",
    "\n",
    "    pred = (output >= 0.5).float()\n",
    "    \n",
    "pred.shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0000, -6.9900, -6.9800,  ...,  7.0700,  7.0800,  7.0900])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x1bfe46c6048>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.pcolormesh(xrange,yrange,pred.cpu().view(yrange.size()[0],xrange.size()[0]), cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1330, 1410]' is invalid for input of size 937650000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-47be79e5d057>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myrange\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxrange\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1330, 1410]' is invalid for input of size 937650000"
     ]
    }
   ],
   "source": [
    "net.active1.cpu().view(yrange.size()[0],xrange.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.active1[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.active1[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (net.active1[:,0] >= 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolarNet(torch.nn.Module):\n",
    "    def __init__(self, num_hid):\n",
    "        super(PolarNet, self).__init__()\n",
    "        self.feed = nn.Linear(2,num_hid) \n",
    "        self.hid = nn.Linear(num_hid,1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input[:,0]\n",
    "        y = input[:,1]\n",
    "        out = torch.zeros(input.shape,dtype=torch.float32)\n",
    "        out[:,0] = torch.sqrt(x*x + y*y) #tranform data\n",
    "        out[:,1] = torch.atan2(y,x)\n",
    "        #here we start the forward pass\n",
    "        self.active1 = torch.tanh(self.feed(out))\n",
    "        self.active2 = torch.tanh(self.hid(self.active1))\n",
    "        self.hidlayer = [self.active1]\n",
    "        return F.sigmoid(self.active2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose Ca\\Anaconda3\\envs\\deepenv\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:  100 loss: 0.6569 acc: 61.34\n",
      "ep:  200 loss: 0.6209 acc: 61.34\n",
      "ep:  300 loss: 0.5497 acc: 67.01\n",
      "ep:  400 loss: 0.4692 acc: 66.49\n",
      "ep:  500 loss: 0.3945 acc: 76.80\n",
      "ep:  600 loss: 0.3650 acc: 76.29\n",
      "ep:  700 loss: 0.3555 acc: 75.77\n",
      "ep:  800 loss: 0.3516 acc: 75.77\n",
      "ep:  900 loss: 0.3497 acc: 75.77\n",
      "ep: 1000 loss: 0.3486 acc: 75.26\n",
      "ep: 1100 loss: 0.3474 acc: 75.26\n",
      "ep: 1200 loss: 0.3466 acc: 75.26\n",
      "ep: 1300 loss: 0.3459 acc: 75.26\n",
      "ep: 1400 loss: 0.3455 acc: 75.26\n",
      "ep: 1500 loss: 0.3453 acc: 74.74\n",
      "ep: 1600 loss: 0.3452 acc: 74.74\n",
      "ep: 1700 loss: 0.3450 acc: 74.74\n",
      "ep: 1800 loss: 0.3449 acc: 76.29\n",
      "ep: 1900 loss: 0.3447 acc: 77.32\n",
      "ep: 2000 loss: 0.3445 acc: 78.87\n",
      "ep: 2100 loss: 0.3446 acc: 79.38\n",
      "ep: 2200 loss: 0.3441 acc: 80.93\n",
      "ep: 2300 loss: 0.3429 acc: 81.44\n",
      "ep: 2400 loss: 0.3412 acc: 90.21\n",
      "ep: 2500 loss: 0.3392 acc: 92.27\n",
      "ep: 2600 loss: 0.3368 acc: 94.33\n",
      "ep: 2700 loss: 0.3329 acc: 98.97\n",
      "ep: 2800 loss: 0.3308 acc: 99.48\n",
      "ep: 2900 loss: 0.3292 acc: 99.48\n",
      "ep: 3000 loss: 0.3283 acc: 99.48\n",
      "ep: 3100 loss: 0.3277 acc: 99.48\n",
      "ep: 3200 loss: 0.3273 acc: 99.48\n",
      "ep: 3300 loss: 0.3271 acc: 99.48\n",
      "ep: 3400 loss: 0.3269 acc: 99.48\n",
      "ep: 3500 loss: 0.3268 acc: 99.48\n",
      "ep: 3600 loss: 0.3267 acc: 99.48\n",
      "ep: 3700 loss: 0.3266 acc: 99.48\n",
      "ep: 3800 loss: 0.3265 acc: 99.48\n",
      "ep: 3900 loss: 0.3265 acc: 99.48\n",
      "ep: 4000 loss: 0.3264 acc: 99.48\n",
      "ep: 4100 loss: 0.3264 acc: 99.48\n",
      "ep: 4200 loss: 0.3264 acc: 99.48\n",
      "ep: 4300 loss: 0.3263 acc: 99.48\n",
      "ep: 4400 loss: 0.3263 acc: 99.48\n",
      "ep: 4500 loss: 0.3263 acc: 99.48\n",
      "ep: 4600 loss: 0.3262 acc: 99.48\n",
      "ep: 4700 loss: 0.3262 acc: 99.48\n",
      "ep: 4800 loss: 0.3262 acc: 99.48\n",
      "ep: 4900 loss: 0.3262 acc: 99.48\n",
      "ep: 5000 loss: 0.3262 acc: 99.48\n",
      "ep: 5100 loss: 0.3262 acc: 99.48\n",
      "ep: 5200 loss: 0.3262 acc: 99.48\n",
      "ep: 5300 loss: 0.3261 acc: 99.48\n",
      "ep: 5400 loss: 0.3261 acc: 99.48\n",
      "ep: 5500 loss: 0.3261 acc: 99.48\n",
      "ep: 5600 loss: 0.3261 acc: 99.48\n",
      "ep: 5700 loss: 0.3261 acc: 99.48\n",
      "ep: 5800 loss: 0.3261 acc: 99.48\n",
      "ep: 5900 loss: 0.3261 acc: 99.48\n",
      "ep: 6000 loss: 0.3261 acc: 99.48\n",
      "ep: 6100 loss: 0.3261 acc: 99.48\n",
      "ep: 6200 loss: 0.3261 acc: 99.48\n",
      "ep: 6300 loss: 0.3261 acc: 99.48\n",
      "ep: 6400 loss: 0.3261 acc: 99.48\n",
      "ep: 6500 loss: 0.3261 acc: 99.48\n",
      "ep: 6600 loss: 0.3261 acc: 99.48\n",
      "ep: 6700 loss: 0.3260 acc: 99.48\n",
      "ep: 6800 loss: 0.3260 acc: 99.48\n",
      "ep: 6900 loss: 0.3260 acc: 99.48\n",
      "ep: 7000 loss: 0.3260 acc: 99.48\n",
      "ep: 7100 loss: 0.3260 acc: 99.48\n",
      "ep: 7200 loss: 0.3260 acc: 99.48\n",
      "ep: 7300 loss: 0.3260 acc: 99.48\n",
      "ep: 7400 loss: 0.3260 acc: 99.48\n",
      "ep: 7500 loss: 0.3260 acc: 99.48\n",
      "ep: 7600 loss: 0.3260 acc: 99.48\n",
      "ep: 7700 loss: 0.3260 acc: 99.48\n",
      "ep: 7800 loss: 0.3260 acc: 99.48\n",
      "ep: 7900 loss: 0.3260 acc: 99.48\n",
      "ep: 8000 loss: 0.3260 acc: 99.48\n",
      "ep: 8100 loss: 0.3260 acc: 99.48\n",
      "ep: 8200 loss: 0.3260 acc: 99.48\n",
      "ep: 8300 loss: 0.3260 acc: 99.48\n",
      "ep: 8400 loss: 0.3260 acc: 99.48\n",
      "ep: 8500 loss: 0.3260 acc: 99.48\n",
      "ep: 8600 loss: 0.3260 acc: 99.48\n",
      "ep: 8700 loss: 0.3260 acc: 99.48\n",
      "ep: 8800 loss: 0.3260 acc: 99.48\n",
      "ep: 8900 loss: 0.3260 acc: 99.48\n",
      "ep: 9000 loss: 0.3260 acc: 99.48\n",
      "ep: 9100 loss: 0.3260 acc: 99.48\n",
      "ep: 9200 loss: 0.3260 acc: 99.48\n",
      "ep: 9300 loss: 0.3260 acc: 99.48\n",
      "ep: 9400 loss: 0.3260 acc: 99.48\n",
      "ep: 9500 loss: 0.3260 acc: 99.48\n",
      "ep: 9600 loss: 0.3260 acc: 99.48\n",
      "ep: 9700 loss: 0.3260 acc: 99.48\n",
      "ep: 9800 loss: 0.3260 acc: 99.48\n",
      "ep: 9900 loss: 0.3260 acc: 99.48\n",
      "ep:10000 loss: 0.3260 acc: 99.48\n",
      "ep:10100 loss: 0.3260 acc: 99.48\n",
      "ep:10200 loss: 0.3260 acc: 99.48\n",
      "ep:10300 loss: 0.3260 acc: 99.48\n",
      "ep:10400 loss: 0.3260 acc: 99.48\n",
      "ep:10500 loss: 0.3260 acc: 99.48\n",
      "ep:10600 loss: 0.3260 acc: 99.48\n",
      "ep:10700 loss: 0.3260 acc: 99.48\n",
      "ep:10800 loss: 0.3260 acc: 99.48\n",
      "ep:10900 loss: 0.3260 acc: 99.48\n",
      "ep:11000 loss: 0.3260 acc: 99.48\n",
      "ep:11100 loss: 0.3260 acc: 99.48\n",
      "ep:11200 loss: 0.3260 acc: 99.48\n",
      "ep:11300 loss: 0.3260 acc: 99.48\n",
      "ep:11400 loss: 0.3260 acc: 99.48\n",
      "ep:11500 loss: 0.3260 acc: 99.48\n",
      "ep:11600 loss: 0.3260 acc: 99.48\n",
      "ep:11700 loss: 0.3260 acc: 99.48\n",
      "ep:11800 loss: 0.3260 acc: 99.48\n",
      "ep:11900 loss: 0.3260 acc: 99.48\n",
      "ep:12000 loss: 0.3260 acc: 99.48\n",
      "ep:12100 loss: 0.3260 acc: 99.48\n",
      "ep:12200 loss: 0.3260 acc: 99.48\n",
      "ep:12300 loss: 0.3260 acc: 99.48\n",
      "ep:12400 loss: 0.3260 acc: 99.48\n",
      "ep:12500 loss: 0.3260 acc: 99.48\n",
      "ep:12600 loss: 0.3260 acc: 99.48\n",
      "ep:12700 loss: 0.3260 acc: 99.48\n",
      "ep:12800 loss: 0.3260 acc: 99.48\n",
      "ep:12900 loss: 0.3260 acc: 99.48\n",
      "ep:13000 loss: 0.3260 acc: 99.48\n",
      "ep:13100 loss: 0.3260 acc: 99.48\n",
      "ep:13200 loss: 0.3260 acc: 99.48\n",
      "ep:13300 loss: 0.3260 acc: 99.48\n",
      "ep:13400 loss: 0.3260 acc: 99.48\n",
      "ep:13500 loss: 0.3260 acc: 99.48\n",
      "ep:13600 loss: 0.3260 acc: 99.48\n",
      "ep:13700 loss: 0.3260 acc: 99.48\n",
      "ep:13800 loss: 0.3261 acc: 99.48\n",
      "ep:13900 loss: 0.3260 acc: 99.48\n",
      "ep:14000 loss: 0.3260 acc: 99.48\n",
      "ep:14100 loss: 0.3260 acc: 99.48\n",
      "ep:14200 loss: 0.3260 acc: 99.48\n",
      "ep:14300 loss: 0.3260 acc: 99.48\n",
      "ep:14400 loss: 0.3260 acc: 99.48\n",
      "ep:14500 loss: 0.3260 acc: 99.48\n",
      "ep:14600 loss: 0.3260 acc: 99.48\n",
      "ep:14700 loss: 0.3260 acc: 99.48\n",
      "ep:14800 loss: 0.3260 acc: 99.48\n",
      "ep:14900 loss: 0.3260 acc: 99.48\n",
      "ep:15000 loss: 0.3260 acc: 99.48\n",
      "ep:15100 loss: 0.3260 acc: 99.48\n",
      "ep:15200 loss: 0.3260 acc: 99.48\n",
      "ep:15300 loss: 0.3260 acc: 99.48\n",
      "ep:15400 loss: 0.3260 acc: 99.48\n",
      "ep:15500 loss: 0.3260 acc: 99.48\n",
      "ep:15600 loss: 0.3260 acc: 99.48\n",
      "ep:15700 loss: 0.3260 acc: 99.48\n",
      "ep:15800 loss: 0.3260 acc: 99.48\n",
      "ep:15900 loss: 0.3260 acc: 99.48\n",
      "ep:16000 loss: 0.3260 acc: 99.48\n",
      "ep:16100 loss: 0.3260 acc: 99.48\n",
      "ep:16200 loss: 0.3260 acc: 99.48\n",
      "ep:16300 loss: 0.3260 acc: 99.48\n",
      "ep:16400 loss: 0.3260 acc: 99.48\n",
      "ep:16500 loss: 0.3260 acc: 99.48\n",
      "ep:16600 loss: 0.3260 acc: 99.48\n",
      "ep:16700 loss: 0.3260 acc: 99.48\n",
      "ep:16800 loss: 0.3260 acc: 99.48\n",
      "ep:16900 loss: 0.3260 acc: 99.48\n",
      "ep:17000 loss: 0.3260 acc: 99.48\n",
      "ep:17100 loss: 0.3260 acc: 99.48\n",
      "ep:17200 loss: 0.3260 acc: 99.48\n",
      "ep:17300 loss: 0.3260 acc: 99.48\n",
      "ep:17400 loss: 0.3260 acc: 99.48\n",
      "ep:17500 loss: 0.3260 acc: 99.48\n",
      "ep:17600 loss: 0.3260 acc: 99.48\n",
      "ep:17700 loss: 0.3260 acc: 99.48\n",
      "ep:17800 loss: 0.3260 acc: 99.48\n",
      "ep:17900 loss: 0.3260 acc: 99.48\n",
      "ep:18000 loss: 0.3260 acc: 99.48\n",
      "ep:18100 loss: 0.3260 acc: 99.48\n",
      "ep:18200 loss: 0.3260 acc: 99.48\n",
      "ep:18300 loss: 0.3260 acc: 99.48\n",
      "ep:18400 loss: 0.3260 acc: 99.48\n",
      "ep:18500 loss: 0.3260 acc: 99.48\n",
      "ep:18600 loss: 0.3260 acc: 99.48\n",
      "ep:18700 loss: 0.3260 acc: 99.48\n",
      "ep:18800 loss: 0.3260 acc: 99.48\n",
      "ep:18900 loss: 0.3260 acc: 99.48\n",
      "ep:19000 loss: 0.3260 acc: 99.48\n",
      "ep:19100 loss: 0.3260 acc: 99.48\n",
      "ep:19200 loss: 0.3260 acc: 99.48\n",
      "ep:19300 loss: 0.3260 acc: 99.48\n",
      "ep:19400 loss: 0.3260 acc: 99.48\n",
      "ep:19500 loss: 0.3260 acc: 99.48\n",
      "ep:19600 loss: 0.3260 acc: 99.48\n",
      "ep:19700 loss: 0.3260 acc: 99.48\n",
      "ep:19800 loss: 0.3260 acc: 99.48\n",
      "ep:19900 loss: 0.3260 acc: 99.48\n",
      "hidden:10, acc:99.48453521728516 , epoch:19999\n",
      "ep:  100 loss: 0.6466 acc: 60.82\n",
      "ep:  200 loss: 0.5323 acc: 68.04\n",
      "ep:  300 loss: 0.4553 acc: 72.16\n",
      "ep:  400 loss: 0.4195 acc: 74.74\n",
      "ep:  500 loss: 0.3831 acc: 77.32\n",
      "ep:  600 loss: 0.3633 acc: 77.32\n",
      "ep:  700 loss: 0.3542 acc: 76.80\n",
      "ep:  800 loss: 0.3503 acc: 76.29\n",
      "ep:  900 loss: 0.3488 acc: 75.77\n",
      "ep: 1000 loss: 0.3484 acc: 75.77\n",
      "ep: 1100 loss: 0.3481 acc: 75.26\n",
      "ep: 1200 loss: 0.3476 acc: 75.26\n",
      "ep: 1300 loss: 0.3470 acc: 75.26\n",
      "ep: 1400 loss: 0.3463 acc: 74.74\n",
      "ep: 1500 loss: 0.3456 acc: 76.29\n",
      "ep: 1600 loss: 0.3451 acc: 77.32\n",
      "ep: 1700 loss: 0.3445 acc: 77.32\n",
      "ep: 1800 loss: 0.3439 acc: 77.84\n",
      "ep: 1900 loss: 0.3435 acc: 77.84\n",
      "ep: 2000 loss: 0.3433 acc: 77.84\n",
      "ep: 2100 loss: 0.3427 acc: 80.93\n",
      "ep: 2200 loss: 0.3345 acc: 82.47\n",
      "ep: 2300 loss: 0.3347 acc: 90.72\n",
      "ep: 2400 loss: 0.3325 acc: 91.75\n",
      "ep: 2500 loss: 0.3308 acc: 98.97\n",
      "ep: 2600 loss: 0.3294 acc: 99.48\n",
      "ep: 2700 loss: 0.3283 acc: 99.48\n",
      "ep: 2800 loss: 0.3276 acc: 99.48\n",
      "ep: 2900 loss: 0.3272 acc: 99.48\n",
      "ep: 3000 loss: 0.3269 acc: 99.48\n",
      "ep: 3100 loss: 0.3267 acc: 99.48\n",
      "ep: 3200 loss: 0.3265 acc: 99.48\n",
      "ep: 3300 loss: 0.3264 acc: 99.48\n",
      "ep: 3400 loss: 0.3263 acc: 99.48\n",
      "ep: 3500 loss: 0.3262 acc: 99.48\n",
      "ep: 3600 loss: 0.3262 acc: 99.48\n",
      "ep: 3700 loss: 0.3262 acc: 99.48\n",
      "ep: 3800 loss: 0.3261 acc: 99.48\n",
      "ep: 3900 loss: 0.3261 acc: 99.48\n",
      "ep: 4000 loss: 0.3261 acc: 99.48\n",
      "ep: 4100 loss: 0.3261 acc: 99.48\n",
      "ep: 4200 loss: 0.3260 acc: 99.48\n",
      "ep: 4300 loss: 0.3260 acc: 99.48\n",
      "ep: 4400 loss: 0.3260 acc: 99.48\n",
      "ep: 4500 loss: 0.3260 acc: 99.48\n",
      "ep: 4600 loss: 0.3260 acc: 99.48\n",
      "ep: 4700 loss: 0.3260 acc: 99.48\n",
      "ep: 4800 loss: 0.3259 acc: 99.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4900 loss: 0.3259 acc: 99.48\n",
      "ep: 5000 loss: 0.3259 acc: 99.48\n",
      "ep: 5100 loss: 0.3259 acc: 99.48\n",
      "ep: 5200 loss: 0.3259 acc: 99.48\n",
      "ep: 5300 loss: 0.3259 acc: 99.48\n",
      "ep: 5400 loss: 0.3259 acc: 99.48\n",
      "ep: 5500 loss: 0.3259 acc: 99.48\n",
      "ep: 5600 loss: 0.3259 acc: 99.48\n",
      "ep: 5700 loss: 0.3259 acc: 99.48\n",
      "ep: 5800 loss: 0.3259 acc: 99.48\n",
      "ep: 5900 loss: 0.3259 acc: 99.48\n",
      "ep: 6000 loss: 0.3259 acc: 99.48\n",
      "ep: 6100 loss: 0.3259 acc: 99.48\n",
      "ep: 6200 loss: 0.3259 acc: 99.48\n",
      "ep: 6300 loss: 0.3259 acc: 99.48\n",
      "ep: 6400 loss: 0.3259 acc: 99.48\n",
      "ep: 6500 loss: 0.3259 acc: 99.48\n",
      "ep: 6600 loss: 0.3259 acc: 99.48\n",
      "ep: 6700 loss: 0.3259 acc: 99.48\n",
      "ep: 6800 loss: 0.3259 acc: 99.48\n",
      "ep: 6900 loss: 0.3259 acc: 99.48\n",
      "ep: 7000 loss: 0.3259 acc: 99.48\n",
      "ep: 7100 loss: 0.3259 acc: 99.48\n",
      "ep: 7200 loss: 0.3259 acc: 99.48\n",
      "ep: 7300 loss: 0.3259 acc: 99.48\n",
      "ep: 7400 loss: 0.3259 acc: 99.48\n",
      "ep: 7500 loss: 0.3259 acc: 99.48\n",
      "ep: 7600 loss: 0.3259 acc: 99.48\n",
      "ep: 7700 loss: 0.3259 acc: 99.48\n",
      "ep: 7800 loss: 0.3259 acc: 99.48\n",
      "ep: 7900 loss: 0.3259 acc: 99.48\n",
      "ep: 8000 loss: 0.3259 acc: 99.48\n",
      "ep: 8100 loss: 0.3259 acc: 99.48\n",
      "ep: 8200 loss: 0.3259 acc: 99.48\n",
      "ep: 8300 loss: 0.3259 acc: 99.48\n",
      "ep: 8400 loss: 0.3259 acc: 99.48\n",
      "ep: 8500 loss: 0.3259 acc: 99.48\n",
      "ep: 8600 loss: 0.3259 acc: 99.48\n",
      "ep: 8700 loss: 0.3259 acc: 99.48\n",
      "ep: 8800 loss: 0.3259 acc: 99.48\n",
      "ep: 8900 loss: 0.3259 acc: 99.48\n",
      "ep: 9000 loss: 0.3259 acc: 99.48\n",
      "ep: 9100 loss: 0.3259 acc: 99.48\n",
      "ep: 9200 loss: 0.3259 acc: 99.48\n",
      "ep: 9300 loss: 0.3259 acc: 99.48\n",
      "ep: 9400 loss: 0.3259 acc: 99.48\n",
      "ep: 9500 loss: 0.3259 acc: 99.48\n",
      "ep: 9600 loss: 0.3259 acc: 99.48\n",
      "ep: 9700 loss: 0.3259 acc: 99.48\n",
      "ep: 9800 loss: 0.3259 acc: 99.48\n",
      "ep: 9900 loss: 0.3259 acc: 99.48\n",
      "ep:10000 loss: 0.3259 acc: 99.48\n",
      "ep:10100 loss: 0.3259 acc: 99.48\n",
      "ep:10200 loss: 0.3259 acc: 99.48\n",
      "ep:10300 loss: 0.3259 acc: 99.48\n",
      "ep:10400 loss: 0.3259 acc: 99.48\n",
      "ep:10500 loss: 0.3259 acc: 99.48\n",
      "ep:10600 loss: 0.3259 acc: 99.48\n",
      "ep:10700 loss: 0.3259 acc: 99.48\n",
      "ep:10800 loss: 0.3259 acc: 99.48\n",
      "ep:10900 loss: 0.3259 acc: 99.48\n",
      "ep:11000 loss: 0.3259 acc: 99.48\n",
      "ep:11100 loss: 0.3259 acc: 99.48\n",
      "ep:11200 loss: 0.3259 acc: 99.48\n",
      "ep:11300 loss: 0.3259 acc: 99.48\n",
      "ep:11400 loss: 0.3259 acc: 99.48\n",
      "ep:11500 loss: 0.3259 acc: 99.48\n",
      "ep:11600 loss: 0.3259 acc: 99.48\n",
      "ep:11700 loss: 0.3259 acc: 99.48\n",
      "ep:11800 loss: 0.3259 acc: 99.48\n",
      "ep:11900 loss: 0.3259 acc: 99.48\n",
      "ep:12000 loss: 0.3259 acc: 99.48\n",
      "ep:12100 loss: 0.3259 acc: 99.48\n",
      "ep:12200 loss: 0.3259 acc: 99.48\n",
      "ep:12300 loss: 0.3259 acc: 99.48\n",
      "ep:12400 loss: 0.3259 acc: 99.48\n",
      "ep:12500 loss: 0.3259 acc: 99.48\n",
      "ep:12600 loss: 0.3259 acc: 99.48\n",
      "ep:12700 loss: 0.3259 acc: 99.48\n",
      "ep:12800 loss: 0.3259 acc: 99.48\n",
      "ep:12900 loss: 0.3259 acc: 99.48\n",
      "ep:13000 loss: 0.3259 acc: 99.48\n",
      "ep:13100 loss: 0.3259 acc: 99.48\n",
      "ep:13200 loss: 0.3259 acc: 99.48\n",
      "ep:13300 loss: 0.3259 acc: 99.48\n",
      "ep:13400 loss: 0.3259 acc: 99.48\n",
      "ep:13500 loss: 0.3259 acc: 99.48\n",
      "ep:13600 loss: 0.3259 acc: 99.48\n",
      "ep:13700 loss: 0.3259 acc: 99.48\n",
      "ep:13800 loss: 0.3259 acc: 99.48\n",
      "ep:13900 loss: 0.3259 acc: 99.48\n",
      "ep:14000 loss: 0.3259 acc: 99.48\n",
      "ep:14100 loss: 0.3259 acc: 99.48\n",
      "ep:14200 loss: 0.3259 acc: 99.48\n",
      "ep:14300 loss: 0.3259 acc: 99.48\n",
      "ep:14400 loss: 0.3259 acc: 99.48\n",
      "ep:14500 loss: 0.3259 acc: 99.48\n",
      "ep:14600 loss: 0.3259 acc: 99.48\n",
      "ep:14700 loss: 0.3259 acc: 99.48\n",
      "ep:14800 loss: 0.3259 acc: 99.48\n",
      "ep:14900 loss: 0.3259 acc: 99.48\n",
      "ep:15000 loss: 0.3259 acc: 99.48\n",
      "ep:15100 loss: 0.3259 acc: 99.48\n",
      "ep:15200 loss: 0.3259 acc: 99.48\n",
      "ep:15300 loss: 0.3259 acc: 99.48\n",
      "ep:15400 loss: 0.3259 acc: 99.48\n",
      "ep:15500 loss: 0.3259 acc: 99.48\n",
      "ep:15600 loss: 0.3259 acc: 99.48\n",
      "ep:15700 loss: 0.3259 acc: 99.48\n",
      "ep:15800 loss: 0.3259 acc: 99.48\n",
      "ep:15900 loss: 0.3259 acc: 99.48\n",
      "ep:16000 loss: 0.3259 acc: 99.48\n",
      "ep:16100 loss: 0.3259 acc: 99.48\n",
      "ep:16200 loss: 0.3259 acc: 99.48\n",
      "ep:16300 loss: 0.3259 acc: 99.48\n",
      "ep:16400 loss: 0.3259 acc: 99.48\n",
      "ep:16500 loss: 0.3259 acc: 99.48\n",
      "ep:16600 loss: 0.3259 acc: 99.48\n",
      "ep:16700 loss: 0.3259 acc: 99.48\n",
      "ep:16800 loss: 0.3259 acc: 99.48\n",
      "ep:16900 loss: 0.3259 acc: 99.48\n",
      "ep:17000 loss: 0.3259 acc: 99.48\n",
      "ep:17100 loss: 0.3259 acc: 99.48\n",
      "ep:17200 loss: 0.3259 acc: 99.48\n",
      "ep:17300 loss: 0.3259 acc: 99.48\n",
      "ep:17400 loss: 0.3259 acc: 99.48\n",
      "ep:17500 loss: 0.3259 acc: 99.48\n",
      "ep:17600 loss: 0.3259 acc: 99.48\n",
      "ep:17700 loss: 0.3259 acc: 99.48\n",
      "ep:17800 loss: 0.3259 acc: 99.48\n",
      "ep:17900 loss: 0.3259 acc: 99.48\n",
      "ep:18000 loss: 0.3259 acc: 99.48\n",
      "ep:18100 loss: 0.3259 acc: 99.48\n",
      "ep:18200 loss: 0.3259 acc: 99.48\n",
      "ep:18300 loss: 0.3259 acc: 99.48\n",
      "ep:18400 loss: 0.3259 acc: 99.48\n",
      "ep:18500 loss: 0.3259 acc: 99.48\n",
      "ep:18600 loss: 0.3259 acc: 99.48\n",
      "ep:18700 loss: 0.3259 acc: 99.48\n",
      "ep:18800 loss: 0.3259 acc: 99.48\n",
      "ep:18900 loss: 0.3259 acc: 99.48\n",
      "ep:19000 loss: 0.3259 acc: 99.48\n",
      "ep:19100 loss: 0.3259 acc: 99.48\n",
      "ep:19200 loss: 0.3259 acc: 99.48\n",
      "ep:19300 loss: 0.3259 acc: 99.48\n",
      "ep:19400 loss: 0.3259 acc: 99.48\n",
      "ep:19500 loss: 0.3259 acc: 99.48\n",
      "ep:19600 loss: 0.3259 acc: 99.48\n",
      "ep:19700 loss: 0.3259 acc: 99.48\n",
      "ep:19800 loss: 0.3259 acc: 99.48\n",
      "ep:19900 loss: 0.3259 acc: 99.48\n",
      "hidden:11, acc:99.48453521728516 , epoch:19998\n",
      "ep:  100 loss: 0.6740 acc: 58.76\n",
      "ep:  200 loss: 0.5860 acc: 58.76\n",
      "ep:  300 loss: 0.4836 acc: 65.98\n",
      "ep:  400 loss: 0.4720 acc: 67.01\n",
      "ep:  500 loss: 0.4650 acc: 66.49\n",
      "ep:  600 loss: 0.4531 acc: 72.16\n",
      "ep:  700 loss: 0.4218 acc: 74.23\n",
      "ep:  800 loss: 0.3728 acc: 77.32\n",
      "ep:  900 loss: 0.3572 acc: 77.32\n",
      "ep: 1000 loss: 0.3518 acc: 76.29\n",
      "ep: 1100 loss: 0.3497 acc: 76.29\n",
      "ep: 1200 loss: 0.3485 acc: 76.29\n",
      "ep: 1300 loss: 0.3478 acc: 76.29\n",
      "ep: 1400 loss: 0.3474 acc: 75.26\n",
      "ep: 1500 loss: 0.3472 acc: 75.26\n",
      "ep: 1600 loss: 0.3472 acc: 75.26\n",
      "ep: 1700 loss: 0.3469 acc: 75.26\n",
      "ep: 1800 loss: 0.3460 acc: 74.74\n",
      "ep: 1900 loss: 0.3455 acc: 74.74\n",
      "ep: 2000 loss: 0.3450 acc: 74.74\n",
      "ep: 2100 loss: 0.3447 acc: 74.74\n",
      "ep: 2200 loss: 0.3444 acc: 77.32\n",
      "ep: 2300 loss: 0.3440 acc: 80.41\n",
      "ep: 2400 loss: 0.3437 acc: 80.93\n",
      "ep: 2500 loss: 0.3434 acc: 80.93\n",
      "ep: 2600 loss: 0.3430 acc: 81.44\n",
      "ep: 2700 loss: 0.3425 acc: 81.44\n",
      "ep: 2800 loss: 0.3428 acc: 81.44\n",
      "ep: 2900 loss: 0.3412 acc: 82.47\n",
      "ep: 3000 loss: 0.3390 acc: 82.47\n",
      "ep: 3100 loss: 0.3390 acc: 83.51\n",
      "ep: 3200 loss: 0.3400 acc: 90.21\n",
      "ep: 3300 loss: 0.3391 acc: 90.72\n",
      "ep: 3400 loss: 0.3376 acc: 90.72\n",
      "ep: 3500 loss: 0.3345 acc: 90.72\n",
      "ep: 3600 loss: 0.3335 acc: 98.45\n",
      "ep: 3700 loss: 0.3315 acc: 99.48\n",
      "ep: 3800 loss: 0.3300 acc: 99.48\n",
      "ep: 3900 loss: 0.3288 acc: 99.48\n",
      "ep: 4000 loss: 0.3280 acc: 99.48\n",
      "ep: 4100 loss: 0.3274 acc: 99.48\n",
      "ep: 4200 loss: 0.3270 acc: 99.48\n",
      "ep: 4300 loss: 0.3268 acc: 99.48\n",
      "ep: 4400 loss: 0.3266 acc: 99.48\n",
      "ep: 4500 loss: 0.3264 acc: 99.48\n",
      "ep: 4600 loss: 0.3263 acc: 99.48\n",
      "ep: 4700 loss: 0.3262 acc: 99.48\n",
      "ep: 4800 loss: 0.3262 acc: 99.48\n",
      "ep: 4900 loss: 0.3261 acc: 99.48\n",
      "ep: 5000 loss: 0.3261 acc: 99.48\n",
      "ep: 5100 loss: 0.3260 acc: 99.48\n",
      "ep: 5200 loss: 0.3260 acc: 99.48\n",
      "ep: 5300 loss: 0.3260 acc: 99.48\n",
      "ep: 5400 loss: 0.3259 acc: 99.48\n",
      "ep: 5500 loss: 0.3259 acc: 99.48\n",
      "ep: 5600 loss: 0.3259 acc: 99.48\n",
      "ep: 5700 loss: 0.3259 acc: 99.48\n",
      "ep: 5800 loss: 0.3259 acc: 99.48\n",
      "ep: 5900 loss: 0.3259 acc: 99.48\n",
      "ep: 6000 loss: 0.3259 acc: 99.48\n",
      "ep: 6100 loss: 0.3259 acc: 99.48\n",
      "ep: 6200 loss: 0.3259 acc: 99.48\n",
      "ep: 6300 loss: 0.3259 acc: 99.48\n",
      "ep: 6400 loss: 0.3259 acc: 99.48\n",
      "ep: 6500 loss: 0.3259 acc: 99.48\n",
      "ep: 6600 loss: 0.3259 acc: 99.48\n",
      "ep: 6700 loss: 0.3259 acc: 99.48\n",
      "ep: 6800 loss: 0.3259 acc: 99.48\n",
      "ep: 6900 loss: 0.3259 acc: 99.48\n",
      "ep: 7000 loss: 0.3259 acc: 99.48\n",
      "ep: 7100 loss: 0.3259 acc: 99.48\n",
      "ep: 7200 loss: 0.3259 acc: 99.48\n",
      "ep: 7300 loss: 0.3259 acc: 99.48\n",
      "ep: 7400 loss: 0.3259 acc: 99.48\n",
      "ep: 7500 loss: 0.3259 acc: 99.48\n",
      "ep: 7600 loss: 0.3259 acc: 99.48\n",
      "ep: 7700 loss: 0.3259 acc: 99.48\n",
      "ep: 7800 loss: 0.3259 acc: 99.48\n",
      "ep: 7900 loss: 0.3259 acc: 99.48\n",
      "ep: 8000 loss: 0.3259 acc: 99.48\n",
      "ep: 8100 loss: 0.3259 acc: 99.48\n",
      "ep: 8200 loss: 0.3259 acc: 99.48\n",
      "ep: 8300 loss: 0.3259 acc: 99.48\n",
      "ep: 8400 loss: 0.3259 acc: 99.48\n",
      "ep: 8500 loss: 0.3259 acc: 99.48\n",
      "ep: 8600 loss: 0.3259 acc: 99.48\n",
      "ep: 8700 loss: 0.3259 acc: 99.48\n",
      "ep: 8800 loss: 0.3259 acc: 99.48\n",
      "ep: 8900 loss: 0.3259 acc: 99.48\n",
      "ep: 9000 loss: 0.3259 acc: 99.48\n",
      "ep: 9100 loss: 0.3260 acc: 99.48\n",
      "ep: 9200 loss: 0.3259 acc: 99.48\n",
      "ep: 9300 loss: 0.3259 acc: 99.48\n",
      "ep: 9400 loss: 0.3259 acc: 99.48\n",
      "ep: 9500 loss: 0.3259 acc: 99.48\n",
      "ep: 9600 loss: 0.3259 acc: 99.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 9700 loss: 0.3259 acc: 99.48\n",
      "ep: 9800 loss: 0.3259 acc: 99.48\n",
      "ep: 9900 loss: 0.3259 acc: 99.48\n",
      "ep:10000 loss: 0.3259 acc: 99.48\n",
      "ep:10100 loss: 0.3259 acc: 99.48\n",
      "ep:10200 loss: 0.3259 acc: 99.48\n",
      "ep:10300 loss: 0.3259 acc: 99.48\n",
      "ep:10400 loss: 0.3259 acc: 99.48\n",
      "ep:10500 loss: 0.3259 acc: 99.48\n",
      "ep:10600 loss: 0.3259 acc: 99.48\n",
      "ep:10700 loss: 0.3259 acc: 99.48\n",
      "ep:10800 loss: 0.3259 acc: 99.48\n",
      "ep:10900 loss: 0.3259 acc: 99.48\n",
      "ep:11000 loss: 0.3259 acc: 99.48\n",
      "ep:11100 loss: 0.3259 acc: 99.48\n",
      "ep:11200 loss: 0.3259 acc: 99.48\n",
      "ep:11300 loss: 0.3259 acc: 99.48\n",
      "ep:11400 loss: 0.3259 acc: 99.48\n",
      "ep:11500 loss: 0.3259 acc: 99.48\n",
      "ep:11600 loss: 0.3259 acc: 99.48\n",
      "ep:11700 loss: 0.3259 acc: 99.48\n",
      "ep:11800 loss: 0.3259 acc: 99.48\n",
      "ep:11900 loss: 0.3259 acc: 99.48\n",
      "ep:12000 loss: 0.3259 acc: 99.48\n",
      "ep:12100 loss: 0.3259 acc: 99.48\n",
      "ep:12200 loss: 0.3259 acc: 99.48\n",
      "ep:12300 loss: 0.3259 acc: 99.48\n",
      "ep:12400 loss: 0.3259 acc: 99.48\n",
      "ep:12500 loss: 0.3259 acc: 99.48\n",
      "ep:12600 loss: 0.3259 acc: 99.48\n",
      "ep:12700 loss: 0.3259 acc: 99.48\n",
      "ep:12800 loss: 0.3259 acc: 99.48\n",
      "ep:12900 loss: 0.3259 acc: 99.48\n",
      "ep:13000 loss: 0.3259 acc: 99.48\n",
      "ep:13100 loss: 0.3259 acc: 99.48\n",
      "ep:13200 loss: 0.3259 acc: 99.48\n",
      "ep:13300 loss: 0.3259 acc: 99.48\n",
      "ep:13400 loss: 0.3259 acc: 99.48\n",
      "ep:13500 loss: 0.3259 acc: 99.48\n",
      "ep:13600 loss: 0.3259 acc: 99.48\n",
      "ep:13700 loss: 0.3259 acc: 99.48\n",
      "ep:13800 loss: 0.3259 acc: 99.48\n",
      "ep:13900 loss: 0.3259 acc: 99.48\n",
      "ep:14000 loss: 0.3259 acc: 99.48\n",
      "ep:14100 loss: 0.3259 acc: 99.48\n",
      "ep:14200 loss: 0.3259 acc: 99.48\n",
      "ep:14300 loss: 0.3259 acc: 99.48\n",
      "ep:14400 loss: 0.3259 acc: 99.48\n",
      "ep:14500 loss: 0.3259 acc: 99.48\n",
      "ep:14600 loss: 0.3259 acc: 99.48\n",
      "ep:14700 loss: 0.3259 acc: 99.48\n",
      "ep:14800 loss: 0.3259 acc: 99.48\n",
      "ep:14900 loss: 0.3259 acc: 99.48\n",
      "ep:15000 loss: 0.3259 acc: 99.48\n",
      "ep:15100 loss: 0.3259 acc: 99.48\n",
      "ep:15200 loss: 0.3259 acc: 99.48\n",
      "ep:15300 loss: 0.3259 acc: 99.48\n",
      "ep:15400 loss: 0.3259 acc: 99.48\n",
      "ep:15500 loss: 0.3259 acc: 99.48\n",
      "ep:15600 loss: 0.3259 acc: 99.48\n",
      "ep:15700 loss: 0.3259 acc: 99.48\n",
      "ep:15800 loss: 0.3259 acc: 99.48\n",
      "ep:15900 loss: 0.3259 acc: 99.48\n",
      "ep:16000 loss: 0.3259 acc: 99.48\n",
      "ep:16100 loss: 0.3259 acc: 99.48\n",
      "ep:16200 loss: 0.3259 acc: 99.48\n",
      "ep:16300 loss: 0.3259 acc: 99.48\n",
      "ep:16400 loss: 0.3259 acc: 99.48\n",
      "ep:16500 loss: 0.3259 acc: 99.48\n",
      "ep:16600 loss: 0.3259 acc: 99.48\n",
      "ep:16700 loss: 0.3259 acc: 99.48\n",
      "ep:16800 loss: 0.3259 acc: 99.48\n",
      "ep:16900 loss: 0.3259 acc: 99.48\n",
      "ep:17000 loss: 0.3259 acc: 99.48\n",
      "ep:17100 loss: 0.3259 acc: 99.48\n",
      "ep:17200 loss: 0.3259 acc: 99.48\n",
      "ep:17300 loss: 0.3259 acc: 99.48\n",
      "ep:17400 loss: 0.3259 acc: 99.48\n",
      "ep:17500 loss: 0.3259 acc: 99.48\n",
      "ep:17600 loss: 0.3259 acc: 99.48\n",
      "ep:17700 loss: 0.3259 acc: 99.48\n",
      "ep:17800 loss: 0.3259 acc: 99.48\n",
      "ep:17900 loss: 0.3259 acc: 99.48\n",
      "ep:18000 loss: 0.3259 acc: 99.48\n",
      "ep:18100 loss: 0.3259 acc: 99.48\n",
      "ep:18200 loss: 0.3259 acc: 99.48\n",
      "ep:18300 loss: 0.3259 acc: 99.48\n",
      "ep:18400 loss: 0.3259 acc: 99.48\n",
      "ep:18500 loss: 0.3259 acc: 99.48\n",
      "ep:18600 loss: 0.3259 acc: 99.48\n",
      "ep:18700 loss: 0.3259 acc: 99.48\n",
      "ep:18800 loss: 0.3259 acc: 99.48\n",
      "ep:18900 loss: 0.3259 acc: 99.48\n",
      "ep:19000 loss: 0.3259 acc: 99.48\n",
      "ep:19100 loss: 0.3259 acc: 99.48\n",
      "ep:19200 loss: 0.3259 acc: 99.48\n",
      "ep:19300 loss: 0.3259 acc: 99.48\n",
      "ep:19400 loss: 0.3259 acc: 99.48\n",
      "ep:19500 loss: 0.3259 acc: 99.48\n",
      "ep:19600 loss: 0.3259 acc: 99.48\n",
      "ep:19700 loss: 0.3259 acc: 99.48\n",
      "ep:19800 loss: 0.3259 acc: 99.48\n",
      "ep:19900 loss: 0.3259 acc: 99.48\n",
      "hidden:12, acc:99.48453521728516 , epoch:19997\n",
      "ep:  100 loss: 0.6489 acc: 60.82\n",
      "ep:  200 loss: 0.5256 acc: 69.07\n",
      "ep:  300 loss: 0.4427 acc: 72.16\n",
      "ep:  400 loss: 0.3944 acc: 76.29\n",
      "ep:  500 loss: 0.3697 acc: 76.29\n",
      "ep:  600 loss: 0.3589 acc: 76.29\n",
      "ep:  700 loss: 0.3539 acc: 76.29\n",
      "ep:  800 loss: 0.3512 acc: 75.26\n",
      "ep:  900 loss: 0.3499 acc: 75.26\n",
      "ep: 1000 loss: 0.3490 acc: 75.26\n",
      "ep: 1100 loss: 0.3483 acc: 75.26\n",
      "ep: 1200 loss: 0.3476 acc: 75.26\n",
      "ep: 1300 loss: 0.3468 acc: 75.26\n",
      "ep: 1400 loss: 0.3461 acc: 75.26\n",
      "ep: 1500 loss: 0.3455 acc: 75.77\n",
      "ep: 1600 loss: 0.3452 acc: 76.80\n",
      "ep: 1700 loss: 0.3449 acc: 77.84\n",
      "ep: 1800 loss: 0.3446 acc: 78.35\n",
      "ep: 1900 loss: 0.3441 acc: 78.87\n",
      "ep: 2000 loss: 0.3431 acc: 81.44\n",
      "ep: 2100 loss: 0.3413 acc: 82.47\n",
      "ep: 2200 loss: 0.3388 acc: 86.60\n",
      "ep: 2300 loss: 0.3360 acc: 90.72\n",
      "ep: 2400 loss: 0.3339 acc: 90.72\n",
      "ep: 2500 loss: 0.3329 acc: 91.75\n",
      "ep: 2600 loss: 0.3312 acc: 89.18\n",
      "ep: 2700 loss: 0.3312 acc: 99.48\n",
      "ep: 2800 loss: 0.3299 acc: 99.48\n",
      "ep: 2900 loss: 0.3289 acc: 99.48\n",
      "ep: 3000 loss: 0.3281 acc: 99.48\n",
      "ep: 3100 loss: 0.3276 acc: 99.48\n",
      "ep: 3200 loss: 0.3272 acc: 99.48\n",
      "ep: 3300 loss: 0.3269 acc: 99.48\n",
      "ep: 3400 loss: 0.3267 acc: 99.48\n",
      "ep: 3500 loss: 0.3265 acc: 99.48\n",
      "ep: 3600 loss: 0.3264 acc: 99.48\n",
      "ep: 3700 loss: 0.3263 acc: 99.48\n",
      "ep: 3800 loss: 0.3263 acc: 99.48\n",
      "ep: 3900 loss: 0.3262 acc: 99.48\n",
      "ep: 4000 loss: 0.3262 acc: 99.48\n",
      "ep: 4100 loss: 0.3261 acc: 99.48\n",
      "ep: 4200 loss: 0.3261 acc: 99.48\n",
      "ep: 4300 loss: 0.3261 acc: 99.48\n",
      "ep: 4400 loss: 0.3260 acc: 99.48\n",
      "ep: 4500 loss: 0.3260 acc: 99.48\n",
      "ep: 4600 loss: 0.3260 acc: 99.48\n",
      "ep: 4700 loss: 0.3260 acc: 99.48\n",
      "ep: 4800 loss: 0.3260 acc: 99.48\n",
      "ep: 4900 loss: 0.3259 acc: 99.48\n",
      "ep: 5000 loss: 0.3259 acc: 99.48\n",
      "ep: 5100 loss: 0.3259 acc: 99.48\n",
      "ep: 5200 loss: 0.3259 acc: 99.48\n",
      "ep: 5300 loss: 0.3259 acc: 99.48\n",
      "ep: 5400 loss: 0.3259 acc: 99.48\n",
      "ep: 5500 loss: 0.3259 acc: 99.48\n",
      "ep: 5600 loss: 0.3259 acc: 99.48\n",
      "ep: 5700 loss: 0.3259 acc: 99.48\n",
      "ep: 5800 loss: 0.3259 acc: 99.48\n",
      "ep: 5900 loss: 0.3259 acc: 99.48\n",
      "ep: 6000 loss: 0.3259 acc: 99.48\n",
      "ep: 6100 loss: 0.3259 acc: 99.48\n",
      "ep: 6200 loss: 0.3259 acc: 99.48\n",
      "ep: 6300 loss: 0.3259 acc: 99.48\n",
      "ep: 6400 loss: 0.3259 acc: 99.48\n",
      "ep: 6500 loss: 0.3259 acc: 99.48\n",
      "ep: 6600 loss: 0.3259 acc: 99.48\n",
      "ep: 6700 loss: 0.3259 acc: 99.48\n",
      "ep: 6800 loss: 0.3259 acc: 99.48\n",
      "ep: 6900 loss: 0.3259 acc: 99.48\n",
      "ep: 7000 loss: 0.3259 acc: 99.48\n",
      "ep: 7100 loss: 0.3259 acc: 99.48\n",
      "ep: 7200 loss: 0.3259 acc: 99.48\n",
      "ep: 7300 loss: 0.3259 acc: 99.48\n",
      "ep: 7400 loss: 0.3259 acc: 99.48\n",
      "ep: 7500 loss: 0.3260 acc: 99.48\n",
      "ep: 7600 loss: 0.3259 acc: 99.48\n",
      "ep: 7700 loss: 0.3259 acc: 99.48\n",
      "ep: 7800 loss: 0.3259 acc: 99.48\n",
      "ep: 7900 loss: 0.3259 acc: 99.48\n",
      "ep: 8000 loss: 0.3259 acc: 99.48\n",
      "ep: 8100 loss: 0.3259 acc: 99.48\n",
      "ep: 8200 loss: 0.3259 acc: 99.48\n",
      "ep: 8300 loss: 0.3259 acc: 99.48\n",
      "ep: 8400 loss: 0.3259 acc: 99.48\n",
      "ep: 8500 loss: 0.3259 acc: 99.48\n",
      "ep: 8600 loss: 0.3259 acc: 99.48\n",
      "ep: 8700 loss: 0.3259 acc: 99.48\n",
      "ep: 8800 loss: 0.3259 acc: 99.48\n",
      "ep: 8900 loss: 0.3259 acc: 99.48\n",
      "ep: 9000 loss: 0.3258 acc: 99.48\n",
      "ep: 9100 loss: 0.3259 acc: 99.48\n",
      "ep: 9200 loss: 0.3259 acc: 99.48\n",
      "ep: 9300 loss: 0.3259 acc: 99.48\n",
      "ep: 9400 loss: 0.3259 acc: 99.48\n",
      "ep: 9500 loss: 0.3259 acc: 99.48\n",
      "ep: 9600 loss: 0.3260 acc: 99.48\n",
      "ep: 9700 loss: 0.3259 acc: 99.48\n",
      "ep: 9800 loss: 0.3259 acc: 99.48\n",
      "ep: 9900 loss: 0.3259 acc: 99.48\n",
      "ep:10000 loss: 0.3259 acc: 99.48\n",
      "ep:10100 loss: 0.3259 acc: 99.48\n",
      "ep:10200 loss: 0.3259 acc: 99.48\n",
      "ep:10300 loss: 0.3259 acc: 99.48\n",
      "ep:10400 loss: 0.3259 acc: 99.48\n",
      "ep:10500 loss: 0.3259 acc: 99.48\n",
      "ep:10600 loss: 0.3259 acc: 99.48\n",
      "ep:10700 loss: 0.3259 acc: 99.48\n",
      "ep:10800 loss: 0.3259 acc: 99.48\n",
      "ep:10900 loss: 0.3259 acc: 99.48\n",
      "ep:11000 loss: 0.3259 acc: 99.48\n",
      "ep:11100 loss: 0.3259 acc: 99.48\n",
      "ep:11200 loss: 0.3259 acc: 99.48\n",
      "ep:11300 loss: 0.3259 acc: 99.48\n",
      "ep:11400 loss: 0.3259 acc: 99.48\n",
      "ep:11500 loss: 0.3259 acc: 99.48\n",
      "ep:11600 loss: 0.3259 acc: 99.48\n",
      "ep:11700 loss: 0.3259 acc: 99.48\n",
      "ep:11800 loss: 0.3259 acc: 99.48\n",
      "ep:11900 loss: 0.3259 acc: 99.48\n",
      "ep:12000 loss: 0.3259 acc: 99.48\n",
      "ep:12100 loss: 0.3259 acc: 99.48\n",
      "ep:12200 loss: 0.3259 acc: 99.48\n",
      "ep:12300 loss: 0.3259 acc: 99.48\n",
      "ep:12400 loss: 0.3259 acc: 99.48\n",
      "ep:12500 loss: 0.3259 acc: 99.48\n",
      "ep:12600 loss: 0.3259 acc: 99.48\n",
      "ep:12700 loss: 0.3259 acc: 99.48\n",
      "ep:12800 loss: 0.3259 acc: 99.48\n",
      "ep:12900 loss: 0.3259 acc: 99.48\n",
      "ep:13000 loss: 0.3259 acc: 99.48\n",
      "ep:13100 loss: 0.3259 acc: 99.48\n",
      "ep:13200 loss: 0.3260 acc: 99.48\n",
      "ep:13300 loss: 0.3259 acc: 99.48\n",
      "ep:13400 loss: 0.3259 acc: 99.48\n",
      "ep:13500 loss: 0.3258 acc: 99.48\n",
      "ep:13600 loss: 0.3259 acc: 99.48\n",
      "ep:13700 loss: 0.3259 acc: 99.48\n",
      "ep:13800 loss: 0.3259 acc: 99.48\n",
      "ep:13900 loss: 0.3259 acc: 99.48\n",
      "ep:14000 loss: 0.3259 acc: 99.48\n",
      "ep:14100 loss: 0.3259 acc: 99.48\n",
      "ep:14200 loss: 0.3259 acc: 99.48\n",
      "ep:14300 loss: 0.3259 acc: 99.48\n",
      "ep:14400 loss: 0.3259 acc: 99.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:14500 loss: 0.3259 acc: 99.48\n",
      "ep:14600 loss: 0.3259 acc: 99.48\n",
      "ep:14700 loss: 0.3259 acc: 99.48\n",
      "ep:14800 loss: 0.3259 acc: 99.48\n",
      "ep:14900 loss: 0.3259 acc: 99.48\n",
      "ep:15000 loss: 0.3259 acc: 99.48\n",
      "ep:15100 loss: 0.3259 acc: 99.48\n",
      "ep:15200 loss: 0.3259 acc: 99.48\n",
      "ep:15300 loss: 0.3259 acc: 99.48\n",
      "ep:15400 loss: 0.3259 acc: 99.48\n",
      "ep:15500 loss: 0.3259 acc: 99.48\n",
      "ep:15600 loss: 0.3259 acc: 99.48\n",
      "ep:15700 loss: 0.3259 acc: 99.48\n",
      "ep:15800 loss: 0.3259 acc: 99.48\n",
      "ep:15900 loss: 0.3259 acc: 99.48\n",
      "ep:16000 loss: 0.3259 acc: 99.48\n",
      "ep:16100 loss: 0.3259 acc: 99.48\n",
      "ep:16200 loss: 0.3259 acc: 99.48\n",
      "ep:16300 loss: 0.3259 acc: 99.48\n",
      "ep:16400 loss: 0.3259 acc: 99.48\n",
      "ep:16500 loss: 0.3259 acc: 99.48\n",
      "ep:16600 loss: 0.3259 acc: 99.48\n",
      "ep:16700 loss: 0.3259 acc: 99.48\n",
      "ep:16800 loss: 0.3259 acc: 99.48\n",
      "ep:16900 loss: 0.3259 acc: 99.48\n",
      "ep:17000 loss: 0.3259 acc: 99.48\n",
      "ep:17100 loss: 0.3259 acc: 99.48\n",
      "ep:17200 loss: 0.3259 acc: 99.48\n",
      "ep:17300 loss: 0.3259 acc: 99.48\n",
      "ep:17400 loss: 0.3259 acc: 99.48\n",
      "ep:17500 loss: 0.3259 acc: 99.48\n",
      "ep:17600 loss: 0.3259 acc: 99.48\n",
      "ep:17700 loss: 0.3259 acc: 99.48\n",
      "ep:17800 loss: 0.3259 acc: 99.48\n",
      "ep:17900 loss: 0.3259 acc: 99.48\n",
      "ep:18000 loss: 0.3259 acc: 99.48\n",
      "ep:18100 loss: 0.3259 acc: 99.48\n",
      "ep:18200 loss: 0.3259 acc: 99.48\n",
      "ep:18300 loss: 0.3259 acc: 99.48\n",
      "ep:18400 loss: 0.3259 acc: 99.48\n",
      "ep:18500 loss: 0.3259 acc: 99.48\n",
      "ep:18600 loss: 0.3259 acc: 99.48\n",
      "ep:18700 loss: 0.3259 acc: 99.48\n",
      "ep:18800 loss: 0.3259 acc: 99.48\n",
      "ep:18900 loss: 0.3259 acc: 99.48\n",
      "ep:19000 loss: 0.3259 acc: 99.48\n",
      "ep:19100 loss: 0.3259 acc: 99.48\n",
      "ep:19200 loss: 0.3259 acc: 99.48\n",
      "ep:19300 loss: 0.3259 acc: 99.48\n",
      "ep:19400 loss: 0.3259 acc: 99.48\n",
      "ep:19500 loss: 0.3259 acc: 99.48\n",
      "ep:19600 loss: 0.3259 acc: 99.48\n",
      "ep:19700 loss: 0.3259 acc: 99.48\n",
      "ep:19800 loss: 0.3259 acc: 99.48\n",
      "ep:19900 loss: 0.3259 acc: 99.48\n",
      "hidden:13, acc:99.48453521728516 , epoch:19996\n",
      "ep:  100 loss: 0.6578 acc: 61.86\n",
      "ep:  200 loss: 0.6110 acc: 58.76\n",
      "ep:  300 loss: 0.4870 acc: 65.98\n",
      "ep:  400 loss: 0.4415 acc: 73.71\n",
      "ep:  500 loss: 0.3922 acc: 77.32\n",
      "ep:  600 loss: 0.3656 acc: 77.32\n",
      "ep:  700 loss: 0.3535 acc: 77.84\n",
      "ep:  800 loss: 0.3458 acc: 78.87\n",
      "ep:  900 loss: 0.3395 acc: 82.99\n",
      "ep: 1000 loss: 0.3238 acc: 83.51\n",
      "ep: 1100 loss: 0.3191 acc: 83.51\n",
      "ep: 1200 loss: 0.3178 acc: 83.51\n",
      "ep: 1300 loss: 0.3171 acc: 83.51\n",
      "ep: 1400 loss: 0.3167 acc: 83.51\n",
      "ep: 1500 loss: 0.3164 acc: 83.51\n",
      "ep: 1600 loss: 0.3163 acc: 83.51\n",
      "ep: 1700 loss: 0.3161 acc: 83.51\n",
      "ep: 1800 loss: 0.3160 acc: 83.51\n",
      "ep: 1900 loss: 0.3159 acc: 83.51\n",
      "ep: 2000 loss: 0.3158 acc: 83.51\n",
      "ep: 2100 loss: 0.3157 acc: 83.51\n",
      "ep: 2200 loss: 0.3157 acc: 83.51\n",
      "ep: 2300 loss: 0.3156 acc: 83.51\n",
      "ep: 2400 loss: 0.3156 acc: 83.51\n",
      "ep: 2500 loss: 0.3155 acc: 83.51\n",
      "ep: 2600 loss: 0.3155 acc: 83.51\n",
      "ep: 2700 loss: 0.3155 acc: 83.51\n",
      "ep: 2800 loss: 0.3154 acc: 83.51\n",
      "ep: 2900 loss: 0.3154 acc: 83.51\n",
      "ep: 3000 loss: 0.3154 acc: 83.51\n",
      "ep: 3100 loss: 0.3154 acc: 83.51\n",
      "ep: 3200 loss: 0.3154 acc: 83.51\n",
      "ep: 3300 loss: 0.3154 acc: 83.51\n",
      "ep: 3400 loss: 0.3154 acc: 83.51\n",
      "ep: 3500 loss: 0.3154 acc: 83.51\n",
      "ep: 3600 loss: 0.3154 acc: 83.51\n",
      "ep: 3700 loss: 0.3154 acc: 83.51\n",
      "ep: 3800 loss: 0.3154 acc: 83.51\n",
      "ep: 3900 loss: 0.3154 acc: 83.51\n",
      "ep: 4000 loss: 0.3154 acc: 83.51\n",
      "ep: 4100 loss: 0.3154 acc: 83.51\n",
      "ep: 4200 loss: 0.3154 acc: 83.51\n",
      "ep: 4300 loss: 0.3154 acc: 83.51\n",
      "ep: 4400 loss: 0.3154 acc: 83.51\n",
      "ep: 4500 loss: 0.3154 acc: 83.51\n",
      "ep: 4600 loss: 0.3154 acc: 83.51\n",
      "ep: 4700 loss: 0.3154 acc: 83.51\n",
      "ep: 4800 loss: 0.3154 acc: 83.51\n",
      "ep: 4900 loss: 0.3154 acc: 83.51\n",
      "ep: 5000 loss: 0.3154 acc: 83.51\n",
      "ep: 5100 loss: 0.3154 acc: 83.51\n",
      "ep: 5200 loss: 0.3154 acc: 83.51\n",
      "ep: 5300 loss: 0.3154 acc: 83.51\n",
      "ep: 5400 loss: 0.3154 acc: 83.51\n",
      "ep: 5500 loss: 0.3154 acc: 83.51\n",
      "ep: 5600 loss: 0.3154 acc: 83.51\n",
      "ep: 5700 loss: 0.3154 acc: 83.51\n",
      "ep: 5800 loss: 0.3154 acc: 83.51\n",
      "ep: 5900 loss: 0.3154 acc: 83.51\n",
      "ep: 6000 loss: 0.3154 acc: 83.51\n",
      "ep: 6100 loss: 0.3154 acc: 83.51\n",
      "ep: 6200 loss: 0.3154 acc: 83.51\n",
      "ep: 6300 loss: 0.3154 acc: 83.51\n",
      "ep: 6400 loss: 0.3154 acc: 83.51\n",
      "ep: 6500 loss: 0.3154 acc: 83.51\n",
      "ep: 6600 loss: 0.3154 acc: 83.51\n",
      "ep: 6700 loss: 0.3154 acc: 83.51\n",
      "ep: 6800 loss: 0.3154 acc: 83.51\n",
      "ep: 6900 loss: 0.3154 acc: 83.51\n",
      "ep: 7000 loss: 0.3154 acc: 83.51\n",
      "ep: 7100 loss: 0.3154 acc: 83.51\n",
      "ep: 7200 loss: 0.3154 acc: 83.51\n",
      "ep: 7300 loss: 0.3154 acc: 83.51\n",
      "ep: 7400 loss: 0.3154 acc: 83.51\n",
      "ep: 7500 loss: 0.3154 acc: 83.51\n",
      "ep: 7600 loss: 0.3154 acc: 83.51\n",
      "ep: 7700 loss: 0.3154 acc: 83.51\n",
      "ep: 7800 loss: 0.3154 acc: 83.51\n",
      "ep: 7900 loss: 0.3154 acc: 83.51\n",
      "ep: 8000 loss: 0.3154 acc: 83.51\n",
      "ep: 8100 loss: 0.3154 acc: 83.51\n",
      "ep: 8200 loss: 0.3154 acc: 83.51\n",
      "ep: 8300 loss: 0.3154 acc: 83.51\n",
      "ep: 8400 loss: 0.3154 acc: 83.51\n",
      "ep: 8500 loss: 0.3154 acc: 83.51\n",
      "ep: 8600 loss: 0.3154 acc: 83.51\n",
      "ep: 8700 loss: 0.3154 acc: 83.51\n",
      "ep: 8800 loss: 0.3154 acc: 83.51\n",
      "ep: 8900 loss: 0.3154 acc: 83.51\n",
      "ep: 9000 loss: 0.3154 acc: 83.51\n",
      "ep: 9100 loss: 0.3154 acc: 83.51\n",
      "ep: 9200 loss: 0.3154 acc: 83.51\n",
      "ep: 9300 loss: 0.3154 acc: 83.51\n",
      "ep: 9400 loss: 0.3154 acc: 83.51\n",
      "ep: 9500 loss: 0.3154 acc: 83.51\n",
      "ep: 9600 loss: 0.3154 acc: 83.51\n",
      "ep: 9700 loss: 0.3154 acc: 83.51\n",
      "ep: 9800 loss: 0.3154 acc: 83.51\n",
      "ep: 9900 loss: 0.3154 acc: 83.51\n",
      "ep:10000 loss: 0.3154 acc: 83.51\n",
      "ep:10100 loss: 0.3154 acc: 83.51\n",
      "ep:10200 loss: 0.3154 acc: 83.51\n",
      "ep:10300 loss: 0.3154 acc: 83.51\n",
      "ep:10400 loss: 0.3154 acc: 83.51\n",
      "ep:10500 loss: 0.3154 acc: 83.51\n",
      "ep:10600 loss: 0.3154 acc: 83.51\n",
      "ep:10700 loss: 0.3154 acc: 83.51\n",
      "ep:10800 loss: 0.3154 acc: 83.51\n",
      "ep:10900 loss: 0.3154 acc: 83.51\n",
      "ep:11000 loss: 0.3154 acc: 83.51\n",
      "ep:11100 loss: 0.3154 acc: 83.51\n",
      "ep:11200 loss: 0.3154 acc: 83.51\n",
      "ep:11300 loss: 0.3154 acc: 83.51\n",
      "ep:11400 loss: 0.3154 acc: 83.51\n",
      "ep:11500 loss: 0.3154 acc: 83.51\n",
      "ep:11600 loss: 0.3154 acc: 83.51\n",
      "ep:11700 loss: 0.3154 acc: 83.51\n",
      "ep:11800 loss: 0.3154 acc: 83.51\n",
      "ep:11900 loss: 0.3154 acc: 83.51\n",
      "ep:12000 loss: 0.3154 acc: 83.51\n",
      "ep:12100 loss: 0.3154 acc: 83.51\n",
      "ep:12200 loss: 0.3154 acc: 83.51\n",
      "ep:12300 loss: 0.3154 acc: 83.51\n",
      "ep:12400 loss: 0.3154 acc: 83.51\n",
      "ep:12500 loss: 0.3154 acc: 83.51\n",
      "ep:12600 loss: 0.3154 acc: 83.51\n",
      "ep:12700 loss: 0.3154 acc: 83.51\n",
      "ep:12800 loss: 0.3154 acc: 83.51\n",
      "ep:12900 loss: 0.3154 acc: 83.51\n",
      "ep:13000 loss: 0.3154 acc: 83.51\n",
      "ep:13100 loss: 0.3154 acc: 83.51\n",
      "ep:13200 loss: 0.3154 acc: 83.51\n",
      "ep:13300 loss: 0.3154 acc: 83.51\n",
      "ep:13400 loss: 0.3154 acc: 83.51\n",
      "ep:13500 loss: 0.3154 acc: 83.51\n",
      "ep:13600 loss: 0.3154 acc: 83.51\n",
      "ep:13700 loss: 0.3154 acc: 83.51\n",
      "ep:13800 loss: 0.3154 acc: 83.51\n",
      "ep:13900 loss: 0.3154 acc: 83.51\n",
      "ep:14000 loss: 0.3154 acc: 83.51\n",
      "ep:14100 loss: 0.3154 acc: 83.51\n",
      "ep:14200 loss: 0.3154 acc: 83.51\n",
      "ep:14300 loss: 0.3154 acc: 83.51\n",
      "ep:14400 loss: 0.3154 acc: 83.51\n",
      "ep:14500 loss: 0.3154 acc: 83.51\n",
      "ep:14600 loss: 0.3154 acc: 83.51\n",
      "ep:14700 loss: 0.3154 acc: 83.51\n",
      "ep:14800 loss: 0.3154 acc: 83.51\n",
      "ep:14900 loss: 0.3154 acc: 83.51\n",
      "ep:15000 loss: 0.3154 acc: 83.51\n",
      "ep:15100 loss: 0.3154 acc: 83.51\n",
      "ep:15200 loss: 0.3154 acc: 83.51\n",
      "ep:15300 loss: 0.3154 acc: 83.51\n",
      "ep:15400 loss: 0.3154 acc: 83.51\n",
      "ep:15500 loss: 0.3154 acc: 83.51\n",
      "ep:15600 loss: 0.3154 acc: 83.51\n",
      "ep:15700 loss: 0.3154 acc: 83.51\n",
      "ep:15800 loss: 0.3154 acc: 83.51\n",
      "ep:15900 loss: 0.3154 acc: 83.51\n",
      "ep:16000 loss: 0.3154 acc: 83.51\n",
      "ep:16100 loss: 0.3154 acc: 83.51\n",
      "ep:16200 loss: 0.3154 acc: 83.51\n",
      "ep:16300 loss: 0.3154 acc: 83.51\n",
      "ep:16400 loss: 0.3154 acc: 83.51\n",
      "ep:16500 loss: 0.3154 acc: 83.51\n",
      "ep:16600 loss: 0.3154 acc: 83.51\n",
      "ep:16700 loss: 0.3154 acc: 83.51\n",
      "ep:16800 loss: 0.3154 acc: 83.51\n",
      "ep:16900 loss: 0.3154 acc: 83.51\n",
      "ep:17000 loss: 0.3154 acc: 83.51\n",
      "ep:17100 loss: 0.3154 acc: 83.51\n",
      "ep:17200 loss: 0.3154 acc: 83.51\n",
      "ep:17300 loss: 0.3154 acc: 83.51\n",
      "ep:17400 loss: 0.3154 acc: 83.51\n",
      "ep:17500 loss: 0.3154 acc: 83.51\n",
      "ep:17600 loss: 0.3154 acc: 83.51\n",
      "ep:17700 loss: 0.3154 acc: 83.51\n",
      "ep:17800 loss: 0.3154 acc: 83.51\n",
      "ep:17900 loss: 0.3154 acc: 83.51\n",
      "ep:18000 loss: 0.3154 acc: 83.51\n",
      "ep:18100 loss: 0.3154 acc: 83.51\n",
      "ep:18200 loss: 0.3154 acc: 83.51\n",
      "ep:18300 loss: 0.3154 acc: 83.51\n",
      "ep:18400 loss: 0.3154 acc: 83.51\n",
      "ep:18500 loss: 0.3154 acc: 83.51\n",
      "ep:18600 loss: 0.3154 acc: 83.51\n",
      "ep:18700 loss: 0.3154 acc: 83.51\n",
      "ep:18800 loss: 0.3154 acc: 83.51\n",
      "ep:18900 loss: 0.3154 acc: 83.51\n",
      "ep:19000 loss: 0.3154 acc: 83.51\n",
      "ep:19100 loss: 0.3154 acc: 83.51\n",
      "ep:19200 loss: 0.3154 acc: 83.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:19300 loss: 0.3154 acc: 83.51\n",
      "ep:19400 loss: 0.3154 acc: 83.51\n",
      "ep:19500 loss: 0.3154 acc: 83.51\n",
      "ep:19600 loss: 0.3154 acc: 83.51\n",
      "ep:19700 loss: 0.3154 acc: 83.51\n",
      "ep:19800 loss: 0.3154 acc: 83.51\n",
      "ep:19900 loss: 0.3154 acc: 83.51\n",
      "hidden:14, acc:83.50515747070312 , epoch:19995\n",
      "ep:  100 loss: 0.6554 acc: 60.82\n",
      "ep:  200 loss: 0.5635 acc: 67.01\n",
      "ep:  300 loss: 0.4446 acc: 71.65\n",
      "ep:  400 loss: 0.3871 acc: 76.80\n",
      "ep:  500 loss: 0.3620 acc: 77.32\n",
      "ep:  600 loss: 0.3533 acc: 76.80\n",
      "ep:  700 loss: 0.3501 acc: 75.77\n",
      "ep:  800 loss: 0.3485 acc: 75.77\n",
      "ep:  900 loss: 0.3476 acc: 75.77\n",
      "ep: 1000 loss: 0.3470 acc: 75.77\n",
      "ep: 1100 loss: 0.3467 acc: 75.77\n",
      "ep: 1200 loss: 0.3466 acc: 75.26\n",
      "ep: 1300 loss: 0.3465 acc: 75.26\n",
      "ep: 1400 loss: 0.3462 acc: 75.26\n",
      "ep: 1500 loss: 0.3459 acc: 76.80\n",
      "ep: 1600 loss: 0.3455 acc: 77.84\n",
      "ep: 1700 loss: 0.3451 acc: 78.35\n",
      "ep: 1800 loss: 0.3447 acc: 77.84\n",
      "ep: 1900 loss: 0.3440 acc: 78.35\n",
      "ep: 2000 loss: 0.3436 acc: 78.35\n",
      "ep: 2100 loss: 0.3432 acc: 78.87\n",
      "ep: 2200 loss: 0.3427 acc: 78.87\n",
      "ep: 2300 loss: 0.3423 acc: 79.38\n",
      "ep: 2400 loss: 0.3417 acc: 79.38\n",
      "ep: 2500 loss: 0.3407 acc: 81.44\n",
      "ep: 2600 loss: 0.3391 acc: 81.96\n",
      "ep: 2700 loss: 0.3371 acc: 81.96\n",
      "ep: 2800 loss: 0.3349 acc: 81.96\n",
      "ep: 2900 loss: 0.3330 acc: 82.47\n",
      "ep: 3000 loss: 0.3331 acc: 85.57\n",
      "ep: 3100 loss: 0.3331 acc: 90.72\n",
      "ep: 3200 loss: 0.3318 acc: 90.72\n",
      "ep: 3300 loss: 0.3313 acc: 91.75\n",
      "ep: 3400 loss: 0.3306 acc: 96.91\n",
      "ep: 3500 loss: 0.3302 acc: 98.97\n",
      "ep: 3600 loss: 0.3291 acc: 99.48\n",
      "ep: 3700 loss: 0.3283 acc: 99.48\n",
      "ep: 3800 loss: 0.3276 acc: 99.48\n",
      "ep: 3900 loss: 0.3272 acc: 99.48\n",
      "ep: 4000 loss: 0.3269 acc: 99.48\n",
      "ep: 4100 loss: 0.3266 acc: 99.48\n",
      "ep: 4200 loss: 0.3265 acc: 99.48\n",
      "ep: 4300 loss: 0.3264 acc: 99.48\n",
      "ep: 4400 loss: 0.3263 acc: 99.48\n",
      "ep: 4500 loss: 0.3262 acc: 99.48\n",
      "ep: 4600 loss: 0.3262 acc: 99.48\n",
      "ep: 4700 loss: 0.3261 acc: 99.48\n",
      "ep: 4800 loss: 0.3261 acc: 99.48\n",
      "ep: 4900 loss: 0.3261 acc: 99.48\n",
      "ep: 5000 loss: 0.3260 acc: 99.48\n",
      "ep: 5100 loss: 0.3260 acc: 99.48\n",
      "ep: 5200 loss: 0.3260 acc: 99.48\n",
      "ep: 5300 loss: 0.3260 acc: 99.48\n",
      "ep: 5400 loss: 0.3260 acc: 99.48\n",
      "ep: 5500 loss: 0.3259 acc: 99.48\n",
      "ep: 5600 loss: 0.3259 acc: 99.48\n",
      "ep: 5700 loss: 0.3259 acc: 99.48\n",
      "ep: 5800 loss: 0.3259 acc: 99.48\n",
      "ep: 5900 loss: 0.3259 acc: 99.48\n",
      "ep: 6000 loss: 0.3259 acc: 99.48\n",
      "ep: 6100 loss: 0.3259 acc: 99.48\n",
      "ep: 6200 loss: 0.3259 acc: 99.48\n",
      "ep: 6300 loss: 0.3259 acc: 99.48\n",
      "ep: 6400 loss: 0.3259 acc: 99.48\n",
      "ep: 6500 loss: 0.3259 acc: 99.48\n",
      "ep: 6600 loss: 0.3259 acc: 99.48\n",
      "ep: 6700 loss: 0.3259 acc: 99.48\n",
      "ep: 6800 loss: 0.3259 acc: 99.48\n",
      "ep: 6900 loss: 0.3259 acc: 99.48\n",
      "ep: 7000 loss: 0.3259 acc: 99.48\n",
      "ep: 7100 loss: 0.3259 acc: 99.48\n",
      "ep: 7200 loss: 0.3273 acc: 99.48\n",
      "ep: 7300 loss: 0.3259 acc: 99.48\n",
      "ep: 7400 loss: 0.3259 acc: 99.48\n",
      "ep: 7500 loss: 0.3259 acc: 99.48\n",
      "ep: 7600 loss: 0.3259 acc: 99.48\n",
      "ep: 7700 loss: 0.3259 acc: 99.48\n",
      "ep: 7800 loss: 0.3259 acc: 99.48\n",
      "ep: 7900 loss: 0.3259 acc: 99.48\n",
      "ep: 8000 loss: 0.3259 acc: 99.48\n",
      "ep: 8100 loss: 0.3259 acc: 99.48\n",
      "ep: 8200 loss: 0.3256 acc: 99.48\n",
      "ep: 8300 loss: 0.3258 acc: 99.48\n",
      "ep: 8400 loss: 0.3258 acc: 99.48\n",
      "ep: 8500 loss: 0.3259 acc: 99.48\n",
      "ep: 8600 loss: 0.3259 acc: 99.48\n",
      "ep: 8700 loss: 0.3259 acc: 99.48\n",
      "ep: 8800 loss: 0.3259 acc: 99.48\n",
      "ep: 8900 loss: 0.3259 acc: 99.48\n",
      "ep: 9000 loss: 0.3259 acc: 99.48\n",
      "ep: 9100 loss: 0.3259 acc: 99.48\n",
      "ep: 9200 loss: 0.3259 acc: 99.48\n",
      "ep: 9300 loss: 0.3259 acc: 99.48\n",
      "ep: 9400 loss: 0.3259 acc: 99.48\n",
      "ep: 9500 loss: 0.3259 acc: 99.48\n",
      "ep: 9600 loss: 0.3259 acc: 99.48\n",
      "ep: 9700 loss: 0.3259 acc: 99.48\n",
      "ep: 9800 loss: 0.3259 acc: 99.48\n",
      "ep: 9900 loss: 0.3259 acc: 99.48\n",
      "ep:10000 loss: 0.3259 acc: 99.48\n",
      "ep:10100 loss: 0.3259 acc: 99.48\n",
      "ep:10200 loss: 0.3259 acc: 99.48\n",
      "ep:10300 loss: 0.3259 acc: 99.48\n",
      "ep:10400 loss: 0.3259 acc: 99.48\n",
      "ep:10500 loss: 0.3259 acc: 99.48\n",
      "ep:10600 loss: 0.3257 acc: 99.48\n",
      "ep:10700 loss: 0.3258 acc: 99.48\n",
      "ep:10800 loss: 0.3258 acc: 99.48\n",
      "ep:10900 loss: 0.3259 acc: 99.48\n",
      "ep:11000 loss: 0.3259 acc: 99.48\n",
      "ep:11100 loss: 0.3259 acc: 99.48\n",
      "ep:11200 loss: 0.3259 acc: 99.48\n",
      "ep:11300 loss: 0.3259 acc: 99.48\n",
      "ep:11400 loss: 0.3259 acc: 99.48\n",
      "ep:11500 loss: 0.3259 acc: 99.48\n",
      "ep:11600 loss: 0.3259 acc: 99.48\n",
      "ep:11700 loss: 0.3259 acc: 99.48\n",
      "ep:11800 loss: 0.3259 acc: 99.48\n",
      "ep:11900 loss: 0.3259 acc: 99.48\n",
      "ep:12000 loss: 0.3259 acc: 99.48\n",
      "ep:12100 loss: 0.3259 acc: 99.48\n",
      "ep:12200 loss: 0.3259 acc: 99.48\n",
      "ep:12300 loss: 0.3259 acc: 99.48\n",
      "ep:12400 loss: 0.3259 acc: 99.48\n",
      "ep:12500 loss: 0.3259 acc: 99.48\n",
      "ep:12600 loss: 0.3259 acc: 99.48\n",
      "ep:12700 loss: 0.3259 acc: 99.48\n",
      "ep:12800 loss: 0.3259 acc: 99.48\n",
      "ep:12900 loss: 0.3259 acc: 99.48\n",
      "ep:13000 loss: 0.3259 acc: 99.48\n",
      "ep:13100 loss: 0.3259 acc: 99.48\n",
      "ep:13200 loss: 0.3259 acc: 99.48\n",
      "ep:13300 loss: 0.3259 acc: 99.48\n",
      "ep:13400 loss: 0.3259 acc: 99.48\n",
      "ep:13500 loss: 0.3259 acc: 99.48\n",
      "ep:13600 loss: 0.3259 acc: 99.48\n",
      "ep:13700 loss: 0.3259 acc: 99.48\n",
      "ep:13800 loss: 0.3259 acc: 99.48\n",
      "ep:13900 loss: 0.3259 acc: 99.48\n",
      "ep:14000 loss: 0.3259 acc: 99.48\n",
      "ep:14100 loss: 0.3259 acc: 99.48\n",
      "ep:14200 loss: 0.3259 acc: 99.48\n",
      "ep:14300 loss: 0.3259 acc: 99.48\n",
      "ep:14400 loss: 0.3259 acc: 99.48\n",
      "ep:14500 loss: 0.3259 acc: 99.48\n",
      "ep:14600 loss: 0.3259 acc: 99.48\n",
      "ep:14700 loss: 0.3259 acc: 99.48\n",
      "ep:14800 loss: 0.3259 acc: 99.48\n",
      "ep:14900 loss: 0.3259 acc: 99.48\n",
      "ep:15000 loss: 0.3259 acc: 99.48\n",
      "ep:15100 loss: 0.3259 acc: 99.48\n",
      "ep:15200 loss: 0.3259 acc: 99.48\n",
      "ep:15300 loss: 0.3259 acc: 99.48\n",
      "ep:15400 loss: 0.3259 acc: 99.48\n",
      "ep:15500 loss: 0.3259 acc: 99.48\n",
      "ep:15600 loss: 0.3259 acc: 99.48\n",
      "ep:15700 loss: 0.3259 acc: 99.48\n",
      "ep:15800 loss: 0.3259 acc: 99.48\n",
      "ep:15900 loss: 0.3259 acc: 99.48\n",
      "ep:16000 loss: 0.3259 acc: 99.48\n",
      "ep:16100 loss: 0.3259 acc: 99.48\n",
      "ep:16200 loss: 0.3259 acc: 99.48\n",
      "ep:16300 loss: 0.3259 acc: 99.48\n",
      "ep:16400 loss: 0.3259 acc: 99.48\n",
      "ep:16500 loss: 0.3259 acc: 99.48\n",
      "ep:16600 loss: 0.3259 acc: 99.48\n",
      "ep:16700 loss: 0.3259 acc: 99.48\n",
      "ep:16800 loss: 0.3259 acc: 99.48\n",
      "ep:16900 loss: 0.3259 acc: 99.48\n",
      "ep:17000 loss: 0.3259 acc: 99.48\n",
      "ep:17100 loss: 0.3259 acc: 99.48\n",
      "ep:17200 loss: 0.3259 acc: 99.48\n",
      "ep:17300 loss: 0.3259 acc: 99.48\n",
      "ep:17400 loss: 0.3259 acc: 99.48\n",
      "ep:17500 loss: 0.3259 acc: 99.48\n",
      "ep:17600 loss: 0.3259 acc: 99.48\n",
      "ep:17700 loss: 0.3259 acc: 99.48\n",
      "ep:17800 loss: 0.3259 acc: 99.48\n",
      "ep:17900 loss: 0.3259 acc: 99.48\n",
      "ep:18000 loss: 0.3259 acc: 99.48\n",
      "ep:18100 loss: 0.3259 acc: 99.48\n",
      "ep:18200 loss: 0.3259 acc: 99.48\n",
      "ep:18300 loss: 0.3259 acc: 99.48\n",
      "ep:18400 loss: 0.3259 acc: 99.48\n",
      "ep:18500 loss: 0.3259 acc: 99.48\n",
      "ep:18600 loss: 0.3259 acc: 99.48\n",
      "ep:18700 loss: 0.3259 acc: 99.48\n",
      "ep:18800 loss: 0.3259 acc: 99.48\n",
      "ep:18900 loss: 0.3259 acc: 99.48\n",
      "ep:19000 loss: 0.3259 acc: 99.48\n",
      "ep:19100 loss: 0.3259 acc: 99.48\n",
      "ep:19200 loss: 0.3258 acc: 99.48\n",
      "ep:19300 loss: 0.3259 acc: 99.48\n",
      "ep:19400 loss: 0.3259 acc: 99.48\n",
      "ep:19500 loss: 0.3259 acc: 99.48\n",
      "ep:19600 loss: 0.3259 acc: 99.48\n",
      "ep:19700 loss: 0.3259 acc: 99.48\n",
      "ep:19800 loss: 0.3259 acc: 99.48\n",
      "ep:19900 loss: 0.3259 acc: 99.48\n",
      "hidden:15, acc:99.48453521728516 , epoch:19994\n",
      "ep:  100 loss: 0.6422 acc: 61.86\n",
      "ep:  200 loss: 0.5494 acc: 67.53\n",
      "ep:  300 loss: 0.4524 acc: 72.16\n",
      "ep:  400 loss: 0.3897 acc: 76.29\n",
      "ep:  500 loss: 0.3619 acc: 76.29\n",
      "ep:  600 loss: 0.3537 acc: 75.77\n",
      "ep:  700 loss: 0.3508 acc: 75.77\n",
      "ep:  800 loss: 0.3494 acc: 75.26\n",
      "ep:  900 loss: 0.3487 acc: 75.26\n",
      "ep: 1000 loss: 0.3481 acc: 75.26\n",
      "ep: 1100 loss: 0.3474 acc: 74.74\n",
      "ep: 1200 loss: 0.3467 acc: 76.80\n",
      "ep: 1300 loss: 0.3462 acc: 78.35\n",
      "ep: 1400 loss: 0.3456 acc: 78.87\n",
      "ep: 1500 loss: 0.3450 acc: 79.38\n",
      "ep: 1600 loss: 0.3440 acc: 80.41\n",
      "ep: 1700 loss: 0.3425 acc: 81.96\n",
      "ep: 1800 loss: 0.3405 acc: 81.96\n",
      "ep: 1900 loss: 0.3388 acc: 82.47\n",
      "ep: 2000 loss: 0.3374 acc: 86.60\n",
      "ep: 2100 loss: 0.3349 acc: 90.72\n",
      "ep: 2200 loss: 0.3327 acc: 90.72\n",
      "ep: 2300 loss: 0.3314 acc: 93.30\n",
      "ep: 2400 loss: 0.3303 acc: 99.48\n",
      "ep: 2500 loss: 0.3292 acc: 99.48\n",
      "ep: 2600 loss: 0.3282 acc: 99.48\n",
      "ep: 2700 loss: 0.3276 acc: 99.48\n",
      "ep: 2800 loss: 0.3271 acc: 99.48\n",
      "ep: 2900 loss: 0.3268 acc: 99.48\n",
      "ep: 3000 loss: 0.3266 acc: 99.48\n",
      "ep: 3100 loss: 0.3265 acc: 99.48\n",
      "ep: 3200 loss: 0.3264 acc: 99.48\n",
      "ep: 3300 loss: 0.3263 acc: 99.48\n",
      "ep: 3400 loss: 0.3262 acc: 99.48\n",
      "ep: 3500 loss: 0.3262 acc: 99.48\n",
      "ep: 3600 loss: 0.3261 acc: 99.48\n",
      "ep: 3700 loss: 0.3261 acc: 99.48\n",
      "ep: 3800 loss: 0.3261 acc: 99.48\n",
      "ep: 3900 loss: 0.3260 acc: 99.48\n",
      "ep: 4000 loss: 0.3260 acc: 99.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4100 loss: 0.3260 acc: 99.48\n",
      "ep: 4200 loss: 0.3260 acc: 99.48\n",
      "ep: 4300 loss: 0.3260 acc: 99.48\n",
      "ep: 4400 loss: 0.3259 acc: 99.48\n",
      "ep: 4500 loss: 0.3259 acc: 99.48\n",
      "ep: 4600 loss: 0.3259 acc: 99.48\n",
      "ep: 4700 loss: 0.3259 acc: 99.48\n",
      "ep: 4800 loss: 0.3259 acc: 99.48\n",
      "ep: 4900 loss: 0.3259 acc: 99.48\n",
      "ep: 5000 loss: 0.3259 acc: 99.48\n",
      "ep: 5100 loss: 0.3259 acc: 99.48\n",
      "ep: 5200 loss: 0.3259 acc: 99.48\n",
      "ep: 5300 loss: 0.3259 acc: 99.48\n",
      "ep: 5400 loss: 0.3259 acc: 99.48\n",
      "ep: 5500 loss: 0.3259 acc: 99.48\n",
      "ep: 5600 loss: 0.3259 acc: 99.48\n",
      "ep: 5700 loss: 0.3259 acc: 99.48\n",
      "ep: 5800 loss: 0.3259 acc: 99.48\n",
      "ep: 5900 loss: 0.3259 acc: 99.48\n",
      "ep: 6000 loss: 0.3259 acc: 99.48\n",
      "ep: 6100 loss: 0.3259 acc: 99.48\n",
      "ep: 6200 loss: 0.3259 acc: 99.48\n",
      "ep: 6300 loss: 0.3259 acc: 99.48\n",
      "ep: 6400 loss: 0.3259 acc: 99.48\n",
      "ep: 6500 loss: 0.3259 acc: 99.48\n",
      "ep: 6600 loss: 0.3259 acc: 99.48\n",
      "ep: 6700 loss: 0.3259 acc: 99.48\n",
      "ep: 6800 loss: 0.3259 acc: 99.48\n",
      "ep: 6900 loss: 0.3259 acc: 99.48\n",
      "ep: 7000 loss: 0.3260 acc: 99.48\n",
      "ep: 7100 loss: 0.3259 acc: 99.48\n",
      "ep: 7200 loss: 0.3259 acc: 99.48\n",
      "ep: 7300 loss: 0.3259 acc: 99.48\n",
      "ep: 7400 loss: 0.3260 acc: 99.48\n",
      "ep: 7500 loss: 0.3259 acc: 99.48\n",
      "ep: 7600 loss: 0.3259 acc: 99.48\n",
      "ep: 7700 loss: 0.3259 acc: 99.48\n",
      "ep: 7800 loss: 0.3259 acc: 99.48\n",
      "ep: 7900 loss: 0.3259 acc: 99.48\n",
      "ep: 8000 loss: 0.3259 acc: 99.48\n",
      "ep: 8100 loss: 0.3259 acc: 99.48\n",
      "ep: 8200 loss: 0.3259 acc: 99.48\n",
      "ep: 8300 loss: 0.3259 acc: 99.48\n",
      "ep: 8400 loss: 0.3259 acc: 99.48\n",
      "ep: 8500 loss: 0.3259 acc: 99.48\n",
      "ep: 8600 loss: 0.3259 acc: 99.48\n",
      "ep: 8700 loss: 0.3259 acc: 99.48\n",
      "ep: 8800 loss: 0.3259 acc: 99.48\n",
      "ep: 8900 loss: 0.3259 acc: 99.48\n",
      "ep: 9000 loss: 0.3259 acc: 99.48\n",
      "ep: 9100 loss: 0.3259 acc: 99.48\n",
      "ep: 9200 loss: 0.3259 acc: 99.48\n",
      "ep: 9300 loss: 0.3259 acc: 99.48\n",
      "ep: 9400 loss: 0.3259 acc: 99.48\n",
      "ep: 9500 loss: 0.3259 acc: 99.48\n",
      "ep: 9600 loss: 0.3259 acc: 99.48\n",
      "ep: 9700 loss: 0.3259 acc: 99.48\n",
      "ep: 9800 loss: 0.3259 acc: 99.48\n",
      "ep: 9900 loss: 0.3259 acc: 99.48\n",
      "ep:10000 loss: 0.3259 acc: 99.48\n",
      "ep:10100 loss: 0.3260 acc: 99.48\n",
      "ep:10200 loss: 0.3259 acc: 99.48\n",
      "ep:10300 loss: 0.3259 acc: 99.48\n",
      "ep:10400 loss: 0.3259 acc: 99.48\n",
      "ep:10500 loss: 0.3258 acc: 99.48\n",
      "ep:10600 loss: 0.3259 acc: 99.48\n",
      "ep:10700 loss: 0.3259 acc: 99.48\n",
      "ep:10800 loss: 0.3259 acc: 99.48\n",
      "ep:10900 loss: 0.3259 acc: 99.48\n",
      "ep:11000 loss: 0.3259 acc: 99.48\n",
      "ep:11100 loss: 0.3259 acc: 99.48\n",
      "ep:11200 loss: 0.3259 acc: 99.48\n",
      "ep:11300 loss: 0.3259 acc: 99.48\n",
      "ep:11400 loss: 0.3259 acc: 99.48\n",
      "ep:11500 loss: 0.3259 acc: 99.48\n",
      "ep:11600 loss: 0.3259 acc: 99.48\n",
      "ep:11700 loss: 0.3259 acc: 99.48\n",
      "ep:11800 loss: 0.3259 acc: 99.48\n",
      "ep:11900 loss: 0.3259 acc: 99.48\n",
      "ep:12000 loss: 0.3259 acc: 99.48\n",
      "ep:12100 loss: 0.3259 acc: 99.48\n",
      "ep:12200 loss: 0.3259 acc: 99.48\n",
      "ep:12300 loss: 0.3259 acc: 99.48\n",
      "ep:12400 loss: 0.3259 acc: 99.48\n",
      "ep:12500 loss: 0.3260 acc: 99.48\n",
      "ep:12600 loss: 0.3259 acc: 99.48\n",
      "ep:12700 loss: 0.3259 acc: 99.48\n",
      "ep:12800 loss: 0.3259 acc: 99.48\n",
      "ep:12900 loss: 0.3259 acc: 99.48\n",
      "ep:13000 loss: 0.3259 acc: 99.48\n",
      "ep:13100 loss: 0.3259 acc: 99.48\n",
      "ep:13200 loss: 0.3259 acc: 99.48\n",
      "ep:13300 loss: 0.3259 acc: 99.48\n",
      "ep:13400 loss: 0.3259 acc: 99.48\n",
      "ep:13500 loss: 0.3259 acc: 99.48\n",
      "ep:13600 loss: 0.3261 acc: 99.48\n",
      "ep:13700 loss: 0.3259 acc: 99.48\n",
      "ep:13800 loss: 0.3259 acc: 99.48\n",
      "ep:13900 loss: 0.3259 acc: 99.48\n",
      "ep:14000 loss: 0.3259 acc: 99.48\n",
      "ep:14100 loss: 0.3259 acc: 99.48\n",
      "ep:14200 loss: 0.3259 acc: 99.48\n",
      "ep:14300 loss: 0.3259 acc: 99.48\n",
      "ep:14400 loss: 0.3259 acc: 99.48\n",
      "ep:14500 loss: 0.3259 acc: 99.48\n",
      "ep:14600 loss: 0.3259 acc: 99.48\n",
      "ep:14700 loss: 0.3259 acc: 99.48\n",
      "ep:14800 loss: 0.3259 acc: 99.48\n",
      "ep:14900 loss: 0.3259 acc: 99.48\n",
      "ep:15000 loss: 0.3259 acc: 99.48\n",
      "ep:15100 loss: 0.3259 acc: 99.48\n",
      "ep:15200 loss: 0.3259 acc: 99.48\n",
      "ep:15300 loss: 0.3259 acc: 99.48\n",
      "ep:15400 loss: 0.3259 acc: 99.48\n",
      "ep:15500 loss: 0.3259 acc: 99.48\n",
      "ep:15600 loss: 0.3259 acc: 99.48\n",
      "ep:15700 loss: 0.3259 acc: 99.48\n",
      "ep:15800 loss: 0.3259 acc: 99.48\n",
      "ep:15900 loss: 0.3259 acc: 99.48\n",
      "ep:16000 loss: 0.3259 acc: 99.48\n",
      "ep:16100 loss: 0.3259 acc: 99.48\n",
      "ep:16200 loss: 0.3259 acc: 99.48\n",
      "ep:16300 loss: 0.3259 acc: 99.48\n",
      "ep:16400 loss: 0.3259 acc: 99.48\n",
      "ep:16500 loss: 0.3259 acc: 99.48\n",
      "ep:16600 loss: 0.3259 acc: 99.48\n",
      "ep:16700 loss: 0.3259 acc: 99.48\n",
      "ep:16800 loss: 0.3259 acc: 99.48\n",
      "ep:16900 loss: 0.3259 acc: 99.48\n",
      "ep:17000 loss: 0.3259 acc: 99.48\n",
      "ep:17100 loss: 0.3259 acc: 99.48\n",
      "ep:17200 loss: 0.3259 acc: 99.48\n",
      "ep:17300 loss: 0.3259 acc: 99.48\n",
      "ep:17400 loss: 0.3259 acc: 99.48\n",
      "ep:17500 loss: 0.3259 acc: 99.48\n",
      "ep:17600 loss: 0.3259 acc: 99.48\n",
      "ep:17700 loss: 0.3259 acc: 99.48\n",
      "ep:17800 loss: 0.3259 acc: 99.48\n",
      "ep:17900 loss: 0.3259 acc: 99.48\n",
      "ep:18000 loss: 0.3259 acc: 99.48\n",
      "ep:18100 loss: 0.3259 acc: 99.48\n",
      "ep:18200 loss: 0.3259 acc: 99.48\n",
      "ep:18300 loss: 0.3259 acc: 99.48\n",
      "ep:18400 loss: 0.3259 acc: 99.48\n",
      "ep:18500 loss: 0.3259 acc: 99.48\n",
      "ep:18600 loss: 0.3259 acc: 99.48\n",
      "ep:18700 loss: 0.3259 acc: 99.48\n",
      "ep:18800 loss: 0.3259 acc: 99.48\n",
      "ep:18900 loss: 0.3259 acc: 99.48\n",
      "ep:19000 loss: 0.3259 acc: 99.48\n",
      "ep:19100 loss: 0.3259 acc: 99.48\n",
      "ep:19200 loss: 0.3259 acc: 99.48\n",
      "ep:19300 loss: 0.3259 acc: 99.48\n",
      "ep:19400 loss: 0.3259 acc: 99.48\n",
      "ep:19500 loss: 0.3259 acc: 99.48\n",
      "ep:19600 loss: 0.3259 acc: 99.48\n",
      "ep:19700 loss: 0.3259 acc: 99.48\n",
      "ep:19800 loss: 0.3259 acc: 99.48\n",
      "ep:19900 loss: 0.3259 acc: 99.48\n",
      "hidden:16, acc:99.48453521728516 , epoch:19993\n",
      "ep:  100 loss: 0.6465 acc: 61.86\n",
      "ep:  200 loss: 0.5145 acc: 67.01\n",
      "ep:  300 loss: 0.4271 acc: 74.23\n",
      "ep:  400 loss: 0.3778 acc: 76.29\n",
      "ep:  500 loss: 0.3604 acc: 76.29\n",
      "ep:  600 loss: 0.3538 acc: 75.77\n",
      "ep:  700 loss: 0.3509 acc: 75.26\n",
      "ep:  800 loss: 0.3494 acc: 75.26\n",
      "ep:  900 loss: 0.3486 acc: 75.26\n",
      "ep: 1000 loss: 0.3482 acc: 75.26\n",
      "ep: 1100 loss: 0.3475 acc: 75.77\n",
      "ep: 1200 loss: 0.3468 acc: 76.80\n",
      "ep: 1300 loss: 0.3461 acc: 77.32\n",
      "ep: 1400 loss: 0.3456 acc: 77.84\n",
      "ep: 1500 loss: 0.3442 acc: 81.96\n",
      "ep: 1600 loss: 0.3369 acc: 83.51\n",
      "ep: 1700 loss: 0.3348 acc: 90.21\n",
      "ep: 1800 loss: 0.3326 acc: 90.72\n",
      "ep: 1900 loss: 0.3313 acc: 91.24\n",
      "ep: 2000 loss: 0.3304 acc: 98.45\n",
      "hidden:17, acc:100.0 , epoch:2087\n",
      "Done with task!, found optimal : 18\n"
     ]
    }
   ],
   "source": [
    "num_input = data.shape[1] - 1\n",
    "\n",
    "full_input  = data[:,0:num_input]\n",
    "full_target = data[:,num_input:num_input+1]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(full_input,full_target)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset,batch_size=97)\n",
    "\n",
    "epoch = 20000\n",
    "converge = False\n",
    "hid_layers = 10\n",
    "while not converge:\n",
    "    net = PolarNet(hid_layers)\n",
    "    if list(net.parameters()):\n",
    "        # initialize weight values\n",
    "        for m in list(net.parameters()):\n",
    "            m.data.normal_(0,.1)\n",
    "        # use Adam optimizer\n",
    "        optimizer = torch.optim.Adam(net.parameters(),eps=0.000001,lr=.01,\n",
    "                                     betas=(0.9,0.999),weight_decay=0.0001)\n",
    "        # training loop\n",
    "        for epoch in range(1, epoch):\n",
    "            accuracy = train(net, train_loader, optimizer)\n",
    "            if accuracy == 100:\n",
    "                converge = True\n",
    "                break\n",
    "    print(f\"hidden:{hid_layers}, acc:{accuracy} , epoch:{epoch}\")\n",
    "    hid_layers += 1\n",
    "print(f\"Done with task!, found optimal : {hid_layers - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "star16 = torch.Tensor(\n",
    "    [[1,1,0,0,0,0,0,0],\n",
    "     [0,1,1,0,0,0,0,0],\n",
    "     [0,0,1,1,0,0,0,0],\n",
    "     [0,0,0,1,1,0,0,0],\n",
    "     [0,0,0,0,1,1,0,0],\n",
    "     [0,0,0,0,0,1,1,0],\n",
    "     [0,0,0,0,0,0,1,1],\n",
    "     [1,0,0,0,0,0,0,1],\n",
    "     [1,1,0,0,0,0,0,1],\n",
    "     [1,1,1,0,0,0,0,0],\n",
    "     [0,1,1,1,0,0,0,0],\n",
    "     [0,0,1,1,1,0,0,0],\n",
    "     [0,0,0,1,1,1,0,0],\n",
    "     [0,0,0,0,1,1,1,0],\n",
    "     [0,0,0,0,0,1,1,1],\n",
    "     [1,0,0,0,0,0,1,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = star16\n",
    "num_in  = target.size()[0]\n",
    "num_out = target.size()[1]\n",
    "\n",
    "# input is one-hot with same number of rows as target\n",
    "input = torch.eye(num_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star16.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart18 =  torch.Tensor([\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,1,0,0,0,0,0,1,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = heart18\n",
    "num_in  = target.size()[0]\n",
    "num_out = target.size()[1]\n",
    "\n",
    "# input is one-hot with same number of rows as target\n",
    "input = torch.eye(num_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
