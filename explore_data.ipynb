{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "# fetch and load training data\n",
    "trainset = datasets.KMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # fetch and load test data\n",
    "testset = datasets.KMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this means we have 64 images of 28x28 pixels in grayscale (1 channel)\n",
    "\n",
    "example_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.view(example_data.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAELCAYAAADtIjDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm81dP+P/DXUppHGkiTuDKXWUqm0s1FIgol/ExFiUskY+QarqtE34wZ6hZFGg1JSeiiZKiUSpNGqWjQuH5/7N1qvd/Z++xx7X1Or+fj4fF4v1v7fPY65yx7nc9a67OWsdaCiIgo2/bJdQWIiGjvwA6HiIiCYIdDRERBsMMhIqIg2OEQEVEQ7HCIiCiIIt3hGGMWGmOa5fD9lxpjzszV+1P62IYoXWxDu6XV4Rhj2hlj/meM2WiMWRWNOxtjTKYqmA3GmPeMMRui/20zxmz18gEpXnOQMebBDNezW7Sx/m6M+dIYc1omr58P2IbENTPahowx93l12mCM2WyM2WGMqZyp98gHbEPimtn4HGpvjFkUrdc7xphKqV4r5Q7HGPNPAH0BPAngAADVAdwEoDGAEjG+pliq75dJ1tqW1tpy1tpyAAYDeGJXbq29Sb/eGFM8dB2NMY0BPAygNYBKAN4A8E6+/0+UDLahrNfxYa9O5QA8BWCCtXZt6LpkC9tQdhljjgXQH8CViPx8twF4NuULWmuT/g9ARQAbAVxSwOteBfB/AMZFX98s+rWvA1gNYBGAewHsE339gwAGeV9fF4AFUDyaT0LkQ/gzAH8A+BBAFe/1HaLXXAOgJ4CFAJolUMdH1L81i37tPQBWABgI4DoAk7zXFI/WrS6AztFfxFYAGwCMiL5mKYDbAXwPYD2AIQBKJvgzvhLA5+pnbgFUTeV3lm//sQ1lvw2p+pjo93Vlrn/3bEOFpw0BeALA615eH8AWAGVS+Z2leofTCEBJACMTeO0VAHoDKA9gCoB+iPyy6wE4A8BVAK5J4r2viL6+GiJ/wdwBAMaYIxFpVB0A1ACwP4CaSVxXqwmgHIDaiPwiY7LW9gfwJoBHbeSvk9Ze8WUAmiPy/Z4QrR+MMcWMMeuMMafGuOxYAKWMMSdF/yK7FsA0a+3qNL6nfMI25MlSG/KdBaAygBFJfxf5i23Ik6U2dBSAb733mANgJ4C/pfLNpNrhVAHwq7V2+65/MMZ8Hq34ZmNMU++1I621n1lrdyLS+7YF0MNa+4e1diEit/kdknjvgdbaudbazQDeAtAw+u9tAIyx1k621m4BcB8iP5hUbQfwoLV2a/S9UtXHWrvCWrsGwJhd9bXW7rDWVrLWTo3xdb8DeAfA54j8RdEDwA1p1CPfsA0lLtU25OsI4C1r7aY06pFv2IYSl2obKofIXZHvd0Q67qSl2uGsAVDFH1O01p5mra0ULfOvu8SLqyDy18Ai798WATgoifde4cWbEPmBAJG/Jtx7WWs3RuuSqpXW2q1pfP0usepbkBsR+R/gSET+irsGwDhjTPUM1CkfsA0lLtU2BAAwxpQFcAmA1zJQl3zCNpS4VNvQBgAV1L9VQGQoMWmpdjhfIPJXd6sEXutvR/0rIn9d1PH+rTaAX6LxRgBlvLIDkqjTcgC1diXGmDKI3M6mSm+jXVDdMr3tdgMAo6y1P0X/ChmLyM+vUYbfJ1fYhrLfhnZpA2AlIkNJRQnbUPbb0ExEPosAAMaYwxDpN35K5WIpdTjW2nUAHgLQ3xjTxhhTzhizjzGmIYCycb5uByK3n72NMeWNMXUQmcwaFH3JDABNjTG1jTEVERlGStRwAOcbY5oYY0oA6IXMPmf0LYBjjTHHGGNKA3hAla9EZHw0U75C5PupayJaADgEkQZQ6LENBWlDu3QE8JqNzvoWFWxDQdrQIAAXGWNOi94p9wIwLNWh2ZR/ENbaJxD5JXUHsAqRb/R5AHchMu8QSxdEeukFiPzF9V8Ar0SvOR6RSa/vAExDZKwx0frMBHBz9HrLAaxFZHVGRlhrZwF4FJEVKnMATFYveQlAA2PMWmPM8IKuF52s22CMiXXHMhCROZzJiIyZPg3g/1lrU/rLIh+xDWW9DcEYUxtAU0SW1Rc5bEPZbUPW2u8A3AJgKCI/35KI/OxSYorYHz1ERJSnivTWNkRElD/Y4RARURDscIiIKAh2OEREFAQ7HCIiCiKp3UeNMXm9pK16dfkQfs2asbcwWrZsmciXL1+elTqFYK0tFDtI53v72Yv9aq2tmutKJIJtKD8l+hkUfLvrbOrYsaPIH3/88Zivve+++0Teu3dvF3OpOO1lFhX8EqL0cUiNiIiCSOrBz3y7nT300ENF/uWXX4q8cuXdBxv269dPlHXr1k3kO3ems6FrbnFIjdI0zVp7Yq4rkQi2ofyU6GcQ73CIiCgIdjhERBREoRtSK1du9zEOEydOFGUnnihHBebPn+/i448/XpT9/vvvCb9n2bJy49m7777bxfPmzRNl7du3d/GgQYNE2WuvZec4Eg6pUZr2+iG1YsWKuVgPr+diEZEx8n/pihUrinyffTJ/r7Bpk9wA+s8//0z4azmkRkREeYUdDhERBcEOh4iIgih0D376D3PqOZvt27eL/Prrr3dxMnM2JUuWFPnLL78s8oULF7p42LBhouzBBx90cf/+/UXZ0qXyHKYJEyYkXCciyhx/zgYAnn/+eRevX79elN11110i158z2aDnaC6//HKR33HHHS6uXbu2KCtePLWP9WuuuUbkr776akrXiYd3OEREFAQ7HCIiCoIdDhERBZH3z+GccMIJIp8yZYqLS5UqJcr0XIs/h5PM99mrVy+Rt2jRQuRnnnmmizdv3izKXnnlFRfrMVE933PZZZclXKd4+BwOpWmvew6nalW5OfasWbNcvN9++4myRo0aiVxvoZULJUqUcLGu39lnn+1iPb/ToUMHka9evdrFek78l19+Sbg+fA6HiIjyCjscIiIKIu+WRettZPTSPH8YbcmSJaLsnnvuEXkyw2itWrVycdeuXUXZySefLHI9jObTSyp9Bx10UML1IaLsqVChgsjLly/vYr0k2f9sAPJjSG3r1q0u/vTTT0WZv6XXkUceKcratWsncv9xEb2dTjbwDoeIiIJgh0NEREGwwyEioiDybg7nscceE/nRRx8d87X+NjcAsGrVqoTfp27duiL3lzPr686dOzfh68YbBz3wwAMTvg7lnv+77N27tyirWbOmyLds2eLi5557TpTNmDFD5GeccYaLa9SoIcqGDBmSWmUpKXp7Gn8uQy+Zbt68uch79uyZvYqlwG9PADBq1CgX62269t13X5EfdthhLp4+fboomzp1qsgfffTRmGWJ4h0OEREFwQ6HiIiCyLshtdNOOy1u+ejRo108YMCAhK+rh7r69esncn+J9b///e+Er6v99ttvMctSvQ2lMPTww2233eZivWPwjh07RP6vf/3LxXoITfvkk09SreJer0SJEmI401/ePGfOHPHaeI8v6M+DeEPhK1asSLaaGad3VTn33HNdPHDgQFHmn4qsfwZ6CbX/2kMPPVSUXXDBBSI/5JBDXHzccce5eNu2bXHr7uMdDhERBcEOh4iIgmCHQ0REQeTFHI4/dn744YfHfe0LL7zgYj2OHo9eyviPf/xD5P52NsmMSWr+thLapk2bUr4uZZ8/Lg4AjzzyiIt37twpytq0aSNyfykqZU+tWrXwxBNPuLx169YuHjlypHjtjTfeKHL/sYmNGzeKMn+rGO3pp59Oqa7pqFy5ssjvv/9+kXfp0sXF+jRjf25bL7PXczg+f4k0IB8VAYCTTjrJxW3btnXx2LFjY15T4x0OEREFwQ6HiIiCYIdDRERB5MWJn9WrV3exXvO+bt06kdeqVcvFGzZsiHvdpk2buvijjz4SZcuXLxd5/fr1Xfznn38WUOPYLrzwQhfrMeXPP/9c5I0bN075fXw88TNxxYvvnrbUY/w6r1Onjov1lkv+8wsA8OSTT7pYt9lCoNCc+FmyZEnrP4fjHxWg5z30M3H+XMOyZctEWY8ePWK+5xFHHCHyH3/8MfEKJ8E/FmHEiBGiTH9vr7/+uov1HIo/V5XMPLemt+JasGCBi/3TQC+66CJ8//33PPGTiIjyBzscIiIKIi+WRcfbRfnbb78VebxhtEqVKom8b9++Ltan+P3zn/8UeTrDaL5ixYrFLFuzZk1G3oMS5w+hAXKJ68033yzK9FJZv034y1CBPdusvwuvP7wGAKtXr06ixhTP1q1bxdDOU0895WI9LKaHoTp27JjQe+jHIvRnR6bonfD9pcbPPvusKJs0aZLI03l0I1F6qyd/+x9/mxv9unh4h0NEREGwwyEioiDY4RARURB5MYejx9l933//fcLXufPOO0XesGFDF+sx0eHDhyd83WTEmwviWH72+VudAMC1114rcn9Lo5kzZ4oyPS9TpUoVF//0009x39dveyVKlBBl3bp1i/u1lDp/vuyzzz4TZfpICf+RCn3ypb+lli5LZ2mxv+W/3u5f1+/iiy928ZVXXinK/Ec8AHkcRra2zNL/L/lzNWvXrnVxMj8f3uEQEVEQ7HCIiCgIdjhERBREXszhHHXUUTHLtm/fHrOsU6dOItdjov5RAXp772zR4/e+eFuDU+o6dOjg4pdeekmU6d+H/1zXddddJ8omTJgg8hYtWrh4ypQpouybb74Rub+1++WXXy7Khg4dKvKKFSu6+NhjjxVl9erVc/Hs2bNj1h2Q27rEO065KPM/HyZPnizK9JyO/zzNTTfdJMqeeeaZmO9RtWpVkeujrH1NmjQR+X333efi8ePHi7LnnntO5A888ICLmzdvLsr0Edh+O9HzPanS80TxPjP9Mn+bm4LwDoeIiIJgh0NEREHkxZDaySefHLPM3/kUAE477TQX+9taAPG3r/GX8SVLb91wzDHHuHjatGmizN/5Wou3/JviK1++vIsHDRokyk4//XQX6zagtzt58cUXXezv7A0AK1euFHnZsmVd3L59e1HmL3fVqlWrJnI9HBdv+6N49M7u/q7F/fv3F2XPP/+8yENshZJv9HJdP99///0Tvo4/BAoAp556qov1ycGNGjUS+fTp013sn1YM7HlSp78s/7LLLhNl/uceILfBee+990SZ/v8jHn+4cPDgwaJMbxW2cOFCF7/xxhsuLmjXfh/vcIiIKAh2OEREFAQ7HCIiCiIvJhVOOumkmGV67NzfJqR06dKiTC87HD16dAZqB2zZskXks2bNcrFe1upvn6Gls0XG3kYvZ/bnJPTciz+P5m/5Aew5P/jdd9+5uEaNGnHr8Pbbb7vYH78G9lyuH285vJ7D8ZdJ6/Zy4om7D94899xzRZleGuufRNmvXz9Rdvvtt4v87rvvdrHe1mnnzp0x615UFfS79+ll7WXKlHGxnjP0j78A5DJ9PWej+acQ+0erAHvOZbds2dLF99xzjyh79913XaznV/SRDf5cjJ7X0nOa/jZR69ev3/MbSADvcIiIKAh2OEREFAQ7HCIiCiInczh6fbc+atV35JFHxizzxzyBPbcUydbYtL8duN5u5Jprron5dXvjWHmizjrrLJHrZxZq167t4rlz54oyf8xaP4+ityXR8yA+fQSBX4f//Oc/oqxz584i13MovnfeeUfkAwYMiPlav356exP9/I4/n+nP/QDAwQcfLPL//ve/LtbHZEycODFmfYoS//kZvQVNPOXKlYtZNmPGDJHrOZwlS5Yk/D7xjBgxQuT+Nl7+cc+APFbjgAMOEGW6nfrzhHpeRm/9lIl2wjscIiIKgh0OEREFkZMhNX9JH7Dn8uZ4/GV+evhqzZo16VUsA/wtWDR/OeXeyh8yuvrqq118xx13iNfpk1N/+OEHFx9//PGizN/CSA9b6nzIkCEu1tu96GWhX3/9dczr6FMhfXr5ezKn1vrb13z44YdxX+tvaXLzzTeLMn3CrT8cp7djKSpDanroy29fANClSxcX+7t7J8v/DNI7NWdqCE3TbWrkyJEu1v/v+D+HRx55RJT9/e9/F/kXX3zhYv3z0kPXmcA7HCIiCoIdDhERBcEOh4iIgsjJHM6NN96Y8Gv1luznnHOOi/0TD/NF3bp1Y5bp72VvU7FiRXGUgL/UWI9RJ7N9vH/apT61018ODAAffPCBi/3lowAwbNgwkftbI/Xs2VOU6eXWPj32rU+eTJU+EsFfGtuuXbuEr3PCCSdkpD75wJ//1dvB6C3+N27cmJH3fPPNN12sT2XNlnvvvVfk/omlem7Yb8eHH364KNPzia1bt3ax3somG3iHQ0REQbDDISKiINjhEBFREMHmcPzta5LZVkI//7Bs2bKM1SkT9HYj8bbpibdFxt6gdOnS4nhuvcVRoubPny/y66+/3sX6KIB4RyvrLd87dOgg8nhb28Q7srxPnz4i37p1a8zXav5xBbfeeqso08+d7bfffglf1zdq1KiUvi4f+c+OnH322aJs6dKlIk/1/z//eSxAHluerXlZvQXTihUrEv5af95Gb2Okn88KMW/j4x0OEREFwQ6HiIiCCDakduedd7pYD0PFo29Z4w2R5IIe8vN3ktaaNm0qcn/nVn2qaFG0YsUKPPnkky6fPn26i/VwSMOGDUU+b948F/vXAPY8jTNVixcvFnmbNm1cPHjwYFGmTx316VM89VYy/u6+l156qSjzv+9MDsH6jxC8+OKLGbtuLvinbPrLg/Uy9zlz5ohc76oci/7M6d69u8j1MFWm+N+Xbl+lSpUS+c8//+ziatWqiTJ/13P9CMrUqVPTrmc6eIdDRERBsMMhIqIg2OEQEVEQJpllfcaYhF/sn64HyOWqyczh6CWl/gl28Zam5srAgQNdrLf71saNG+fitm3bijJ/C/SCWGtjH2OZR5JpP/lGL0E+5ZRTRO4v8dZLWPXv1p/TqVmzZkbq980334j8tddeE/lLL73k4r/Y4mWatfZE/Y/5qHTp0tafixk+fLiL9TYuqdLzss8884zIS5Ys6WJ9zIGeT/TnHv2TaYE955yaNWvmYn2SsL8FDQCceeaZLtbbQvmn52ZqfrMgiX4G8Q6HiIiCYIdDRERBZG1Izd9pFwDOP//8xGvl0cug/SG13377LaVrZlP9+vVd7C/7BeKf+Kmf/m7VqlXC78khtezzhymAMKdk1qtXT+QVKlQQ+cyZM128fft2UZbkE/CFZkitePHi1l8u/t1337lYLw/WS4l9+kRZf6mzPs3V/8xJx8MPPyxyvSP6VVdd5WK903Xnzp1F7g+j3X777aJswYIFadUzFRxSIyKivMIOh4iIgmCHQ0REQWRsDqdixYoiX7NmjciTWQrt0/M0/nhqvm1zo+ndff2lqYDcykIvpT3wwAMTfh/O4VCaCs0cjm5D/tywXr5co0YNkY8dO9bF+gRXfxd6/VnlL0EGgN69e7v4iCOOEGV6ibJ/rZEjR4qyV199VeRly5Z1cfPmzWPWD5DbO+XD4yGcwyEiorzCDoeIiIJgh0NEREFkbA6ncuXKItdzOP52GrqsTp06Md9z3bp1Iq9evbqLkzlJMRf0+n29Xbr/XMWsWbNE2VFHHZXw+3AOh9JUaOdwfPozqHTp0iL3P0viHSNSkKpVq7q4Y8eOoqxr164i94+q0PPR+hgLf85XnxzcpUsXkevnrnKNczhERJRX2OEQEVEQGRtS09tB9O3bV+T+TqkHH3ywKBswYEDM93zzzTdF3q5du4IrmkP+Trbjx48XZfr7HjFihIv1yXzJnCrIITVKU5EYUssHehjdX4qth99+//13kfuPTeitbfTUQr7hkBoREeUVdjhERBQEOxwiIgoia8cTxKO3ev/4449drOujt5WYPHlyJqqQMY0bNxb50KFDXay3H9fzNIMHD3axPmUwGZzDoTRxDidL/CNJ9FLsEiVKiNzfBmfz5s3ZrViGcQ6HiIjyCjscIiIKgh0OEREFUTwXbzp//nyR+/MXEyZMEGVTpkwJUqdk+Gvt/XX2ml5LP2bMmKzViYjyT7wtdPJ9a65s4B0OEREFwQ6HiIiCyMmQ2qpVq0Ter18/Fz/xxBOiLJ3lwtni7+qqTzq94oorXMwhNCKi3XiHQ0REQbDDISKiINjhEBFREMlubbMawKLsVYdSUMdaW7Xgl+Ue20/eYhuidCTcfpLqcIiIiFLFITUiIgqCHQ4REQXBDoeIiIJgh0NEREGwwyEioiDY4RARURDscIiIKAh2OEREFAQ7HCIiCoIdDhERBcEOh4iIgmCHQ0REQbDDISKiIIp0h2OMWWiMaZbD919qjDkzV+9P6WMbonSxDe2WVodjjGlnjPmfMWajMWZVNO5sjDGZqmA2GGPeM8ZsiP63zRiz1csHpHjNQcaYBzNYx4OMMaONMcuNMdYYUzNT184nbEPimhltQ+rab0TbUd1sXD+X2IbENTP9OXShMeZzY8y66GfR88aYcqleL+UOxxjzTwB9ATwJ4AAA1QHcBKAxgBIxvqZYqu+XSdbaltbactbacgAGA3hiV26tvUm/3hhTPHwtsRPAOABtcvDeQbANhRH967ZOrt4/m9iGsq48gIcAHAjgKAAHA3gs5atZa5P+D0BFABsBXFLA614F8H+IfHBuBNAs+rWvA9h1ct+9APaJvv5BAIO8r68LwAIoHs0nAXgYwGcA/gDwIYAq3us7RK+5BkBPAAsBNEugjo+of2sW/dp7AKwAMBDAdQAmea8pHq1bXQCdAWwDsBXABgAjoq9ZCuB2AN8DWA9gCICSSf6sS0Xfp2Yqv6t8/Y9tKEwbArAvgG8BNNj1Xrn+3bMNFa42pOp0GYBvUv2dpXqH0whASQAjE3jtFQB6I9JTTgHQD5Ffdj0AZwC4CsA1Sbz3FdHXV0PkL5g7AMAYcyQijaoDgBoA9geQzjBUTQDlANRG5BcZk7W2P4A3ATxqI3+dtPaKLwPQHJHv94Ro/WCMKRa9TT01jToWZmxDniy2oTsAfARgZsrfRf5iG/IE+hxqijTaUqodThUAv1prt+/6B2+cb7Mxpqn32pHW2s+stTsR6X3bAuhhrf3DWrsQwFOIfvMJGmitnWut3QzgLQANo//eBsAYa+1ka+0WAPchMiyVqu0AHrTWbo2+V6r6WGtXWGvXABizq77W2h3W2krW2qlpXLswYxtKXEptyBhTB8C1iPzFXhSxDSUu7c8hY0xLRDraB1KtRKodzhoAVfwxRWvtadbaStEy/7pLvLgKIn8NLPL+bRGAg5J47xVevAmR3h+I/DXh3stauzFal1SttNZuTePrd4lV370d21DiUm1DzwB4wFr7RwbqkI/YhhKX1ueQMeY0AG8AuNhaOz/VSqTa4XwBYAuAVgm81nrxr4j8deFPYNYG8Es03gigjFd2QBJ1Wg6g1q7EGFMGkdvZVFmVF1Q3/XqKj20o+23oHAD/McasQGQcHwC+Msa0zfD75ArbUIDPIWPMiQDeBXCVtXZSOtdKqcOx1q5DZOVCf2NMG2NMOWPMPsaYhgDKxvm6HYjcfvY2xpSP3vLfDmBQ9CUzADQ1xtQ2xlQE0COJag0HcL4xpokxpgSAXsjsc0bfAjjWGHOMMaY09rytXInI+GjGGGNKITJGDQAljTEl472+MGEbCtKG6iEydNIQkXF7ADgPwKgMvkfOsA1lvw0ZYxogstiis7V2XLrXS/kHYa19ApFfUncAqxD5Rp8HcBeAz+N8aRdEeukFiEze/RfAK9Frjkdk0us7ANMQGWtMtD4zAdwcvd5yAGux+6+6tFlrZwF4FJEVKnMATFYveQlAA2PMWmPM8IKuF52s22CMaRSjvDiAzQDWRf9pHiI/tyKDbSi7bchauyo6br8CkZ8tAKxOcy4gr7ANZbcNIbIYYn8Ar3rPCH2bav1NdKkbERFRVhXprW2IiCh/sMMhIqIg2OEQEVEQ7HCIiCgIdjhERBREUruPGmOCL2krUUJu+HrQQQfFLJs3b57Id+zYkdJ7VqhQQeR/+9vfRL516+4Hf7///vuU3iOTrLV5vQ37LrloP5lStqx8rKN69eoi//PPP128bNmyIHXKoF+ttVVzXYlEFOY2lI599pH3Bn57/OMPuZFE8eK7P9YrVaokyrZt2yby9evXZ6R+iX4G5WzL9ETVqFFD5L169XJx3bp1RdlFF10k8jVrUttRonHjxiIfN04+77R06e5l9bVq1QIVfcccc4zIb7vtNpH7f+zcf//9oizVP3wCWlTwSyiXypWTO9GccsopLv7oo49Emd/JtG7dWpStWLFC5KNHj85UFRPCITUiIgoi7+9wVq9eLfIyZXZvI9SkSRNR9uyzz4r88ssvT+k927aNv9XUhg0bUrouhecPwer8yy+/FGV16sgzyh566CEXt2jRQpRVq1ZN5L/88ouLJ02aJMrGjx+feIUpCH0YqD88v2XLltDVKZC+a77llltcfNhhh4myJUt271M6ceJEUbZgwYIs1C5xvMMhIqIg2OEQEVEQSe2llg8rREqVKuXi6dOnizK9mqxBgwYunjVrVtzrHnrooS7+4Ycf4r7WH8r7+uuv4742BK5Si61r164iv/rqq1187bXXirLBgweLvFixYi7WwxZ68nXmzN2HIL7//vui7Kmnnkq8wrkxzVp7Yq4rkYhMtSH/swEAevbs6eKPP/5YlOn/x/1hqc2b5T6oehXYzp07/zL+K/5KNL0CVw/T+osG9KKml156ycXffPONKFu4cGHcOqQq0c8g3uEQEVEQ7HCIiCgIdjhERBREoZvD8enly0OHDhX5m2++6eJ27dqJMn98HpAPQLVs2VKU9e3bV+TdunVLvrJZxDmc3fTvtU+fPiK/7rrrXKznYfRSZ3/pbOnSpUWZfpjzgw8+cHH79u1F2dq1awuqdq7tdXM4WsmSuw/T1Q9SHn300SJ/7733XKyX3es5Er9N+Y90AHvujOK/tkqVKqJM5/6cs/4M99umnsPRj5L4u6akg3M4RESUV9jhEBFREOxwiIgoiLzf2iaed955R+Q//vijyP05Hn8+BwDq1asncn/eRj+H88ADD6RVTwpHz62MGTNG5L///ruLa9euLcr0XMtZZ53lYj2Or/nj+oVgzoYUfzubyy67TJR16tRJ5P5GmosXLxbKa8F9AAAM0ElEQVRls2fPFnnHjh1dvGnTJlFWtarcoHvffff9y/oAcs4GAN59910X67lIv066LeZ6I1ne4RARURDscIiIKIhCvSxau+CCC0Q+atQoF69cuVKU6UPW/OWMTZs2FWW//vprhmqYHVwWnRljx44V+XnnnRfztXqork2bNi7Ox92GC7DXL4vOFn9pvf6s1Uv4/eGuVq1aibI33nhD5P5ybH0AWy5wWTQREeUVdjhERBQEOxwiIgqiUC+L1vS4un/SYvPmzeN+7YUXXujifJ+zoczQWyPpUz39bT/0OPnUqVNFfu6557r4559/FmX+0QXAnmP5VHTF+13HW6JcvLj8aF60aJHIN27cmF7FcoR3OEREFAQ7HCIiCqJIDanpEz/r16+f8Nc2a9bMxXoXVyo6/N0Fbr31VlGmhzj84RD/NEYAeOihh0Tul7/99tui7K233hK5X17QKZC0d/J3MwD2PPGzsLYb3uEQEVEQ7HCIiCgIdjhERBREoZ7Dueiii0T+wgsviNzfjVUvT/S3nACAZ555xsVz5swRZRMnTkyrnpQ7ennpHXfc4eJGjRolfB3/REhgz2XRDRo0cLHeluTggw8WuT+32Lt3b1HGJdP0V/TO+IUV73CIiCgIdjhERBQEOxwiIgqiUM/hTJs2TeTly5eP+Vo9Z6P5p+0NGTJElPmn9gHABx98kGgVKcfuvPNOkXfu3NnFBc2X+Fsc3X///aJMP2vTpEkTF3fr1k2UlS5dWuQ1atRwsT4mY/369XHrRHsHvR3S6aefLvLCOq/MOxwiIgqCHQ4REQVRqIfUlixZIvKBAweK/OSTT3ZxiRIlRFnlypVF7p+gV716dVF28cUXi5xDavmrVq1aIj/++ONFvn37dhdv27ZNlOlTYa+99loXT548Oe77jhgxwsWjR48WZf5poABw7LHHurh79+6irGfPnnHfh/YO559/vsgL6+7QGu9wiIgoCHY4REQUBDscIiIKwiSzlYYxpsjuu/H666+7uEOHDqJMb3Xjb4mydu3a7FYsAdba+Gu+80S22k+dOnVc/NVXX4myUqVKidyfy1uzZo0o00tPFyxYkJH66fnDUaNGubhx48Yx6zBjxoyMvH8CpllrTwz1Zukoyp9BxYoVc/Hw4cNF2SeffCLyPn36BKlTohL9DOIdDhERBcEOh4iIgmCHQ0REQRTq53AK4m9Xo48P1ke03n777S7W28vro6r9owz0fA9l36GHHiry9957z8X+kRQA8Oeff4r8/fffd/GNN94oyvRzONkycuRIF594opw6admypYsDzuFQDuj5Rf/5rCOOOEKULVy4UORlypRx8ebNm0VZvHl5f54I2PNzMdt4h0NEREGwwyEioiCK9JDaXXfd5eJOnTqJsh9//FHkX3/9tYs3bdokyvSOvpdeeqmLr7vuOlG2ZcuW1CpLCdPb1/hDCPrn//jjj4u8b9++Lg61pH3r1q0i95dj65NE/WE+vfRVD51Q4aJ3rPdPnwWA2267zcX77befKNPLovXQWKL22UfeY3BIjYiIiiR2OEREFAQ7HCIiCqJIz+E88sgjLn755ZdFmT4J8tZbb3WxHlfX/PKGDRuKsv/9739J15OSs3z5cpHXq1fPxXpMWv8+Up230ePvekmrv71O69atRZkef/fnYvyl+wCwYsUKF3M+sGjxlz0D8vRZYM+TYX2LFy8W+R9//JFSHfSRHKHxDoeIiIJgh0NEREGwwyEioiCK9ByOT4/7+1vZAED//v1d3KNHD1HmHzUMAD/88IOL586dm6kqUoIqVaokcv/ZgnvvvVeU+VvZFKRmzZoiP+ecc1x8yimniDJ9rID/bM3RRx8tynr16iXySZMmuVg/W1O3bl0Xly1bVpSlOm5PueM/w+cfbw4AM2fOFLm/fY1+vk/P9fltXm/Tlc94h0NEREGwwyEioiD2miG1gsybN8/Fr7zyiijTQ2pLly51cT6c+Lm38bchAoB27dq5ePTo0aJM75zrL2k/99xzRdljjz0mcn97Eb1EWW+v4w/lPffcc6Js2LBhIm/atKmL9W7W/nDhUUcdJcqmTp0KKlz8bY1mz54tyvSJs3rY1nfGGWeI3F+Wn8xu0bnGOxwiIgqCHQ4REQXBDoeIiILgHM5fqFatWtxyvXyWwtLLS996662Yr9XbuD/88MMuvvzyy0WZ/r36y031ljl6q5FVq1a5+IILLohbX39cv3hx+b+gn+vl1ZzDKXz0HJ3vnXfeEXnbtm1jvla3t2OOOcbFhWk7Ld7hEBFREOxwiIgoCHY4REQURKGbw/GPEejevbso01tFDBw40MUjRowQZfHGVs8777y4dfj5559drLetz+c18HsDPSfitxcAuPnmm12stwTRuX9UwC233CLKVq9eHfO6l1xyiSjT25LMmTPHxf7RCoCcc3r66adF2YcffihyPa5Phcu4ceNE7m+Zpefv/G1vANlOypQpI8o2bdqUoRpmHu9wiIgoCHY4REQUhElmCMgYE3y8SO8M7A8j6CGQ8ePHi/y4445z8UEHHSTKFi1aJPKffvrJxc2bNxdl+n1uuOEGF3/88ceibNmyZQjNWmsKflXuZav9+MOa+iTXTp06idwfitDLladMmSLyQw45xMV62OKII44QeYkSJVz8yy+/iDJ/GTQgh+NOPvlkUeZ/LytXroxZHwDYuHEjMmSatfbETF0sm3LxGRRKq1atXPzuu++KMn8ZNCC31NJt6IMPPnBxqOG1RD+DeIdDRERBsMMhIqIg2OEQEVEQeb8s+thjjxV5+fLlXdynTx9Rdtttt4ncX47qz7v81dfWr18/Zh06duwo8kGDBsWpMYVWp04dF+s5G39uDgDmz5/vYn2q4qWXXirydevWubhq1aqiTG+Z4/O3uQH2HI+/8sorXaznd/zjE/Qy6AzO2VCG+CdvAumdvunP2fltD5DtFpBHEnz11Vei7OKLL3bxyJEjRVmuT43lHQ4REQXBDoeIiILI+yG1WbNmidxfxu0PpfyVbdu2uVifwvjZZ5+J3N+p9YsvvhBlo0aNSqyylBP+7gLVq1cXZXXr1hX5Oeec4+KChkNKly4d87X6cYKJEye6uHPnzqJs0qRJIvfrGG8IRu+aQPlH/470EGkyGjVq5OLp06eLMn2qp08/iuG3N70TSq53RuEdDhERBcEOh4iIgmCHQ0REQeT9IPGvv/4qcv90uwsvvFCU6R1W/d1XtRkzZsTNqfCYN2+ei/WS9euvv17kei7G529PA8jx+bFjx4qy119/XeT+8uvhw4eLsgMOOCDme+rl1f6cztChQ2N+HeXOgQce6OIjjzxSlE2YMCHl65500kkuTuYUTz0PuHTpUhfrNq23CvO3yAmBdzhERBQEOxwiIgqCHQ4REQWR93M42pgxY1x86qmnirIhQ4aI/IILLnCxPjGPiqb77rtP5P54OwD89ttvLq5cubIo0/MyP/74o4tnz54tyvTxBP6W8FWqVEm4vn59AKBr164u9ts65Q9/+yE9R5IMPZ/YpEkTF3/77bcpX9ennwtK5zmhTOAdDhERBcEOh4iIgsj7Ez+1GjVquFgvHaxZs6bI/V17/R16AeCjjz7KQu1yY28/8TOegrakiVdWqlQpFz/44IOirEuXLiL3TwTdsWOHKNOnLs6dO9fF7du3F2X+MF5APPEzCf7v+uGHHxZl3bt3F7luCz69XN4/KfaSSy4RZXp7rXR2pc4GnvhJRER5hR0OEREFwQ6HiIiCKHTLov2tuFu0aCHK9Dbw1apVc/G4ceNE2csvvyzyHj16uFiftkeFVzJj3WXLlhW5f6RFhw4dRJmeG/LbjD6pc9iwYSL3t8mJt+085Sf/d6aPT/GXTAN7zt/5ateuLXK/TenjCUIfI5AtvMMhIqIg2OEQEVEQ7HCIiCiIQjeH49Pjp82aNRP5p59+6uIKFSqIsptuuknky5cvd3GvXr1Emf88BgDccMMNLv7kk09EWaa2pKDsO/zww0U+YMAAkZ9xxhkxv3b+/Pki99tTUXrGi/bkz6f4x1IAe87LJPNclf/coP95pN+zMOMdDhERBcEOh4iIgijUQ2rad999J/KWLVu6+NFHHxVl9erVE7m/bFpvkTN+/HiR+0MxZ511VmqVpZzwf196CO2www4T+bZt21ysl9E/8MADIveHQ2jvMWXKFJHrYdg5c+a4WA+LNWjQQOSLFy92sd/2/krFihVdvH79+sQqmwd4h0NEREGwwyEioiDY4RARURDJHk+wGsCi7FWHUlDHWls115VIBNtP3mIbonQk3H6S6nCIiIhSxSE1IiIKgh0OEREFwQ6HiIiCYIdDRERBsMMhIqIg2OEQEVEQ7HCIiCgIdjhERBQEOxwiIgri/wPXqMEePiLmMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape\n",
    "x = example_data\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(x.shape[0], -1)#flatten image into 784x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spirals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.50000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.31380</td>\n",
       "      <td>1.25590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.31380</td>\n",
       "      <td>-1.25590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.88973</td>\n",
       "      <td>2.43961</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-5.88973</td>\n",
       "      <td>-2.43961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.24865</td>\n",
       "      <td>3.50704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-5.24865</td>\n",
       "      <td>-3.50704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.41941</td>\n",
       "      <td>4.41943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-4.41941</td>\n",
       "      <td>-4.41943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_coord   y_coord  class\n",
       "0  6.50000   0.00000      0\n",
       "1 -6.50000  -0.00000      1\n",
       "2  6.31380   1.25590      0\n",
       "3 -6.31380  -1.25590      1\n",
       "4  5.88973   2.43961      0\n",
       "5 -5.88973  -2.43961      1\n",
       "6  5.24865   3.50704      0\n",
       "7 -5.24865  -3.50704      1\n",
       "8  4.41941   4.41943      0\n",
       "9 -4.41941  -4.41943      1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(df.values,dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-6.5000e+00, -0.0000e+00,  1.0000e+00],\n",
       "        [ 6.3138e+00,  1.2559e+00,  0.0000e+00],\n",
       "        [-6.3138e+00, -1.2559e+00,  1.0000e+00],\n",
       "        [ 5.8897e+00,  2.4396e+00,  0.0000e+00],\n",
       "        [-5.8897e+00, -2.4396e+00,  1.0000e+00],\n",
       "        [ 5.2487e+00,  3.5070e+00,  0.0000e+00],\n",
       "        [-5.2487e+00, -3.5070e+00,  1.0000e+00],\n",
       "        [ 4.4194e+00,  4.4194e+00,  0.0000e+00],\n",
       "        [-4.4194e+00, -4.4194e+00,  1.0000e+00],\n",
       "        [ 3.4376e+00,  5.1447e+00,  0.0000e+00],\n",
       "        [-3.4376e+00, -5.1447e+00,  1.0000e+00],\n",
       "        [ 2.3439e+00,  5.6588e+00,  0.0000e+00],\n",
       "        [-2.3439e+00, -5.6588e+00,  1.0000e+00],\n",
       "        [ 1.1827e+00,  5.9460e+00,  0.0000e+00],\n",
       "        [-1.1827e+00, -5.9460e+00,  1.0000e+00],\n",
       "        [-2.0000e-05,  6.0000e+00,  0.0000e+00],\n",
       "        [ 2.0000e-05, -6.0000e+00,  1.0000e+00],\n",
       "        [-1.1584e+00,  5.8234e+00,  0.0000e+00],\n",
       "        [ 1.1584e+00, -5.8234e+00,  1.0000e+00],\n",
       "        [-2.2483e+00,  5.4278e+00,  0.0000e+00],\n",
       "        [ 2.2483e+00, -5.4278e+00,  1.0000e+00],\n",
       "        [-3.2293e+00,  4.8329e+00,  0.0000e+00],\n",
       "        [ 3.2293e+00, -4.8329e+00,  1.0000e+00],\n",
       "        [-4.0659e+00,  4.0658e+00,  0.0000e+00],\n",
       "        [ 4.0659e+00, -4.0658e+00,  1.0000e+00],\n",
       "        [-4.7290e+00,  3.1598e+00,  0.0000e+00],\n",
       "        [ 4.7290e+00, -3.1598e+00,  1.0000e+00],\n",
       "        [-5.1968e+00,  2.1526e+00,  0.0000e+00],\n",
       "        [ 5.1968e+00, -2.1526e+00,  1.0000e+00],\n",
       "        [-5.4556e+00,  1.0852e+00,  0.0000e+00],\n",
       "        [ 5.4556e+00, -1.0852e+00,  1.0000e+00],\n",
       "        [-5.5000e+00, -4.0000e-05,  0.0000e+00],\n",
       "        [ 5.5000e+00,  4.0000e-05,  1.0000e+00],\n",
       "        [-5.3330e+00, -1.0609e+00,  0.0000e+00],\n",
       "        [ 5.3330e+00,  1.0609e+00,  1.0000e+00],\n",
       "        [-4.9658e+00, -2.0570e+00,  0.0000e+00],\n",
       "        [ 4.9658e+00,  2.0570e+00,  1.0000e+00],\n",
       "        [-4.4172e+00, -2.9515e+00,  0.0000e+00],\n",
       "        [ 4.4172e+00,  2.9515e+00,  1.0000e+00],\n",
       "        [-3.7123e+00, -3.7123e+00,  0.0000e+00],\n",
       "        [ 3.7123e+00,  3.7123e+00,  1.0000e+00],\n",
       "        [-2.8820e+00, -4.3133e+00,  0.0000e+00],\n",
       "        [ 2.8820e+00,  4.3133e+00,  1.0000e+00],\n",
       "        [-1.9612e+00, -4.7349e+00,  0.0000e+00],\n",
       "        [ 1.9612e+00,  4.7349e+00,  1.0000e+00],\n",
       "        [-9.8759e-01, -4.9652e+00,  0.0000e+00],\n",
       "        [ 9.8759e-01,  4.9652e+00,  1.0000e+00],\n",
       "        [ 6.0000e-05, -5.0000e+00,  0.0000e+00],\n",
       "        [-6.0000e-05,  5.0000e+00,  1.0000e+00],\n",
       "        [ 9.6331e-01, -4.8426e+00,  0.0000e+00],\n",
       "        [-9.6331e-01,  4.8426e+00,  1.0000e+00],\n",
       "        [ 1.8656e+00, -4.5039e+00,  0.0000e+00],\n",
       "        [-1.8656e+00,  4.5039e+00,  1.0000e+00],\n",
       "        [ 2.6737e+00, -4.0014e+00,  0.0000e+00],\n",
       "        [-2.6737e+00,  4.0014e+00,  1.0000e+00],\n",
       "        [ 3.3588e+00, -3.3587e+00,  0.0000e+00],\n",
       "        [-3.3588e+00,  3.3587e+00,  1.0000e+00],\n",
       "        [ 3.8976e+00, -2.6042e+00,  0.0000e+00],\n",
       "        [-3.8976e+00,  2.6042e+00,  1.0000e+00],\n",
       "        [ 4.2730e+00, -1.7699e+00,  0.0000e+00],\n",
       "        [-4.2730e+00,  1.7699e+00,  1.0000e+00],\n",
       "        [ 4.4749e+00, -8.9004e-01,  0.0000e+00],\n",
       "        [-4.4749e+00,  8.9004e-01,  1.0000e+00],\n",
       "        [ 4.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-4.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 4.3522e+00,  8.6578e-01,  0.0000e+00],\n",
       "        [-4.3522e+00, -8.6578e-01,  1.0000e+00],\n",
       "        [ 4.0420e+00,  1.6743e+00,  0.0000e+00],\n",
       "        [-4.0420e+00, -1.6743e+00,  1.0000e+00],\n",
       "        [ 3.5857e+00,  2.3960e+00,  0.0000e+00],\n",
       "        [-3.5857e+00, -2.3960e+00,  1.0000e+00],\n",
       "        [ 3.0052e+00,  3.0052e+00,  0.0000e+00],\n",
       "        [-3.0052e+00, -3.0052e+00,  1.0000e+00],\n",
       "        [ 2.3264e+00,  3.4818e+00,  0.0000e+00],\n",
       "        [-2.3264e+00, -3.4818e+00,  1.0000e+00],\n",
       "        [ 1.5785e+00,  3.8110e+00,  0.0000e+00],\n",
       "        [-1.5785e+00, -3.8110e+00,  1.0000e+00],\n",
       "        [ 7.9248e-01,  3.9845e+00,  0.0000e+00],\n",
       "        [-7.9248e-01, -3.9845e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  4.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -4.0000e+00,  1.0000e+00],\n",
       "        [-7.6824e-01,  3.8618e+00,  0.0000e+00],\n",
       "        [ 7.6824e-01, -3.8618e+00,  1.0000e+00],\n",
       "        [-1.4830e+00,  3.5800e+00,  0.0000e+00],\n",
       "        [ 1.4830e+00, -3.5800e+00,  1.0000e+00],\n",
       "        [-2.1182e+00,  3.1699e+00,  0.0000e+00],\n",
       "        [ 2.1182e+00, -3.1699e+00,  1.0000e+00],\n",
       "        [-2.6517e+00,  2.6516e+00,  0.0000e+00],\n",
       "        [ 2.6517e+00, -2.6516e+00,  1.0000e+00],\n",
       "        [-3.0661e+00,  2.0486e+00,  0.0000e+00],\n",
       "        [ 3.0661e+00, -2.0486e+00,  1.0000e+00],\n",
       "        [-3.3491e+00,  1.3872e+00,  0.0000e+00],\n",
       "        [ 3.3491e+00, -1.3872e+00,  1.0000e+00],\n",
       "        [-3.4941e+00,  6.9493e-01,  0.0000e+00],\n",
       "        [ 3.4941e+00, -6.9493e-01,  1.0000e+00],\n",
       "        [-3.5000e+00, -8.0000e-05,  0.0000e+00],\n",
       "        [ 3.5000e+00,  8.0000e-05,  1.0000e+00],\n",
       "        [-3.3714e+00, -6.7070e-01,  0.0000e+00],\n",
       "        [ 3.3714e+00,  6.7070e-01,  1.0000e+00],\n",
       "        [-3.1181e+00, -1.2916e+00,  0.0000e+00],\n",
       "        [ 3.1181e+00,  1.2916e+00,  1.0000e+00],\n",
       "        [-2.7542e+00, -1.8404e+00,  0.0000e+00],\n",
       "        [ 2.7542e+00,  1.8404e+00,  1.0000e+00],\n",
       "        [-2.2980e+00, -2.2982e+00,  0.0000e+00],\n",
       "        [ 2.2980e+00,  2.2982e+00,  1.0000e+00],\n",
       "        [-1.7708e+00, -2.6504e+00,  0.0000e+00],\n",
       "        [ 1.7708e+00,  2.6504e+00,  1.0000e+00],\n",
       "        [-1.1958e+00, -2.8872e+00,  0.0000e+00],\n",
       "        [ 1.1958e+00,  2.8872e+00,  1.0000e+00],\n",
       "        [-5.9739e-01, -3.0037e+00,  0.0000e+00],\n",
       "        [ 5.9739e-01,  3.0037e+00,  1.0000e+00],\n",
       "        [ 8.0000e-05, -3.0000e+00,  0.0000e+00],\n",
       "        [-8.0000e-05,  3.0000e+00,  1.0000e+00],\n",
       "        [ 5.7315e-01, -2.8810e+00,  0.0000e+00],\n",
       "        [-5.7315e-01,  2.8810e+00,  1.0000e+00],\n",
       "        [ 1.1003e+00, -2.6561e+00,  0.0000e+00],\n",
       "        [-1.1003e+00,  2.6561e+00,  1.0000e+00],\n",
       "        [ 1.5626e+00, -2.3385e+00,  0.0000e+00],\n",
       "        [-1.5626e+00,  2.3385e+00,  1.0000e+00],\n",
       "        [ 1.9446e+00, -1.9445e+00,  0.0000e+00],\n",
       "        [-1.9446e+00,  1.9445e+00,  1.0000e+00],\n",
       "        [ 2.2346e+00, -1.4930e+00,  0.0000e+00],\n",
       "        [-2.2346e+00,  1.4930e+00,  1.0000e+00],\n",
       "        [ 2.4252e+00, -1.0045e+00,  0.0000e+00],\n",
       "        [-2.4252e+00,  1.0045e+00,  1.0000e+00],\n",
       "        [ 2.5133e+00, -4.9985e-01,  0.0000e+00],\n",
       "        [-2.5133e+00,  4.9985e-01,  1.0000e+00],\n",
       "        [ 2.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-2.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 2.3907e+00,  4.7560e-01,  0.0000e+00],\n",
       "        [-2.3907e+00, -4.7560e-01,  1.0000e+00],\n",
       "        [ 2.1942e+00,  9.0894e-01,  0.0000e+00],\n",
       "        [-2.1942e+00, -9.0894e-01,  1.0000e+00],\n",
       "        [ 1.9227e+00,  1.2848e+00,  0.0000e+00],\n",
       "        [-1.9227e+00, -1.2848e+00,  1.0000e+00],\n",
       "        [ 1.5909e+00,  1.5910e+00,  0.0000e+00],\n",
       "        [-1.5909e+00, -1.5910e+00,  1.0000e+00],\n",
       "        [ 1.2153e+00,  1.8189e+00,  0.0000e+00],\n",
       "        [-1.2153e+00, -1.8189e+00,  1.0000e+00],\n",
       "        [ 8.1314e-01,  1.9633e+00,  0.0000e+00],\n",
       "        [-8.1314e-01, -1.9633e+00,  1.0000e+00],\n",
       "        [ 4.0231e-01,  2.0229e+00,  0.0000e+00],\n",
       "        [-4.0231e-01, -2.0229e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  2.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -2.0000e+00,  1.0000e+00],\n",
       "        [-3.7805e-01,  1.9003e+00,  0.0000e+00],\n",
       "        [ 3.7805e-01, -1.9003e+00,  1.0000e+00],\n",
       "        [-7.1759e-01,  1.7322e+00,  0.0000e+00],\n",
       "        [ 7.1759e-01, -1.7322e+00,  1.0000e+00],\n",
       "        [-1.0070e+00,  1.5070e+00,  0.0000e+00],\n",
       "        [ 1.0070e+00, -1.5070e+00,  1.0000e+00],\n",
       "        [-1.2375e+00,  1.2374e+00,  0.0000e+00],\n",
       "        [ 1.2375e+00, -1.2374e+00,  1.0000e+00],\n",
       "        [-1.4031e+00,  9.3748e-01,  0.0000e+00],\n",
       "        [ 1.4031e+00, -9.3748e-01,  1.0000e+00],\n",
       "        [-1.5013e+00,  6.2181e-01,  0.0000e+00],\n",
       "        [ 1.5013e+00, -6.2181e-01,  1.0000e+00],\n",
       "        [-1.5325e+00,  3.0477e-01,  0.0000e+00],\n",
       "        [ 1.5325e+00, -3.0477e-01,  1.0000e+00],\n",
       "        [-1.5000e+00, -6.0000e-05,  0.0000e+00],\n",
       "        [ 1.5000e+00,  6.0000e-05,  1.0000e+00],\n",
       "        [-1.4099e+00, -2.8049e-01,  0.0000e+00],\n",
       "        [ 1.4099e+00,  2.8049e-01,  1.0000e+00],\n",
       "        [-1.2703e+00, -5.2624e-01,  0.0000e+00],\n",
       "        [ 1.2703e+00,  5.2624e-01,  1.0000e+00],\n",
       "        [-1.0913e+00, -7.2923e-01,  0.0000e+00],\n",
       "        [ 1.0913e+00,  7.2923e-01,  1.0000e+00],\n",
       "        [-8.8385e-01, -8.8392e-01,  0.0000e+00],\n",
       "        [ 8.8385e-01,  8.8392e-01,  1.0000e+00],\n",
       "        [-6.5970e-01, -9.8740e-01,  0.0000e+00],\n",
       "        [ 6.5970e-01,  9.8740e-01,  1.0000e+00],\n",
       "        [-4.3048e-01, -1.0394e+00,  0.0000e+00],\n",
       "        [ 4.3048e-01,  1.0394e+00,  1.0000e+00],\n",
       "        [-2.0724e-01, -1.0421e+00,  0.0000e+00],\n",
       "        [ 2.0724e-01,  1.0421e+00,  1.0000e+00],\n",
       "        [ 4.0000e-05, -1.0000e+00,  0.0000e+00],\n",
       "        [-4.0000e-05,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.8293e-01, -9.1948e-01,  0.0000e+00],\n",
       "        [-1.8293e-01,  9.1948e-01,  1.0000e+00],\n",
       "        [ 3.3488e-01, -8.0838e-01,  0.0000e+00],\n",
       "        [-3.3488e-01,  8.0838e-01,  1.0000e+00],\n",
       "        [ 4.5143e-01, -6.7555e-01,  0.0000e+00],\n",
       "        [-4.5143e-01,  6.7555e-01,  1.0000e+00],\n",
       "        [ 5.3035e-01, -5.3031e-01,  0.0000e+00],\n",
       "        [-5.3035e-01,  5.3031e-01,  1.0000e+00],\n",
       "        [ 5.7165e-01, -3.8193e-01,  0.0000e+00],\n",
       "        [-5.7165e-01,  3.8193e-01,  1.0000e+00],\n",
       "        [ 5.7744e-01, -2.3915e-01,  0.0000e+00],\n",
       "        [-5.7744e-01,  2.3915e-01,  1.0000e+00],\n",
       "        [ 5.5170e-01, -1.0971e-01,  0.0000e+00],\n",
       "        [-5.5170e-01,  1.0971e-01,  1.0000e+00],\n",
       "        [ 5.0000e-01,  2.0000e-05,  0.0000e+00],\n",
       "        [-5.0000e-01, -2.0000e-05,  1.0000e+00]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data #transform all data into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_input # number of features without the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data\n",
    "full_input  = data[:,0:num_input]\n",
    "full_target = data[:,num_input:num_input+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000e+00,  0.0000e+00],\n",
       "        [-6.5000e+00, -0.0000e+00],\n",
       "        [ 6.3138e+00,  1.2559e+00],\n",
       "        [-6.3138e+00, -1.2559e+00],\n",
       "        [ 5.8897e+00,  2.4396e+00],\n",
       "        [-5.8897e+00, -2.4396e+00],\n",
       "        [ 5.2487e+00,  3.5070e+00],\n",
       "        [-5.2487e+00, -3.5070e+00],\n",
       "        [ 4.4194e+00,  4.4194e+00],\n",
       "        [-4.4194e+00, -4.4194e+00],\n",
       "        [ 3.4376e+00,  5.1447e+00],\n",
       "        [-3.4376e+00, -5.1447e+00],\n",
       "        [ 2.3439e+00,  5.6588e+00],\n",
       "        [-2.3439e+00, -5.6588e+00],\n",
       "        [ 1.1827e+00,  5.9460e+00],\n",
       "        [-1.1827e+00, -5.9460e+00],\n",
       "        [-2.0000e-05,  6.0000e+00],\n",
       "        [ 2.0000e-05, -6.0000e+00],\n",
       "        [-1.1584e+00,  5.8234e+00],\n",
       "        [ 1.1584e+00, -5.8234e+00],\n",
       "        [-2.2483e+00,  5.4278e+00],\n",
       "        [ 2.2483e+00, -5.4278e+00],\n",
       "        [-3.2293e+00,  4.8329e+00],\n",
       "        [ 3.2293e+00, -4.8329e+00],\n",
       "        [-4.0659e+00,  4.0658e+00],\n",
       "        [ 4.0659e+00, -4.0658e+00],\n",
       "        [-4.7290e+00,  3.1598e+00],\n",
       "        [ 4.7290e+00, -3.1598e+00],\n",
       "        [-5.1968e+00,  2.1526e+00],\n",
       "        [ 5.1968e+00, -2.1526e+00],\n",
       "        [-5.4556e+00,  1.0852e+00],\n",
       "        [ 5.4556e+00, -1.0852e+00],\n",
       "        [-5.5000e+00, -4.0000e-05],\n",
       "        [ 5.5000e+00,  4.0000e-05],\n",
       "        [-5.3330e+00, -1.0609e+00],\n",
       "        [ 5.3330e+00,  1.0609e+00],\n",
       "        [-4.9658e+00, -2.0570e+00],\n",
       "        [ 4.9658e+00,  2.0570e+00],\n",
       "        [-4.4172e+00, -2.9515e+00],\n",
       "        [ 4.4172e+00,  2.9515e+00],\n",
       "        [-3.7123e+00, -3.7123e+00],\n",
       "        [ 3.7123e+00,  3.7123e+00],\n",
       "        [-2.8820e+00, -4.3133e+00],\n",
       "        [ 2.8820e+00,  4.3133e+00],\n",
       "        [-1.9612e+00, -4.7349e+00],\n",
       "        [ 1.9612e+00,  4.7349e+00],\n",
       "        [-9.8759e-01, -4.9652e+00],\n",
       "        [ 9.8759e-01,  4.9652e+00],\n",
       "        [ 6.0000e-05, -5.0000e+00],\n",
       "        [-6.0000e-05,  5.0000e+00],\n",
       "        [ 9.6331e-01, -4.8426e+00],\n",
       "        [-9.6331e-01,  4.8426e+00],\n",
       "        [ 1.8656e+00, -4.5039e+00],\n",
       "        [-1.8656e+00,  4.5039e+00],\n",
       "        [ 2.6737e+00, -4.0014e+00],\n",
       "        [-2.6737e+00,  4.0014e+00],\n",
       "        [ 3.3588e+00, -3.3587e+00],\n",
       "        [-3.3588e+00,  3.3587e+00],\n",
       "        [ 3.8976e+00, -2.6042e+00],\n",
       "        [-3.8976e+00,  2.6042e+00],\n",
       "        [ 4.2730e+00, -1.7699e+00],\n",
       "        [-4.2730e+00,  1.7699e+00],\n",
       "        [ 4.4749e+00, -8.9004e-01],\n",
       "        [-4.4749e+00,  8.9004e-01],\n",
       "        [ 4.5000e+00,  7.0000e-05],\n",
       "        [-4.5000e+00, -7.0000e-05],\n",
       "        [ 4.3522e+00,  8.6578e-01],\n",
       "        [-4.3522e+00, -8.6578e-01],\n",
       "        [ 4.0420e+00,  1.6743e+00],\n",
       "        [-4.0420e+00, -1.6743e+00],\n",
       "        [ 3.5857e+00,  2.3960e+00],\n",
       "        [-3.5857e+00, -2.3960e+00],\n",
       "        [ 3.0052e+00,  3.0052e+00],\n",
       "        [-3.0052e+00, -3.0052e+00],\n",
       "        [ 2.3264e+00,  3.4818e+00],\n",
       "        [-2.3264e+00, -3.4818e+00],\n",
       "        [ 1.5785e+00,  3.8110e+00],\n",
       "        [-1.5785e+00, -3.8110e+00],\n",
       "        [ 7.9248e-01,  3.9845e+00],\n",
       "        [-7.9248e-01, -3.9845e+00],\n",
       "        [-7.0000e-05,  4.0000e+00],\n",
       "        [ 7.0000e-05, -4.0000e+00],\n",
       "        [-7.6824e-01,  3.8618e+00],\n",
       "        [ 7.6824e-01, -3.8618e+00],\n",
       "        [-1.4830e+00,  3.5800e+00],\n",
       "        [ 1.4830e+00, -3.5800e+00],\n",
       "        [-2.1182e+00,  3.1699e+00],\n",
       "        [ 2.1182e+00, -3.1699e+00],\n",
       "        [-2.6517e+00,  2.6516e+00],\n",
       "        [ 2.6517e+00, -2.6516e+00],\n",
       "        [-3.0661e+00,  2.0486e+00],\n",
       "        [ 3.0661e+00, -2.0486e+00],\n",
       "        [-3.3491e+00,  1.3872e+00],\n",
       "        [ 3.3491e+00, -1.3872e+00],\n",
       "        [-3.4941e+00,  6.9493e-01],\n",
       "        [ 3.4941e+00, -6.9493e-01],\n",
       "        [-3.5000e+00, -8.0000e-05],\n",
       "        [ 3.5000e+00,  8.0000e-05],\n",
       "        [-3.3714e+00, -6.7070e-01],\n",
       "        [ 3.3714e+00,  6.7070e-01],\n",
       "        [-3.1181e+00, -1.2916e+00],\n",
       "        [ 3.1181e+00,  1.2916e+00],\n",
       "        [-2.7542e+00, -1.8404e+00],\n",
       "        [ 2.7542e+00,  1.8404e+00],\n",
       "        [-2.2980e+00, -2.2982e+00],\n",
       "        [ 2.2980e+00,  2.2982e+00],\n",
       "        [-1.7708e+00, -2.6504e+00],\n",
       "        [ 1.7708e+00,  2.6504e+00],\n",
       "        [-1.1958e+00, -2.8872e+00],\n",
       "        [ 1.1958e+00,  2.8872e+00],\n",
       "        [-5.9739e-01, -3.0037e+00],\n",
       "        [ 5.9739e-01,  3.0037e+00],\n",
       "        [ 8.0000e-05, -3.0000e+00],\n",
       "        [-8.0000e-05,  3.0000e+00],\n",
       "        [ 5.7315e-01, -2.8810e+00],\n",
       "        [-5.7315e-01,  2.8810e+00],\n",
       "        [ 1.1003e+00, -2.6561e+00],\n",
       "        [-1.1003e+00,  2.6561e+00],\n",
       "        [ 1.5626e+00, -2.3385e+00],\n",
       "        [-1.5626e+00,  2.3385e+00],\n",
       "        [ 1.9446e+00, -1.9445e+00],\n",
       "        [-1.9446e+00,  1.9445e+00],\n",
       "        [ 2.2346e+00, -1.4930e+00],\n",
       "        [-2.2346e+00,  1.4930e+00],\n",
       "        [ 2.4252e+00, -1.0045e+00],\n",
       "        [-2.4252e+00,  1.0045e+00],\n",
       "        [ 2.5133e+00, -4.9985e-01],\n",
       "        [-2.5133e+00,  4.9985e-01],\n",
       "        [ 2.5000e+00,  7.0000e-05],\n",
       "        [-2.5000e+00, -7.0000e-05],\n",
       "        [ 2.3907e+00,  4.7560e-01],\n",
       "        [-2.3907e+00, -4.7560e-01],\n",
       "        [ 2.1942e+00,  9.0894e-01],\n",
       "        [-2.1942e+00, -9.0894e-01],\n",
       "        [ 1.9227e+00,  1.2848e+00],\n",
       "        [-1.9227e+00, -1.2848e+00],\n",
       "        [ 1.5909e+00,  1.5910e+00],\n",
       "        [-1.5909e+00, -1.5910e+00],\n",
       "        [ 1.2153e+00,  1.8189e+00],\n",
       "        [-1.2153e+00, -1.8189e+00],\n",
       "        [ 8.1314e-01,  1.9633e+00],\n",
       "        [-8.1314e-01, -1.9633e+00],\n",
       "        [ 4.0231e-01,  2.0229e+00],\n",
       "        [-4.0231e-01, -2.0229e+00],\n",
       "        [-7.0000e-05,  2.0000e+00],\n",
       "        [ 7.0000e-05, -2.0000e+00],\n",
       "        [-3.7805e-01,  1.9003e+00],\n",
       "        [ 3.7805e-01, -1.9003e+00],\n",
       "        [-7.1759e-01,  1.7322e+00],\n",
       "        [ 7.1759e-01, -1.7322e+00],\n",
       "        [-1.0070e+00,  1.5070e+00],\n",
       "        [ 1.0070e+00, -1.5070e+00],\n",
       "        [-1.2375e+00,  1.2374e+00],\n",
       "        [ 1.2375e+00, -1.2374e+00],\n",
       "        [-1.4031e+00,  9.3748e-01],\n",
       "        [ 1.4031e+00, -9.3748e-01],\n",
       "        [-1.5013e+00,  6.2181e-01],\n",
       "        [ 1.5013e+00, -6.2181e-01],\n",
       "        [-1.5325e+00,  3.0477e-01],\n",
       "        [ 1.5325e+00, -3.0477e-01],\n",
       "        [-1.5000e+00, -6.0000e-05],\n",
       "        [ 1.5000e+00,  6.0000e-05],\n",
       "        [-1.4099e+00, -2.8049e-01],\n",
       "        [ 1.4099e+00,  2.8049e-01],\n",
       "        [-1.2703e+00, -5.2624e-01],\n",
       "        [ 1.2703e+00,  5.2624e-01],\n",
       "        [-1.0913e+00, -7.2923e-01],\n",
       "        [ 1.0913e+00,  7.2923e-01],\n",
       "        [-8.8385e-01, -8.8392e-01],\n",
       "        [ 8.8385e-01,  8.8392e-01],\n",
       "        [-6.5970e-01, -9.8740e-01],\n",
       "        [ 6.5970e-01,  9.8740e-01],\n",
       "        [-4.3048e-01, -1.0394e+00],\n",
       "        [ 4.3048e-01,  1.0394e+00],\n",
       "        [-2.0724e-01, -1.0421e+00],\n",
       "        [ 2.0724e-01,  1.0421e+00],\n",
       "        [ 4.0000e-05, -1.0000e+00],\n",
       "        [-4.0000e-05,  1.0000e+00],\n",
       "        [ 1.8293e-01, -9.1948e-01],\n",
       "        [-1.8293e-01,  9.1948e-01],\n",
       "        [ 3.3488e-01, -8.0838e-01],\n",
       "        [-3.3488e-01,  8.0838e-01],\n",
       "        [ 4.5143e-01, -6.7555e-01],\n",
       "        [-4.5143e-01,  6.7555e-01],\n",
       "        [ 5.3035e-01, -5.3031e-01],\n",
       "        [-5.3035e-01,  5.3031e-01],\n",
       "        [ 5.7165e-01, -3.8193e-01],\n",
       "        [-5.7165e-01,  3.8193e-01],\n",
       "        [ 5.7744e-01, -2.3915e-01],\n",
       "        [-5.7744e-01,  2.3915e-01],\n",
       "        [ 5.5170e-01, -1.0971e-01],\n",
       "        [-5.5170e-01,  1.0971e-01],\n",
       "        [ 5.0000e-01,  2.0000e-05],\n",
       "        [-5.0000e-01, -2.0000e-05]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_input #this will be fed in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_target #this will be used for the output loss calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(full_input,full_target)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset,batch_size=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x17b2e91e0b8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset #transforming to speed training in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x17b2e91e080>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 6.5000e+00,  0.0000e+00],\n",
      "        [-6.5000e+00, -0.0000e+00],\n",
      "        [ 6.3138e+00,  1.2559e+00],\n",
      "        [-6.3138e+00, -1.2559e+00],\n",
      "        [ 5.8897e+00,  2.4396e+00],\n",
      "        [-5.8897e+00, -2.4396e+00],\n",
      "        [ 5.2487e+00,  3.5070e+00],\n",
      "        [-5.2487e+00, -3.5070e+00],\n",
      "        [ 4.4194e+00,  4.4194e+00],\n",
      "        [-4.4194e+00, -4.4194e+00],\n",
      "        [ 3.4376e+00,  5.1447e+00],\n",
      "        [-3.4376e+00, -5.1447e+00],\n",
      "        [ 2.3439e+00,  5.6588e+00],\n",
      "        [-2.3439e+00, -5.6588e+00],\n",
      "        [ 1.1827e+00,  5.9460e+00],\n",
      "        [-1.1827e+00, -5.9460e+00],\n",
      "        [-2.0000e-05,  6.0000e+00],\n",
      "        [ 2.0000e-05, -6.0000e+00],\n",
      "        [-1.1584e+00,  5.8234e+00],\n",
      "        [ 1.1584e+00, -5.8234e+00],\n",
      "        [-2.2483e+00,  5.4278e+00],\n",
      "        [ 2.2483e+00, -5.4278e+00],\n",
      "        [-3.2293e+00,  4.8329e+00],\n",
      "        [ 3.2293e+00, -4.8329e+00],\n",
      "        [-4.0659e+00,  4.0658e+00],\n",
      "        [ 4.0659e+00, -4.0658e+00],\n",
      "        [-4.7290e+00,  3.1598e+00],\n",
      "        [ 4.7290e+00, -3.1598e+00],\n",
      "        [-5.1968e+00,  2.1526e+00],\n",
      "        [ 5.1968e+00, -2.1526e+00],\n",
      "        [-5.4556e+00,  1.0852e+00],\n",
      "        [ 5.4556e+00, -1.0852e+00],\n",
      "        [-5.5000e+00, -4.0000e-05],\n",
      "        [ 5.5000e+00,  4.0000e-05],\n",
      "        [-5.3330e+00, -1.0609e+00],\n",
      "        [ 5.3330e+00,  1.0609e+00],\n",
      "        [-4.9658e+00, -2.0570e+00],\n",
      "        [ 4.9658e+00,  2.0570e+00],\n",
      "        [-4.4172e+00, -2.9515e+00],\n",
      "        [ 4.4172e+00,  2.9515e+00],\n",
      "        [-3.7123e+00, -3.7123e+00],\n",
      "        [ 3.7123e+00,  3.7123e+00],\n",
      "        [-2.8820e+00, -4.3133e+00],\n",
      "        [ 2.8820e+00,  4.3133e+00],\n",
      "        [-1.9612e+00, -4.7349e+00],\n",
      "        [ 1.9612e+00,  4.7349e+00],\n",
      "        [-9.8759e-01, -4.9652e+00],\n",
      "        [ 9.8759e-01,  4.9652e+00],\n",
      "        [ 6.0000e-05, -5.0000e+00],\n",
      "        [-6.0000e-05,  5.0000e+00],\n",
      "        [ 9.6331e-01, -4.8426e+00],\n",
      "        [-9.6331e-01,  4.8426e+00],\n",
      "        [ 1.8656e+00, -4.5039e+00],\n",
      "        [-1.8656e+00,  4.5039e+00],\n",
      "        [ 2.6737e+00, -4.0014e+00],\n",
      "        [-2.6737e+00,  4.0014e+00],\n",
      "        [ 3.3588e+00, -3.3587e+00],\n",
      "        [-3.3588e+00,  3.3587e+00],\n",
      "        [ 3.8976e+00, -2.6042e+00],\n",
      "        [-3.8976e+00,  2.6042e+00],\n",
      "        [ 4.2730e+00, -1.7699e+00],\n",
      "        [-4.2730e+00,  1.7699e+00],\n",
      "        [ 4.4749e+00, -8.9004e-01],\n",
      "        [-4.4749e+00,  8.9004e-01],\n",
      "        [ 4.5000e+00,  7.0000e-05],\n",
      "        [-4.5000e+00, -7.0000e-05],\n",
      "        [ 4.3522e+00,  8.6578e-01],\n",
      "        [-4.3522e+00, -8.6578e-01],\n",
      "        [ 4.0420e+00,  1.6743e+00],\n",
      "        [-4.0420e+00, -1.6743e+00],\n",
      "        [ 3.5857e+00,  2.3960e+00],\n",
      "        [-3.5857e+00, -2.3960e+00],\n",
      "        [ 3.0052e+00,  3.0052e+00],\n",
      "        [-3.0052e+00, -3.0052e+00],\n",
      "        [ 2.3264e+00,  3.4818e+00],\n",
      "        [-2.3264e+00, -3.4818e+00],\n",
      "        [ 1.5785e+00,  3.8110e+00],\n",
      "        [-1.5785e+00, -3.8110e+00],\n",
      "        [ 7.9248e-01,  3.9845e+00],\n",
      "        [-7.9248e-01, -3.9845e+00],\n",
      "        [-7.0000e-05,  4.0000e+00],\n",
      "        [ 7.0000e-05, -4.0000e+00],\n",
      "        [-7.6824e-01,  3.8618e+00],\n",
      "        [ 7.6824e-01, -3.8618e+00],\n",
      "        [-1.4830e+00,  3.5800e+00],\n",
      "        [ 1.4830e+00, -3.5800e+00],\n",
      "        [-2.1182e+00,  3.1699e+00],\n",
      "        [ 2.1182e+00, -3.1699e+00],\n",
      "        [-2.6517e+00,  2.6516e+00],\n",
      "        [ 2.6517e+00, -2.6516e+00],\n",
      "        [-3.0661e+00,  2.0486e+00],\n",
      "        [ 3.0661e+00, -2.0486e+00],\n",
      "        [-3.3491e+00,  1.3872e+00],\n",
      "        [ 3.3491e+00, -1.3872e+00],\n",
      "        [-3.4941e+00,  6.9493e-01],\n",
      "        [ 3.4941e+00, -6.9493e-01],\n",
      "        [-3.5000e+00, -8.0000e-05]])\n"
     ]
    }
   ],
   "source": [
    "for _, (data_feed,target) in enumerate(train_loader):\n",
    "    print('x:',data_feed)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.5000, 0.0000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.5000e+00, -6.5000e+00,  6.3138e+00, -6.3138e+00,  5.8897e+00,\n",
       "        -5.8897e+00,  5.2487e+00, -5.2487e+00,  4.4194e+00, -4.4194e+00,\n",
       "         3.4376e+00, -3.4376e+00,  2.3439e+00, -2.3439e+00,  1.1827e+00,\n",
       "        -1.1827e+00, -2.0000e-05,  2.0000e-05, -1.1584e+00,  1.1584e+00,\n",
       "        -2.2483e+00,  2.2483e+00, -3.2293e+00,  3.2293e+00, -4.0659e+00,\n",
       "         4.0659e+00, -4.7290e+00,  4.7290e+00, -5.1968e+00,  5.1968e+00,\n",
       "        -5.4556e+00,  5.4556e+00, -5.5000e+00,  5.5000e+00, -5.3330e+00,\n",
       "         5.3330e+00, -4.9658e+00,  4.9658e+00, -4.4172e+00,  4.4172e+00,\n",
       "        -3.7123e+00,  3.7123e+00, -2.8820e+00,  2.8820e+00, -1.9612e+00,\n",
       "         1.9612e+00, -9.8759e-01,  9.8759e-01,  6.0000e-05, -6.0000e-05,\n",
       "         9.6331e-01, -9.6331e-01,  1.8656e+00, -1.8656e+00,  2.6737e+00,\n",
       "        -2.6737e+00,  3.3588e+00, -3.3588e+00,  3.8976e+00, -3.8976e+00,\n",
       "         4.2730e+00, -4.2730e+00,  4.4749e+00, -4.4749e+00,  4.5000e+00,\n",
       "        -4.5000e+00,  4.3522e+00, -4.3522e+00,  4.0420e+00, -4.0420e+00,\n",
       "         3.5857e+00, -3.5857e+00,  3.0052e+00, -3.0052e+00,  2.3264e+00,\n",
       "        -2.3264e+00,  1.5785e+00, -1.5785e+00,  7.9248e-01, -7.9248e-01,\n",
       "        -7.0000e-05,  7.0000e-05, -7.6824e-01,  7.6824e-01, -1.4830e+00,\n",
       "         1.4830e+00, -2.1182e+00,  2.1182e+00, -2.6517e+00,  2.6517e+00,\n",
       "        -3.0661e+00,  3.0661e+00, -3.3491e+00,  3.3491e+00, -3.4941e+00,\n",
       "         3.4941e+00, -3.5000e+00])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_feed[:,0] # x vector\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00, -0.0000e+00,  1.2559e+00, -1.2559e+00,  2.4396e+00,\n",
       "        -2.4396e+00,  3.5070e+00, -3.5070e+00,  4.4194e+00, -4.4194e+00,\n",
       "         5.1447e+00, -5.1447e+00,  5.6588e+00, -5.6588e+00,  5.9460e+00,\n",
       "        -5.9460e+00,  6.0000e+00, -6.0000e+00,  5.8234e+00, -5.8234e+00,\n",
       "         5.4278e+00, -5.4278e+00,  4.8329e+00, -4.8329e+00,  4.0658e+00,\n",
       "        -4.0658e+00,  3.1598e+00, -3.1598e+00,  2.1526e+00, -2.1526e+00,\n",
       "         1.0852e+00, -1.0852e+00, -4.0000e-05,  4.0000e-05, -1.0609e+00,\n",
       "         1.0609e+00, -2.0570e+00,  2.0570e+00, -2.9515e+00,  2.9515e+00,\n",
       "        -3.7123e+00,  3.7123e+00, -4.3133e+00,  4.3133e+00, -4.7349e+00,\n",
       "         4.7349e+00, -4.9652e+00,  4.9652e+00, -5.0000e+00,  5.0000e+00,\n",
       "        -4.8426e+00,  4.8426e+00, -4.5039e+00,  4.5039e+00, -4.0014e+00,\n",
       "         4.0014e+00, -3.3587e+00,  3.3587e+00, -2.6042e+00,  2.6042e+00,\n",
       "        -1.7699e+00,  1.7699e+00, -8.9004e-01,  8.9004e-01,  7.0000e-05,\n",
       "        -7.0000e-05,  8.6578e-01, -8.6578e-01,  1.6743e+00, -1.6743e+00,\n",
       "         2.3960e+00, -2.3960e+00,  3.0052e+00, -3.0052e+00,  3.4818e+00,\n",
       "        -3.4818e+00,  3.8110e+00, -3.8110e+00,  3.9845e+00, -3.9845e+00,\n",
       "         4.0000e+00, -4.0000e+00,  3.8618e+00, -3.8618e+00,  3.5800e+00,\n",
       "        -3.5800e+00,  3.1699e+00, -3.1699e+00,  2.6516e+00, -2.6516e+00,\n",
       "         2.0486e+00, -2.0486e+00,  1.3872e+00, -1.3872e+00,  6.9493e-01,\n",
       "        -6.9493e-01, -8.0000e-05])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data_feed[:,1] # y vector \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.atan2(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.sqrt(x*x + y*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.zeros(data_feed.shape,dtype=torch.float32)\n",
    "out[:,0] = a\n",
    "out[:,1] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  6.5000e+00],\n",
       "        [-3.1416e+00,  6.5000e+00],\n",
       "        [ 1.9635e-01,  6.4375e+00],\n",
       "        [-2.9452e+00,  6.4375e+00],\n",
       "        [ 3.9270e-01,  6.3750e+00],\n",
       "        [-2.7489e+00,  6.3750e+00],\n",
       "        [ 5.8905e-01,  6.3125e+00],\n",
       "        [-2.5525e+00,  6.3125e+00],\n",
       "        [ 7.8540e-01,  6.2500e+00],\n",
       "        [-2.3562e+00,  6.2500e+00],\n",
       "        [ 9.8175e-01,  6.1875e+00],\n",
       "        [-2.1598e+00,  6.1875e+00],\n",
       "        [ 1.1781e+00,  6.1250e+00],\n",
       "        [-1.9635e+00,  6.1250e+00],\n",
       "        [ 1.3744e+00,  6.0625e+00],\n",
       "        [-1.7671e+00,  6.0625e+00],\n",
       "        [ 1.5708e+00,  6.0000e+00],\n",
       "        [-1.5708e+00,  6.0000e+00],\n",
       "        [ 1.7671e+00,  5.9375e+00],\n",
       "        [-1.3744e+00,  5.9375e+00],\n",
       "        [ 1.9635e+00,  5.8750e+00],\n",
       "        [-1.1781e+00,  5.8750e+00],\n",
       "        [ 2.1599e+00,  5.8125e+00],\n",
       "        [-9.8174e-01,  5.8125e+00],\n",
       "        [ 2.3562e+00,  5.7500e+00],\n",
       "        [-7.8539e-01,  5.7500e+00],\n",
       "        [ 2.5525e+00,  5.6875e+00],\n",
       "        [-5.8904e-01,  5.6875e+00],\n",
       "        [ 2.7489e+00,  5.6250e+00],\n",
       "        [-3.9269e-01,  5.6250e+00],\n",
       "        [ 2.9453e+00,  5.5625e+00],\n",
       "        [-1.9634e-01,  5.5625e+00],\n",
       "        [-3.1416e+00,  5.5000e+00],\n",
       "        [ 7.2727e-06,  5.5000e+00],\n",
       "        [-2.9452e+00,  5.4375e+00],\n",
       "        [ 1.9636e-01,  5.4375e+00],\n",
       "        [-2.7489e+00,  5.3750e+00],\n",
       "        [ 3.9271e-01,  5.3750e+00],\n",
       "        [-2.5525e+00,  5.3125e+00],\n",
       "        [ 5.8906e-01,  5.3125e+00],\n",
       "        [-2.3562e+00,  5.2500e+00],\n",
       "        [ 7.8541e-01,  5.2500e+00],\n",
       "        [-2.1598e+00,  5.1875e+00],\n",
       "        [ 9.8176e-01,  5.1875e+00],\n",
       "        [-1.9635e+00,  5.1250e+00],\n",
       "        [ 1.1781e+00,  5.1250e+00],\n",
       "        [-1.7671e+00,  5.0625e+00],\n",
       "        [ 1.3745e+00,  5.0625e+00],\n",
       "        [-1.5708e+00,  5.0000e+00],\n",
       "        [ 1.5708e+00,  5.0000e+00],\n",
       "        [-1.3744e+00,  4.9375e+00],\n",
       "        [ 1.7672e+00,  4.9375e+00],\n",
       "        [-1.1781e+00,  4.8750e+00],\n",
       "        [ 1.9635e+00,  4.8750e+00],\n",
       "        [-9.8174e-01,  4.8125e+00],\n",
       "        [ 2.1599e+00,  4.8125e+00],\n",
       "        [-7.8538e-01,  4.7500e+00],\n",
       "        [ 2.3562e+00,  4.7500e+00],\n",
       "        [-5.8903e-01,  4.6875e+00],\n",
       "        [ 2.5526e+00,  4.6875e+00],\n",
       "        [-3.9268e-01,  4.6250e+00],\n",
       "        [ 2.7489e+00,  4.6250e+00],\n",
       "        [-1.9634e-01,  4.5625e+00],\n",
       "        [ 2.9453e+00,  4.5625e+00],\n",
       "        [ 1.5556e-05,  4.5000e+00],\n",
       "        [-3.1416e+00,  4.5000e+00],\n",
       "        [ 1.9636e-01,  4.4375e+00],\n",
       "        [-2.9452e+00,  4.4375e+00],\n",
       "        [ 3.9271e-01,  4.3750e+00],\n",
       "        [-2.7489e+00,  4.3750e+00],\n",
       "        [ 5.8906e-01,  4.3125e+00],\n",
       "        [-2.5525e+00,  4.3125e+00],\n",
       "        [ 7.8541e-01,  4.2500e+00],\n",
       "        [-2.3562e+00,  4.2500e+00],\n",
       "        [ 9.8177e-01,  4.1875e+00],\n",
       "        [-2.1598e+00,  4.1875e+00],\n",
       "        [ 1.1781e+00,  4.1250e+00],\n",
       "        [-1.9635e+00,  4.1250e+00],\n",
       "        [ 1.3745e+00,  4.0625e+00],\n",
       "        [-1.7671e+00,  4.0625e+00],\n",
       "        [ 1.5708e+00,  4.0000e+00],\n",
       "        [-1.5708e+00,  4.0000e+00],\n",
       "        [ 1.7672e+00,  3.9375e+00],\n",
       "        [-1.3744e+00,  3.9375e+00],\n",
       "        [ 1.9635e+00,  3.8750e+00],\n",
       "        [-1.1781e+00,  3.8750e+00],\n",
       "        [ 2.1599e+00,  3.8125e+00],\n",
       "        [-9.8173e-01,  3.8125e+00],\n",
       "        [ 2.3562e+00,  3.7500e+00],\n",
       "        [-7.8538e-01,  3.7500e+00],\n",
       "        [ 2.5526e+00,  3.6875e+00],\n",
       "        [-5.8903e-01,  3.6875e+00],\n",
       "        [ 2.7489e+00,  3.6250e+00],\n",
       "        [-3.9268e-01,  3.6250e+00],\n",
       "        [ 2.9453e+00,  3.5625e+00],\n",
       "        [-1.9633e-01,  3.5625e+00],\n",
       "        [-3.1416e+00,  3.5000e+00]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = torch.arange(start=-7,end=7.1,step=0.01,dtype=torch.float32)\n",
    "yrange = torch.arange(start=-6.6,end=6.7,step=0.01,dtype=torch.float32)\n",
    "xcoord = xrange.repeat(yrange.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0000, -6.9900, -6.9800,  ...,  7.0700,  7.0800,  7.0900])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.6000, -6.5900, -6.5800,  ...,  6.6700,  6.6800,  6.6900])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1330])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrange.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0000],\n",
       "        [-6.9900],\n",
       "        [-6.9800],\n",
       "        ...,\n",
       "        [ 7.0700],\n",
       "        [ 7.0800],\n",
       "        [ 7.0900]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcoord.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcoord = xrange.repeat(yrange.size()[0])\n",
    "ycoord = torch.repeat_interleave(yrange, xrange.size()[0], dim=0)\n",
    "grid = torch.cat((xcoord.unsqueeze(1),ycoord.unsqueeze(1)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0000, -6.9900, -6.9800,  ...,  7.0700,  7.0800,  7.0900])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.6000, -6.6000, -6.6000,  ...,  6.6900,  6.6900,  6.6900])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ycoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0000, -6.6000],\n",
       "        [-6.9900, -6.6000],\n",
       "        [-6.9800, -6.6000],\n",
       "        ...,\n",
       "        [ 7.0700,  6.6900],\n",
       "        [ 7.0800,  6.6900],\n",
       "        [ 7.0900,  6.6900]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hidden layer\n",
    "#first define the class\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RawNet(torch.nn.Module):\n",
    "    def __init__(self, num_hid):\n",
    "        super(RawNet, self).__init__()\n",
    "        self.feed = nn.Linear(2,num_hid)\n",
    "        self.hid = nn.Linear(num_hid,num_hid)\n",
    "        self.out = nn.Linear(num_hid,1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.active1 = torch.tanh(self.feed(x))\n",
    "        self.active2 = torch.tanh(self.hid(self.active1))\n",
    "        self.hidlayer = [self.active1,self.active2]\n",
    "        output = torch.sigmoid(self.out(self.active2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, optimizer):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for _, (data,target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()    # zero the gradients\n",
    "        output = net(data)       # apply network\n",
    "        loss = F.binary_cross_entropy(output,target)\n",
    "        loss.backward()          # compute gradients\n",
    "        optimizer.step()         # update weights\n",
    "        pred = (output >= 0.5).float()\n",
    "        correct += (pred == target).float().sum()\n",
    "        total += target.size()[0]\n",
    "        accuracy = 100*correct/total\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('ep:%5d loss: %6.4f acc: %5.2f' %\n",
    "             (epoch,loss.item(),accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-6.5000e+00, -0.0000e+00,  1.0000e+00],\n",
       "        [ 6.3138e+00,  1.2559e+00,  0.0000e+00],\n",
       "        [-6.3138e+00, -1.2559e+00,  1.0000e+00],\n",
       "        [ 5.8897e+00,  2.4396e+00,  0.0000e+00],\n",
       "        [-5.8897e+00, -2.4396e+00,  1.0000e+00],\n",
       "        [ 5.2487e+00,  3.5070e+00,  0.0000e+00],\n",
       "        [-5.2487e+00, -3.5070e+00,  1.0000e+00],\n",
       "        [ 4.4194e+00,  4.4194e+00,  0.0000e+00],\n",
       "        [-4.4194e+00, -4.4194e+00,  1.0000e+00],\n",
       "        [ 3.4376e+00,  5.1447e+00,  0.0000e+00],\n",
       "        [-3.4376e+00, -5.1447e+00,  1.0000e+00],\n",
       "        [ 2.3439e+00,  5.6588e+00,  0.0000e+00],\n",
       "        [-2.3439e+00, -5.6588e+00,  1.0000e+00],\n",
       "        [ 1.1827e+00,  5.9460e+00,  0.0000e+00],\n",
       "        [-1.1827e+00, -5.9460e+00,  1.0000e+00],\n",
       "        [-2.0000e-05,  6.0000e+00,  0.0000e+00],\n",
       "        [ 2.0000e-05, -6.0000e+00,  1.0000e+00],\n",
       "        [-1.1584e+00,  5.8234e+00,  0.0000e+00],\n",
       "        [ 1.1584e+00, -5.8234e+00,  1.0000e+00],\n",
       "        [-2.2483e+00,  5.4278e+00,  0.0000e+00],\n",
       "        [ 2.2483e+00, -5.4278e+00,  1.0000e+00],\n",
       "        [-3.2293e+00,  4.8329e+00,  0.0000e+00],\n",
       "        [ 3.2293e+00, -4.8329e+00,  1.0000e+00],\n",
       "        [-4.0659e+00,  4.0658e+00,  0.0000e+00],\n",
       "        [ 4.0659e+00, -4.0658e+00,  1.0000e+00],\n",
       "        [-4.7290e+00,  3.1598e+00,  0.0000e+00],\n",
       "        [ 4.7290e+00, -3.1598e+00,  1.0000e+00],\n",
       "        [-5.1968e+00,  2.1526e+00,  0.0000e+00],\n",
       "        [ 5.1968e+00, -2.1526e+00,  1.0000e+00],\n",
       "        [-5.4556e+00,  1.0852e+00,  0.0000e+00],\n",
       "        [ 5.4556e+00, -1.0852e+00,  1.0000e+00],\n",
       "        [-5.5000e+00, -4.0000e-05,  0.0000e+00],\n",
       "        [ 5.5000e+00,  4.0000e-05,  1.0000e+00],\n",
       "        [-5.3330e+00, -1.0609e+00,  0.0000e+00],\n",
       "        [ 5.3330e+00,  1.0609e+00,  1.0000e+00],\n",
       "        [-4.9658e+00, -2.0570e+00,  0.0000e+00],\n",
       "        [ 4.9658e+00,  2.0570e+00,  1.0000e+00],\n",
       "        [-4.4172e+00, -2.9515e+00,  0.0000e+00],\n",
       "        [ 4.4172e+00,  2.9515e+00,  1.0000e+00],\n",
       "        [-3.7123e+00, -3.7123e+00,  0.0000e+00],\n",
       "        [ 3.7123e+00,  3.7123e+00,  1.0000e+00],\n",
       "        [-2.8820e+00, -4.3133e+00,  0.0000e+00],\n",
       "        [ 2.8820e+00,  4.3133e+00,  1.0000e+00],\n",
       "        [-1.9612e+00, -4.7349e+00,  0.0000e+00],\n",
       "        [ 1.9612e+00,  4.7349e+00,  1.0000e+00],\n",
       "        [-9.8759e-01, -4.9652e+00,  0.0000e+00],\n",
       "        [ 9.8759e-01,  4.9652e+00,  1.0000e+00],\n",
       "        [ 6.0000e-05, -5.0000e+00,  0.0000e+00],\n",
       "        [-6.0000e-05,  5.0000e+00,  1.0000e+00],\n",
       "        [ 9.6331e-01, -4.8426e+00,  0.0000e+00],\n",
       "        [-9.6331e-01,  4.8426e+00,  1.0000e+00],\n",
       "        [ 1.8656e+00, -4.5039e+00,  0.0000e+00],\n",
       "        [-1.8656e+00,  4.5039e+00,  1.0000e+00],\n",
       "        [ 2.6737e+00, -4.0014e+00,  0.0000e+00],\n",
       "        [-2.6737e+00,  4.0014e+00,  1.0000e+00],\n",
       "        [ 3.3588e+00, -3.3587e+00,  0.0000e+00],\n",
       "        [-3.3588e+00,  3.3587e+00,  1.0000e+00],\n",
       "        [ 3.8976e+00, -2.6042e+00,  0.0000e+00],\n",
       "        [-3.8976e+00,  2.6042e+00,  1.0000e+00],\n",
       "        [ 4.2730e+00, -1.7699e+00,  0.0000e+00],\n",
       "        [-4.2730e+00,  1.7699e+00,  1.0000e+00],\n",
       "        [ 4.4749e+00, -8.9004e-01,  0.0000e+00],\n",
       "        [-4.4749e+00,  8.9004e-01,  1.0000e+00],\n",
       "        [ 4.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-4.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 4.3522e+00,  8.6578e-01,  0.0000e+00],\n",
       "        [-4.3522e+00, -8.6578e-01,  1.0000e+00],\n",
       "        [ 4.0420e+00,  1.6743e+00,  0.0000e+00],\n",
       "        [-4.0420e+00, -1.6743e+00,  1.0000e+00],\n",
       "        [ 3.5857e+00,  2.3960e+00,  0.0000e+00],\n",
       "        [-3.5857e+00, -2.3960e+00,  1.0000e+00],\n",
       "        [ 3.0052e+00,  3.0052e+00,  0.0000e+00],\n",
       "        [-3.0052e+00, -3.0052e+00,  1.0000e+00],\n",
       "        [ 2.3264e+00,  3.4818e+00,  0.0000e+00],\n",
       "        [-2.3264e+00, -3.4818e+00,  1.0000e+00],\n",
       "        [ 1.5785e+00,  3.8110e+00,  0.0000e+00],\n",
       "        [-1.5785e+00, -3.8110e+00,  1.0000e+00],\n",
       "        [ 7.9248e-01,  3.9845e+00,  0.0000e+00],\n",
       "        [-7.9248e-01, -3.9845e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  4.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -4.0000e+00,  1.0000e+00],\n",
       "        [-7.6824e-01,  3.8618e+00,  0.0000e+00],\n",
       "        [ 7.6824e-01, -3.8618e+00,  1.0000e+00],\n",
       "        [-1.4830e+00,  3.5800e+00,  0.0000e+00],\n",
       "        [ 1.4830e+00, -3.5800e+00,  1.0000e+00],\n",
       "        [-2.1182e+00,  3.1699e+00,  0.0000e+00],\n",
       "        [ 2.1182e+00, -3.1699e+00,  1.0000e+00],\n",
       "        [-2.6517e+00,  2.6516e+00,  0.0000e+00],\n",
       "        [ 2.6517e+00, -2.6516e+00,  1.0000e+00],\n",
       "        [-3.0661e+00,  2.0486e+00,  0.0000e+00],\n",
       "        [ 3.0661e+00, -2.0486e+00,  1.0000e+00],\n",
       "        [-3.3491e+00,  1.3872e+00,  0.0000e+00],\n",
       "        [ 3.3491e+00, -1.3872e+00,  1.0000e+00],\n",
       "        [-3.4941e+00,  6.9493e-01,  0.0000e+00],\n",
       "        [ 3.4941e+00, -6.9493e-01,  1.0000e+00],\n",
       "        [-3.5000e+00, -8.0000e-05,  0.0000e+00],\n",
       "        [ 3.5000e+00,  8.0000e-05,  1.0000e+00],\n",
       "        [-3.3714e+00, -6.7070e-01,  0.0000e+00],\n",
       "        [ 3.3714e+00,  6.7070e-01,  1.0000e+00],\n",
       "        [-3.1181e+00, -1.2916e+00,  0.0000e+00],\n",
       "        [ 3.1181e+00,  1.2916e+00,  1.0000e+00],\n",
       "        [-2.7542e+00, -1.8404e+00,  0.0000e+00],\n",
       "        [ 2.7542e+00,  1.8404e+00,  1.0000e+00],\n",
       "        [-2.2980e+00, -2.2982e+00,  0.0000e+00],\n",
       "        [ 2.2980e+00,  2.2982e+00,  1.0000e+00],\n",
       "        [-1.7708e+00, -2.6504e+00,  0.0000e+00],\n",
       "        [ 1.7708e+00,  2.6504e+00,  1.0000e+00],\n",
       "        [-1.1958e+00, -2.8872e+00,  0.0000e+00],\n",
       "        [ 1.1958e+00,  2.8872e+00,  1.0000e+00],\n",
       "        [-5.9739e-01, -3.0037e+00,  0.0000e+00],\n",
       "        [ 5.9739e-01,  3.0037e+00,  1.0000e+00],\n",
       "        [ 8.0000e-05, -3.0000e+00,  0.0000e+00],\n",
       "        [-8.0000e-05,  3.0000e+00,  1.0000e+00],\n",
       "        [ 5.7315e-01, -2.8810e+00,  0.0000e+00],\n",
       "        [-5.7315e-01,  2.8810e+00,  1.0000e+00],\n",
       "        [ 1.1003e+00, -2.6561e+00,  0.0000e+00],\n",
       "        [-1.1003e+00,  2.6561e+00,  1.0000e+00],\n",
       "        [ 1.5626e+00, -2.3385e+00,  0.0000e+00],\n",
       "        [-1.5626e+00,  2.3385e+00,  1.0000e+00],\n",
       "        [ 1.9446e+00, -1.9445e+00,  0.0000e+00],\n",
       "        [-1.9446e+00,  1.9445e+00,  1.0000e+00],\n",
       "        [ 2.2346e+00, -1.4930e+00,  0.0000e+00],\n",
       "        [-2.2346e+00,  1.4930e+00,  1.0000e+00],\n",
       "        [ 2.4252e+00, -1.0045e+00,  0.0000e+00],\n",
       "        [-2.4252e+00,  1.0045e+00,  1.0000e+00],\n",
       "        [ 2.5133e+00, -4.9985e-01,  0.0000e+00],\n",
       "        [-2.5133e+00,  4.9985e-01,  1.0000e+00],\n",
       "        [ 2.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-2.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 2.3907e+00,  4.7560e-01,  0.0000e+00],\n",
       "        [-2.3907e+00, -4.7560e-01,  1.0000e+00],\n",
       "        [ 2.1942e+00,  9.0894e-01,  0.0000e+00],\n",
       "        [-2.1942e+00, -9.0894e-01,  1.0000e+00],\n",
       "        [ 1.9227e+00,  1.2848e+00,  0.0000e+00],\n",
       "        [-1.9227e+00, -1.2848e+00,  1.0000e+00],\n",
       "        [ 1.5909e+00,  1.5910e+00,  0.0000e+00],\n",
       "        [-1.5909e+00, -1.5910e+00,  1.0000e+00],\n",
       "        [ 1.2153e+00,  1.8189e+00,  0.0000e+00],\n",
       "        [-1.2153e+00, -1.8189e+00,  1.0000e+00],\n",
       "        [ 8.1314e-01,  1.9633e+00,  0.0000e+00],\n",
       "        [-8.1314e-01, -1.9633e+00,  1.0000e+00],\n",
       "        [ 4.0231e-01,  2.0229e+00,  0.0000e+00],\n",
       "        [-4.0231e-01, -2.0229e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  2.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -2.0000e+00,  1.0000e+00],\n",
       "        [-3.7805e-01,  1.9003e+00,  0.0000e+00],\n",
       "        [ 3.7805e-01, -1.9003e+00,  1.0000e+00],\n",
       "        [-7.1759e-01,  1.7322e+00,  0.0000e+00],\n",
       "        [ 7.1759e-01, -1.7322e+00,  1.0000e+00],\n",
       "        [-1.0070e+00,  1.5070e+00,  0.0000e+00],\n",
       "        [ 1.0070e+00, -1.5070e+00,  1.0000e+00],\n",
       "        [-1.2375e+00,  1.2374e+00,  0.0000e+00],\n",
       "        [ 1.2375e+00, -1.2374e+00,  1.0000e+00],\n",
       "        [-1.4031e+00,  9.3748e-01,  0.0000e+00],\n",
       "        [ 1.4031e+00, -9.3748e-01,  1.0000e+00],\n",
       "        [-1.5013e+00,  6.2181e-01,  0.0000e+00],\n",
       "        [ 1.5013e+00, -6.2181e-01,  1.0000e+00],\n",
       "        [-1.5325e+00,  3.0477e-01,  0.0000e+00],\n",
       "        [ 1.5325e+00, -3.0477e-01,  1.0000e+00],\n",
       "        [-1.5000e+00, -6.0000e-05,  0.0000e+00],\n",
       "        [ 1.5000e+00,  6.0000e-05,  1.0000e+00],\n",
       "        [-1.4099e+00, -2.8049e-01,  0.0000e+00],\n",
       "        [ 1.4099e+00,  2.8049e-01,  1.0000e+00],\n",
       "        [-1.2703e+00, -5.2624e-01,  0.0000e+00],\n",
       "        [ 1.2703e+00,  5.2624e-01,  1.0000e+00],\n",
       "        [-1.0913e+00, -7.2923e-01,  0.0000e+00],\n",
       "        [ 1.0913e+00,  7.2923e-01,  1.0000e+00],\n",
       "        [-8.8385e-01, -8.8392e-01,  0.0000e+00],\n",
       "        [ 8.8385e-01,  8.8392e-01,  1.0000e+00],\n",
       "        [-6.5970e-01, -9.8740e-01,  0.0000e+00],\n",
       "        [ 6.5970e-01,  9.8740e-01,  1.0000e+00],\n",
       "        [-4.3048e-01, -1.0394e+00,  0.0000e+00],\n",
       "        [ 4.3048e-01,  1.0394e+00,  1.0000e+00],\n",
       "        [-2.0724e-01, -1.0421e+00,  0.0000e+00],\n",
       "        [ 2.0724e-01,  1.0421e+00,  1.0000e+00],\n",
       "        [ 4.0000e-05, -1.0000e+00,  0.0000e+00],\n",
       "        [-4.0000e-05,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.8293e-01, -9.1948e-01,  0.0000e+00],\n",
       "        [-1.8293e-01,  9.1948e-01,  1.0000e+00],\n",
       "        [ 3.3488e-01, -8.0838e-01,  0.0000e+00],\n",
       "        [-3.3488e-01,  8.0838e-01,  1.0000e+00],\n",
       "        [ 4.5143e-01, -6.7555e-01,  0.0000e+00],\n",
       "        [-4.5143e-01,  6.7555e-01,  1.0000e+00],\n",
       "        [ 5.3035e-01, -5.3031e-01,  0.0000e+00],\n",
       "        [-5.3035e-01,  5.3031e-01,  1.0000e+00],\n",
       "        [ 5.7165e-01, -3.8193e-01,  0.0000e+00],\n",
       "        [-5.7165e-01,  3.8193e-01,  1.0000e+00],\n",
       "        [ 5.7744e-01, -2.3915e-01,  0.0000e+00],\n",
       "        [-5.7744e-01,  2.3915e-01,  1.0000e+00],\n",
       "        [ 5.5170e-01, -1.0971e-01,  0.0000e+00],\n",
       "        [-5.5170e-01,  1.0971e-01,  1.0000e+00],\n",
       "        [ 5.0000e-01,  2.0000e-05,  0.0000e+00],\n",
       "        [-5.0000e-01, -2.0000e-05,  1.0000e+00]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_output(net):\n",
    "    xrange = torch.arange(start=-7,end=7.1,step=0.01,dtype=torch.float32)\n",
    "    yrange = torch.arange(start=-6.6,end=6.7,step=0.01,dtype=torch.float32)\n",
    "    xcoord = xrange.repeat(yrange.size()[0])\n",
    "    ycoord = torch.repeat_interleave(yrange, xrange.size()[0], dim=0)\n",
    "    grid = torch.cat((xcoord.unsqueeze(1),ycoord.unsqueeze(1)),1)\n",
    "    with torch.no_grad(): # suppress updating of gradients\n",
    "        net.eval()        # toggle batch norm, dropout\n",
    "        output = net(grid)\n",
    "        net.train() # toggle batch norm, dropout back again\n",
    "\n",
    "        pred = (output >= 0.5).float()\n",
    "\n",
    "        # plot function computed by model\n",
    "        plt.clf()\n",
    "        plt.pcolormesh(xrange,yrange,pred.cpu().view(yrange.size()[0],xrange.size()[0]), cmap='Wistia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hidden(net, layer, node):\n",
    "    print(\"node:\",node)\n",
    "    print(\"layer:\",layer)\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 1.0,\n",
       " 1.1,\n",
       " 1.2000000000000002,\n",
       " 1.3000000000000003,\n",
       " 1.4000000000000001,\n",
       " 1.5000000000000002,\n",
       " 1.6,\n",
       " 1.7000000000000002,\n",
       " 1.8000000000000003,\n",
       " 1.9000000000000001]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import arange\n",
    "p = [j for j in arange(.1,2,.1)]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:  100 loss: 0.7124 acc: 50.52\n",
      "ep:  200 loss: 0.7125 acc: 50.00\n",
      "ep:  300 loss: 0.7126 acc: 50.00\n",
      "ep:  400 loss: 0.7127 acc: 50.00\n",
      "ep:  500 loss: 0.7127 acc: 50.00\n",
      "ep:  600 loss: 0.7127 acc: 50.00\n",
      "ep:  700 loss: 0.7128 acc: 50.00\n",
      "ep:  800 loss: 0.7128 acc: 50.00\n",
      "ep:  900 loss: 0.7128 acc: 50.00\n",
      "ep: 1000 loss: 0.7128 acc: 50.00\n",
      "ep: 1100 loss: 0.7128 acc: 50.00\n",
      "ep: 1200 loss: 0.7128 acc: 50.00\n",
      "ep: 1300 loss: 0.7128 acc: 50.00\n",
      "ep: 1400 loss: 0.7128 acc: 50.00\n",
      "ep: 1500 loss: 0.7128 acc: 50.00\n",
      "ep: 1600 loss: 0.7128 acc: 50.00\n",
      "ep: 1700 loss: 0.7128 acc: 50.00\n",
      "ep: 1800 loss: 0.7128 acc: 50.00\n",
      "ep: 1900 loss: 0.7128 acc: 50.00\n",
      "ep: 2000 loss: 0.7128 acc: 50.00\n",
      "ep: 2100 loss: 0.7128 acc: 50.52\n",
      "ep: 2200 loss: 0.7128 acc: 50.52\n",
      "ep: 2300 loss: 0.7128 acc: 50.52\n",
      "ep: 2400 loss: 0.7128 acc: 50.52\n",
      "ep: 2500 loss: 0.7128 acc: 50.52\n",
      "ep: 2600 loss: 0.7128 acc: 50.52\n",
      "ep: 2700 loss: 0.7128 acc: 50.52\n",
      "ep: 2800 loss: 0.7128 acc: 50.52\n",
      "ep: 2900 loss: 0.7128 acc: 50.52\n",
      "ep: 3000 loss: 0.7128 acc: 50.52\n",
      "ep: 3100 loss: 0.7128 acc: 50.52\n",
      "ep: 3200 loss: 0.7128 acc: 50.52\n",
      "ep: 3300 loss: 0.7128 acc: 50.52\n",
      "ep: 3400 loss: 0.7128 acc: 50.52\n",
      "ep: 3500 loss: 0.7128 acc: 50.52\n",
      "ep: 3600 loss: 0.7128 acc: 50.52\n",
      "ep: 3700 loss: 0.7128 acc: 50.52\n",
      "ep: 3800 loss: 0.7128 acc: 50.52\n",
      "ep: 3900 loss: 0.7128 acc: 50.52\n",
      "ep: 4000 loss: 0.7128 acc: 50.52\n",
      "ep: 4100 loss: 0.7128 acc: 50.52\n",
      "ep: 4200 loss: 0.7128 acc: 50.52\n",
      "ep: 4300 loss: 0.7128 acc: 50.52\n",
      "ep: 4400 loss: 0.7128 acc: 50.52\n",
      "ep: 4500 loss: 0.7128 acc: 50.52\n",
      "ep: 4600 loss: 0.7128 acc: 50.52\n",
      "ep: 4700 loss: 0.7128 acc: 50.52\n",
      "ep: 4800 loss: 0.7128 acc: 50.52\n",
      "ep: 4900 loss: 0.7128 acc: 50.52\n",
      "ep: 5000 loss: 0.7128 acc: 50.52\n",
      "ep: 5100 loss: 0.7128 acc: 50.52\n",
      "ep: 5200 loss: 0.7128 acc: 50.52\n",
      "ep: 5300 loss: 0.7128 acc: 50.52\n",
      "ep: 5400 loss: 0.7128 acc: 50.52\n",
      "ep: 5500 loss: 0.7128 acc: 50.52\n",
      "ep: 5600 loss: 0.7128 acc: 50.52\n",
      "ep: 5700 loss: 0.7128 acc: 50.52\n",
      "ep: 5800 loss: 0.7128 acc: 50.52\n",
      "ep: 5900 loss: 0.7128 acc: 50.52\n",
      "ep: 6000 loss: 0.7128 acc: 50.52\n",
      "ep: 6100 loss: 0.7128 acc: 50.52\n",
      "ep: 6200 loss: 0.7128 acc: 50.52\n",
      "ep: 6300 loss: 0.7128 acc: 50.52\n",
      "ep: 6400 loss: 0.7128 acc: 50.52\n",
      "ep: 6500 loss: 0.7128 acc: 50.52\n",
      "ep: 6600 loss: 0.7128 acc: 50.52\n",
      "ep: 6700 loss: 0.7128 acc: 50.52\n",
      "ep: 6800 loss: 0.7128 acc: 50.52\n",
      "ep: 6900 loss: 0.7128 acc: 50.52\n",
      "ep: 7000 loss: 0.7128 acc: 50.52\n",
      "ep: 7100 loss: 0.7128 acc: 50.52\n",
      "ep: 7200 loss: 0.7128 acc: 50.52\n",
      "ep: 7300 loss: 0.7128 acc: 50.52\n",
      "ep: 7400 loss: 0.7128 acc: 50.52\n",
      "ep: 7500 loss: 0.7128 acc: 50.52\n",
      "ep: 7600 loss: 0.7128 acc: 50.52\n",
      "ep: 7700 loss: 0.7128 acc: 50.52\n",
      "ep: 7800 loss: 0.7128 acc: 50.52\n",
      "ep: 7900 loss: 0.7128 acc: 50.52\n",
      "ep: 8000 loss: 0.7128 acc: 50.52\n",
      "ep: 8100 loss: 0.7128 acc: 50.52\n",
      "ep: 8200 loss: 0.7128 acc: 50.52\n",
      "ep: 8300 loss: 0.7128 acc: 50.52\n",
      "ep: 8400 loss: 0.7128 acc: 50.52\n",
      "ep: 8500 loss: 0.7128 acc: 50.52\n",
      "ep: 8600 loss: 0.7128 acc: 50.52\n",
      "ep: 8700 loss: 0.7128 acc: 50.52\n",
      "ep: 8800 loss: 0.7128 acc: 50.52\n",
      "ep: 8900 loss: 0.7128 acc: 50.52\n",
      "ep: 9000 loss: 0.7128 acc: 50.52\n",
      "ep: 9100 loss: 0.7128 acc: 50.52\n",
      "ep: 9200 loss: 0.7128 acc: 50.52\n",
      "ep: 9300 loss: 0.7128 acc: 50.52\n",
      "ep: 9400 loss: 0.7128 acc: 50.52\n",
      "ep: 9500 loss: 0.7128 acc: 50.52\n",
      "ep: 9600 loss: 0.7128 acc: 50.52\n",
      "ep: 9700 loss: 0.7128 acc: 50.52\n",
      "ep: 9800 loss: 0.7128 acc: 50.52\n",
      "ep: 9900 loss: 0.7128 acc: 50.52\n",
      "ep:10000 loss: 0.7128 acc: 50.52\n",
      "ep:10100 loss: 0.7128 acc: 50.52\n",
      "ep:10200 loss: 0.7128 acc: 50.52\n",
      "ep:10300 loss: 0.7128 acc: 50.52\n",
      "ep:10400 loss: 0.7128 acc: 50.52\n",
      "ep:10500 loss: 0.7128 acc: 50.52\n",
      "ep:10600 loss: 0.7128 acc: 50.52\n",
      "ep:10700 loss: 0.7128 acc: 50.52\n",
      "ep:10800 loss: 0.7128 acc: 50.52\n",
      "ep:10900 loss: 0.7128 acc: 50.52\n",
      "ep:11000 loss: 0.7128 acc: 50.52\n",
      "ep:11100 loss: 0.7128 acc: 50.52\n",
      "ep:11200 loss: 0.7128 acc: 50.52\n",
      "ep:11300 loss: 0.7128 acc: 50.52\n",
      "ep:11400 loss: 0.7128 acc: 50.52\n",
      "ep:11500 loss: 0.7128 acc: 50.52\n",
      "ep:11600 loss: 0.7128 acc: 50.52\n",
      "ep:11700 loss: 0.7128 acc: 50.52\n",
      "ep:11800 loss: 0.7128 acc: 50.52\n",
      "ep:11900 loss: 0.7128 acc: 50.52\n",
      "ep:12000 loss: 0.7128 acc: 50.52\n",
      "ep:12100 loss: 0.7128 acc: 50.52\n",
      "ep:12200 loss: 0.7128 acc: 50.52\n",
      "ep:12300 loss: 0.7128 acc: 50.52\n",
      "ep:12400 loss: 0.7128 acc: 50.52\n",
      "ep:12500 loss: 0.7128 acc: 50.52\n",
      "ep:12600 loss: 0.7128 acc: 50.52\n",
      "ep:12700 loss: 0.7128 acc: 50.52\n",
      "ep:12800 loss: 0.7128 acc: 50.52\n",
      "ep:12900 loss: 0.7128 acc: 50.52\n",
      "ep:13000 loss: 0.7128 acc: 50.52\n",
      "ep:13100 loss: 0.7128 acc: 50.52\n",
      "ep:13200 loss: 0.7128 acc: 50.52\n",
      "ep:13300 loss: 0.7128 acc: 50.52\n",
      "ep:13400 loss: 0.7128 acc: 50.52\n",
      "ep:13500 loss: 0.7128 acc: 50.52\n",
      "ep:13600 loss: 0.7128 acc: 50.52\n",
      "ep:13700 loss: 0.7128 acc: 50.52\n",
      "ep:13800 loss: 0.7128 acc: 50.52\n",
      "ep:13900 loss: 0.7128 acc: 50.52\n",
      "ep:14000 loss: 0.7128 acc: 50.52\n",
      "ep:14100 loss: 0.7128 acc: 50.52\n",
      "ep:14200 loss: 0.7128 acc: 50.52\n",
      "ep:14300 loss: 0.7128 acc: 50.52\n",
      "ep:14400 loss: 0.7128 acc: 50.52\n",
      "ep:14500 loss: 0.7128 acc: 50.52\n",
      "ep:14600 loss: 0.7128 acc: 50.52\n",
      "ep:14700 loss: 0.7128 acc: 50.52\n",
      "ep:14800 loss: 0.7128 acc: 50.52\n",
      "ep:14900 loss: 0.7128 acc: 50.52\n",
      "ep:15000 loss: 0.7128 acc: 50.52\n",
      "ep:15100 loss: 0.7128 acc: 50.52\n",
      "ep:15200 loss: 0.7128 acc: 50.52\n",
      "ep:15300 loss: 0.7128 acc: 50.52\n",
      "ep:15400 loss: 0.7128 acc: 50.52\n",
      "ep:15500 loss: 0.7128 acc: 50.52\n",
      "ep:15600 loss: 0.7128 acc: 50.52\n",
      "ep:15700 loss: 0.7128 acc: 50.52\n",
      "ep:15800 loss: 0.7128 acc: 50.52\n",
      "ep:15900 loss: 0.7128 acc: 50.52\n",
      "ep:16000 loss: 0.7128 acc: 50.52\n",
      "ep:16100 loss: 0.7128 acc: 50.52\n",
      "ep:16200 loss: 0.7128 acc: 50.52\n",
      "ep:16300 loss: 0.7128 acc: 50.52\n",
      "ep:16400 loss: 0.7128 acc: 50.52\n",
      "ep:16500 loss: 0.7128 acc: 50.52\n",
      "ep:16600 loss: 0.7128 acc: 50.52\n",
      "ep:16700 loss: 0.7128 acc: 50.52\n",
      "ep:16800 loss: 0.7128 acc: 50.52\n",
      "ep:16900 loss: 0.7128 acc: 50.52\n",
      "ep:17000 loss: 0.7128 acc: 50.52\n",
      "ep:17100 loss: 0.7128 acc: 50.52\n",
      "ep:17200 loss: 0.7128 acc: 50.52\n",
      "ep:17300 loss: 0.7128 acc: 50.52\n",
      "ep:17400 loss: 0.7128 acc: 50.52\n",
      "ep:17500 loss: 0.7128 acc: 50.52\n",
      "ep:17600 loss: 0.7128 acc: 50.52\n",
      "ep:17700 loss: 0.7128 acc: 50.52\n",
      "ep:17800 loss: 0.7128 acc: 50.52\n",
      "ep:17900 loss: 0.7128 acc: 50.52\n",
      "ep:18000 loss: 0.7128 acc: 50.52\n",
      "ep:18100 loss: 0.7128 acc: 50.52\n",
      "ep:18200 loss: 0.7128 acc: 50.52\n",
      "ep:18300 loss: 0.7128 acc: 50.52\n",
      "ep:18400 loss: 0.7128 acc: 50.52\n",
      "ep:18500 loss: 0.7128 acc: 50.52\n",
      "ep:18600 loss: 0.7128 acc: 50.52\n",
      "ep:18700 loss: 0.7128 acc: 50.52\n",
      "ep:18800 loss: 0.7128 acc: 50.52\n",
      "ep:18900 loss: 0.7128 acc: 50.52\n",
      "ep:19000 loss: 0.7128 acc: 50.52\n",
      "ep:19100 loss: 0.7128 acc: 50.52\n",
      "ep:19200 loss: 0.7128 acc: 50.52\n",
      "ep:19300 loss: 0.7128 acc: 50.52\n",
      "ep:19400 loss: 0.7128 acc: 50.52\n",
      "ep:19500 loss: 0.7128 acc: 50.52\n",
      "ep:19600 loss: 0.7128 acc: 50.52\n",
      "ep:19700 loss: 0.7128 acc: 50.52\n",
      "ep:19800 loss: 0.7128 acc: 50.52\n",
      "ep:19900 loss: 0.7128 acc: 50.52\n",
      "init:0.1, acc:50.515464782714844 , epoch:19999\n",
      "ep:  100 loss: 0.6853 acc: 62.89\n",
      "ep:  200 loss: 0.6780 acc: 63.92\n",
      "ep:  300 loss: 0.6683 acc: 59.79\n",
      "ep:  400 loss: 0.6564 acc: 58.76\n",
      "ep:  500 loss: 0.6434 acc: 58.25\n",
      "ep:  600 loss: 0.6249 acc: 54.64\n",
      "ep:  700 loss: 0.5772 acc: 59.79\n",
      "ep:  800 loss: 0.5085 acc: 59.79\n",
      "ep:  900 loss: 0.4144 acc: 60.82\n",
      "ep: 1000 loss: 0.3418 acc: 63.40\n",
      "ep: 1100 loss: 0.2897 acc: 65.46\n",
      "ep: 1200 loss: 0.2506 acc: 67.53\n",
      "ep: 1300 loss: 0.2192 acc: 68.04\n",
      "ep: 1400 loss: 0.1942 acc: 68.04\n",
      "ep: 1500 loss: 0.1726 acc: 70.62\n",
      "ep: 1600 loss: 0.1560 acc: 71.65\n",
      "ep: 1700 loss: 0.1444 acc: 72.68\n",
      "ep: 1800 loss: 0.1355 acc: 72.68\n",
      "ep: 1900 loss: 0.1285 acc: 73.20\n",
      "ep: 2000 loss: 0.1217 acc: 75.26\n",
      "ep: 2100 loss: 0.1154 acc: 75.77\n",
      "ep: 2200 loss: 0.1101 acc: 76.80\n",
      "ep: 2300 loss: 0.1054 acc: 78.35\n",
      "ep: 2400 loss: 0.1010 acc: 78.87\n",
      "ep: 2500 loss: 0.0970 acc: 79.38\n",
      "ep: 2600 loss: 0.0932 acc: 79.90\n",
      "ep: 2700 loss: 0.0895 acc: 81.44\n",
      "ep: 2800 loss: 0.0859 acc: 81.44\n",
      "ep: 2900 loss: 0.0824 acc: 81.44\n",
      "ep: 3000 loss: 0.0790 acc: 81.44\n",
      "ep: 3100 loss: 0.0756 acc: 81.44\n",
      "ep: 3200 loss: 0.0724 acc: 81.96\n",
      "ep: 3300 loss: 0.0693 acc: 82.99\n",
      "ep: 3400 loss: 0.0665 acc: 82.99\n",
      "ep: 3500 loss: 0.0637 acc: 82.99\n",
      "ep: 3600 loss: 0.0611 acc: 82.47\n",
      "ep: 3700 loss: 0.0587 acc: 82.47\n",
      "ep: 3800 loss: 0.0563 acc: 81.96\n",
      "ep: 3900 loss: 0.0535 acc: 81.44\n",
      "ep: 4000 loss: 0.0511 acc: 81.44\n",
      "ep: 4100 loss: 0.0493 acc: 81.44\n",
      "ep: 4200 loss: 0.0483 acc: 83.51\n",
      "ep: 4300 loss: 0.0469 acc: 85.57\n",
      "ep: 4400 loss: 0.0457 acc: 85.57\n",
      "ep: 4500 loss: 0.0444 acc: 86.60\n",
      "ep: 4600 loss: 0.0423 acc: 86.60\n",
      "ep: 4700 loss: 0.0401 acc: 86.60\n",
      "ep: 4800 loss: 0.0382 acc: 86.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4900 loss: 0.0353 acc: 85.57\n",
      "ep: 5000 loss: 0.0322 acc: 86.60\n",
      "ep: 5100 loss: 0.0306 acc: 88.66\n",
      "ep: 5200 loss: 0.0284 acc: 88.66\n",
      "ep: 5300 loss: 0.0266 acc: 86.60\n",
      "ep: 5400 loss: 0.0251 acc: 86.60\n",
      "ep: 5500 loss: 0.0239 acc: 86.60\n",
      "ep: 5600 loss: 0.0228 acc: 87.63\n",
      "ep: 5700 loss: 0.0220 acc: 88.66\n",
      "ep: 5800 loss: 0.0210 acc: 92.78\n",
      "ep: 5900 loss: 0.0189 acc: 91.75\n",
      "ep: 6000 loss: 0.0195 acc: 92.27\n",
      "ep: 6100 loss: 0.0186 acc: 91.24\n",
      "ep: 6200 loss: 0.0172 acc: 91.75\n",
      "ep: 6300 loss: 0.0167 acc: 90.72\n",
      "ep: 6400 loss: 0.0178 acc: 92.27\n",
      "ep: 6500 loss: 0.0211 acc: 96.91\n",
      "ep: 6600 loss: 0.0251 acc: 95.88\n",
      "Achieves 100%\n",
      "init:0.2, acc:100.0 , epoch:6635\n",
      "ep:  100 loss: 0.6829 acc: 61.86\n",
      "ep:  200 loss: 0.6649 acc: 62.89\n",
      "ep:  300 loss: 0.6469 acc: 60.31\n",
      "ep:  400 loss: 0.6183 acc: 52.06\n",
      "ep:  500 loss: 0.5561 acc: 58.76\n",
      "ep:  600 loss: 0.4692 acc: 59.79\n",
      "ep:  700 loss: 0.4006 acc: 62.37\n",
      "ep:  800 loss: 0.3558 acc: 66.49\n",
      "ep:  900 loss: 0.3309 acc: 70.10\n",
      "ep: 1000 loss: 0.3146 acc: 71.13\n",
      "ep: 1100 loss: 0.3036 acc: 70.62\n",
      "ep: 1200 loss: 0.2951 acc: 72.16\n",
      "ep: 1300 loss: 0.2874 acc: 74.23\n",
      "ep: 1400 loss: 0.2805 acc: 74.74\n",
      "ep: 1500 loss: 0.2723 acc: 77.84\n",
      "ep: 1600 loss: 0.2616 acc: 77.32\n",
      "ep: 1700 loss: 0.2509 acc: 78.35\n",
      "ep: 1800 loss: 0.2420 acc: 78.87\n",
      "ep: 1900 loss: 0.2348 acc: 78.87\n",
      "ep: 2000 loss: 0.2287 acc: 78.87\n",
      "ep: 2100 loss: 0.2233 acc: 78.87\n",
      "ep: 2200 loss: 0.2181 acc: 78.87\n",
      "ep: 2300 loss: 0.2120 acc: 79.38\n",
      "ep: 2400 loss: 0.2064 acc: 79.90\n",
      "ep: 2500 loss: 0.2020 acc: 79.38\n",
      "ep: 2600 loss: 0.1979 acc: 79.90\n",
      "ep: 2700 loss: 0.1941 acc: 80.93\n",
      "ep: 2800 loss: 0.1905 acc: 81.96\n",
      "ep: 2900 loss: 0.1868 acc: 81.96\n",
      "ep: 3000 loss: 0.1826 acc: 82.99\n",
      "ep: 3100 loss: 0.1776 acc: 84.02\n",
      "ep: 3200 loss: 0.1711 acc: 82.99\n",
      "ep: 3300 loss: 0.1625 acc: 82.99\n",
      "ep: 3400 loss: 0.1515 acc: 85.05\n",
      "ep: 3500 loss: 0.1398 acc: 86.08\n",
      "ep: 3600 loss: 0.1300 acc: 87.11\n",
      "ep: 3700 loss: 0.1228 acc: 86.60\n",
      "ep: 3800 loss: 0.1173 acc: 87.11\n",
      "ep: 3900 loss: 0.1129 acc: 86.60\n",
      "ep: 4000 loss: 0.1092 acc: 86.60\n",
      "ep: 4100 loss: 0.1059 acc: 87.63\n",
      "ep: 4200 loss: 0.1031 acc: 87.63\n",
      "ep: 4300 loss: 0.1006 acc: 87.63\n",
      "ep: 4400 loss: 0.0979 acc: 88.66\n",
      "ep: 4500 loss: 0.0948 acc: 88.66\n",
      "ep: 4600 loss: 0.0913 acc: 89.18\n",
      "ep: 4700 loss: 0.0883 acc: 89.18\n",
      "ep: 4800 loss: 0.0856 acc: 89.18\n",
      "ep: 4900 loss: 0.0831 acc: 90.21\n",
      "ep: 5000 loss: 0.0808 acc: 90.21\n",
      "ep: 5100 loss: 0.0787 acc: 90.21\n",
      "ep: 5200 loss: 0.0768 acc: 91.24\n",
      "ep: 5300 loss: 0.0749 acc: 90.72\n",
      "ep: 5400 loss: 0.0730 acc: 90.72\n",
      "ep: 5500 loss: 0.0711 acc: 90.72\n",
      "ep: 5600 loss: 0.0691 acc: 90.72\n",
      "ep: 5700 loss: 0.0670 acc: 91.24\n",
      "ep: 5800 loss: 0.0649 acc: 91.24\n",
      "ep: 5900 loss: 0.0629 acc: 91.24\n",
      "ep: 6000 loss: 0.0612 acc: 91.75\n",
      "ep: 6100 loss: 0.0597 acc: 92.27\n",
      "ep: 6200 loss: 0.0584 acc: 92.78\n",
      "ep: 6300 loss: 0.0570 acc: 92.78\n",
      "ep: 6400 loss: 0.0557 acc: 93.30\n",
      "ep: 6500 loss: 0.0545 acc: 93.30\n",
      "ep: 6600 loss: 0.0534 acc: 93.30\n",
      "init:0.30000000000000004, acc:93.29896545410156 , epoch:6634\n",
      "ep:  100 loss: 0.6720 acc: 58.76\n",
      "ep:  200 loss: 0.6439 acc: 55.15\n",
      "ep:  300 loss: 0.6146 acc: 56.19\n",
      "ep:  400 loss: 0.5492 acc: 59.79\n",
      "ep:  500 loss: 0.4855 acc: 63.92\n",
      "ep:  600 loss: 0.4552 acc: 69.59\n",
      "ep:  700 loss: 0.4338 acc: 70.62\n",
      "ep:  800 loss: 0.4149 acc: 68.56\n",
      "ep:  900 loss: 0.3998 acc: 69.59\n",
      "ep: 1000 loss: 0.3763 acc: 72.68\n",
      "ep: 1100 loss: 0.3366 acc: 75.26\n",
      "ep: 1200 loss: 0.3132 acc: 76.80\n",
      "ep: 1300 loss: 0.3071 acc: 79.90\n",
      "ep: 1400 loss: 0.2926 acc: 78.35\n",
      "ep: 1500 loss: 0.2806 acc: 80.93\n",
      "ep: 1600 loss: 0.2704 acc: 81.44\n",
      "ep: 1700 loss: 0.2631 acc: 82.99\n",
      "ep: 1800 loss: 0.2565 acc: 82.99\n",
      "ep: 1900 loss: 0.2532 acc: 84.02\n",
      "ep: 2000 loss: 0.2490 acc: 84.54\n",
      "ep: 2100 loss: 0.2456 acc: 86.60\n",
      "ep: 2200 loss: 0.2422 acc: 86.60\n",
      "ep: 2300 loss: 0.2345 acc: 87.11\n",
      "ep: 2400 loss: 0.2250 acc: 88.14\n",
      "ep: 2500 loss: 0.2153 acc: 89.69\n",
      "ep: 2600 loss: 0.2088 acc: 90.21\n",
      "ep: 2700 loss: 0.1991 acc: 90.72\n",
      "ep: 2800 loss: 0.1919 acc: 89.69\n",
      "ep: 2900 loss: 0.1856 acc: 91.24\n",
      "ep: 3000 loss: 0.1801 acc: 91.24\n",
      "ep: 3100 loss: 0.1744 acc: 92.27\n",
      "ep: 3200 loss: 0.1682 acc: 93.30\n",
      "ep: 3300 loss: 0.1613 acc: 94.33\n",
      "ep: 3400 loss: 0.1516 acc: 94.85\n",
      "ep: 3500 loss: 0.1342 acc: 95.88\n",
      "ep: 3600 loss: 0.1143 acc: 95.88\n",
      "ep: 3700 loss: 0.0972 acc: 95.36\n",
      "ep: 3800 loss: 0.0856 acc: 95.88\n",
      "ep: 3900 loss: 0.0778 acc: 95.88\n",
      "ep: 4000 loss: 0.0712 acc: 95.88\n",
      "ep: 4100 loss: 0.0673 acc: 95.88\n",
      "ep: 4200 loss: 0.0637 acc: 95.88\n",
      "ep: 4300 loss: 0.0602 acc: 95.88\n",
      "ep: 4400 loss: 0.0572 acc: 95.88\n",
      "ep: 4500 loss: 0.0547 acc: 95.88\n",
      "ep: 4600 loss: 0.0526 acc: 95.88\n",
      "ep: 4700 loss: 0.0509 acc: 95.88\n",
      "ep: 4800 loss: 0.0494 acc: 95.88\n",
      "ep: 4900 loss: 0.0481 acc: 95.88\n",
      "ep: 5000 loss: 0.0469 acc: 95.88\n",
      "ep: 5100 loss: 0.0458 acc: 95.88\n",
      "ep: 5200 loss: 0.0447 acc: 95.88\n",
      "ep: 5300 loss: 0.0436 acc: 95.88\n",
      "ep: 5400 loss: 0.0424 acc: 96.39\n",
      "ep: 5500 loss: 0.0413 acc: 96.39\n",
      "ep: 5600 loss: 0.0402 acc: 96.39\n",
      "ep: 5700 loss: 0.0391 acc: 96.39\n",
      "ep: 5800 loss: 0.0380 acc: 96.39\n",
      "ep: 5900 loss: 0.0374 acc: 96.39\n",
      "ep: 6000 loss: 0.0364 acc: 96.91\n",
      "ep: 6100 loss: 0.0341 acc: 96.91\n",
      "ep: 6200 loss: 0.0312 acc: 96.91\n",
      "ep: 6300 loss: 0.0293 acc: 96.91\n",
      "ep: 6400 loss: 0.0278 acc: 96.91\n",
      "ep: 6500 loss: 0.0263 acc: 96.91\n",
      "ep: 6600 loss: 0.0248 acc: 96.91\n",
      "init:0.4, acc:96.90721893310547 , epoch:6633\n",
      "ep:  100 loss: 0.6403 acc: 61.86\n",
      "ep:  200 loss: 0.5739 acc: 61.34\n",
      "ep:  300 loss: 0.5210 acc: 64.95\n",
      "ep:  400 loss: 0.4420 acc: 65.98\n",
      "ep:  500 loss: 0.3783 acc: 67.53\n",
      "ep:  600 loss: 0.3446 acc: 70.10\n",
      "ep:  700 loss: 0.3276 acc: 72.16\n",
      "ep:  800 loss: 0.3157 acc: 72.68\n",
      "ep:  900 loss: 0.3059 acc: 72.16\n",
      "ep: 1000 loss: 0.2972 acc: 72.68\n",
      "ep: 1100 loss: 0.2893 acc: 73.20\n",
      "ep: 1200 loss: 0.2800 acc: 74.74\n",
      "ep: 1300 loss: 0.2655 acc: 75.77\n",
      "ep: 1400 loss: 0.2545 acc: 75.77\n",
      "ep: 1500 loss: 0.2437 acc: 75.77\n",
      "ep: 1600 loss: 0.2336 acc: 75.26\n",
      "ep: 1700 loss: 0.2164 acc: 76.80\n",
      "ep: 1800 loss: 0.2044 acc: 79.90\n",
      "ep: 1900 loss: 0.1900 acc: 80.41\n",
      "ep: 2000 loss: 0.1796 acc: 80.41\n",
      "ep: 2100 loss: 0.1721 acc: 82.99\n",
      "ep: 2200 loss: 0.1651 acc: 82.47\n",
      "ep: 2300 loss: 0.1576 acc: 82.99\n",
      "ep: 2400 loss: 0.1521 acc: 83.51\n",
      "ep: 2500 loss: 0.1475 acc: 84.02\n",
      "ep: 2600 loss: 0.1437 acc: 84.02\n",
      "ep: 2700 loss: 0.1405 acc: 84.02\n",
      "ep: 2800 loss: 0.1379 acc: 83.51\n",
      "ep: 2900 loss: 0.1357 acc: 83.51\n",
      "ep: 3000 loss: 0.1332 acc: 84.02\n",
      "ep: 3100 loss: 0.1319 acc: 84.02\n",
      "ep: 3200 loss: 0.1306 acc: 84.02\n",
      "ep: 3300 loss: 0.1291 acc: 84.02\n",
      "ep: 3400 loss: 0.1276 acc: 84.54\n",
      "ep: 3500 loss: 0.1264 acc: 84.54\n",
      "ep: 3600 loss: 0.1254 acc: 84.54\n",
      "ep: 3700 loss: 0.1246 acc: 84.54\n",
      "ep: 3800 loss: 0.1238 acc: 84.54\n",
      "ep: 3900 loss: 0.1231 acc: 85.05\n",
      "ep: 4000 loss: 0.1225 acc: 85.57\n",
      "ep: 4100 loss: 0.1222 acc: 86.08\n",
      "ep: 4200 loss: 0.1221 acc: 86.08\n",
      "ep: 4300 loss: 0.1223 acc: 86.60\n",
      "ep: 4400 loss: 0.1220 acc: 87.11\n",
      "ep: 4500 loss: 0.1216 acc: 87.11\n",
      "ep: 4600 loss: 0.1210 acc: 87.11\n",
      "ep: 4700 loss: 0.1205 acc: 86.60\n",
      "ep: 4800 loss: 0.1199 acc: 86.60\n",
      "ep: 4900 loss: 0.1194 acc: 87.11\n",
      "ep: 5000 loss: 0.1188 acc: 87.11\n",
      "ep: 5100 loss: 0.1182 acc: 87.11\n",
      "ep: 5200 loss: 0.1177 acc: 86.60\n",
      "ep: 5300 loss: 0.1171 acc: 86.60\n",
      "ep: 5400 loss: 0.1166 acc: 86.60\n",
      "ep: 5500 loss: 0.1160 acc: 86.60\n",
      "ep: 5600 loss: 0.1155 acc: 86.60\n",
      "ep: 5700 loss: 0.1150 acc: 86.60\n",
      "ep: 5800 loss: 0.1145 acc: 86.60\n",
      "ep: 5900 loss: 0.1140 acc: 86.08\n",
      "ep: 6000 loss: 0.1136 acc: 86.08\n",
      "ep: 6100 loss: 0.1131 acc: 86.08\n",
      "ep: 6200 loss: 0.1127 acc: 86.08\n",
      "ep: 6300 loss: 0.1122 acc: 86.08\n",
      "ep: 6400 loss: 0.1118 acc: 86.08\n",
      "ep: 6500 loss: 0.1114 acc: 86.08\n",
      "ep: 6600 loss: 0.1110 acc: 86.08\n",
      "init:0.5, acc:86.08247375488281 , epoch:6632\n",
      "ep:  100 loss: 0.6449 acc: 62.89\n",
      "ep:  200 loss: 0.5618 acc: 64.95\n",
      "ep:  300 loss: 0.4666 acc: 66.49\n",
      "ep:  400 loss: 0.3839 acc: 70.10\n",
      "ep:  500 loss: 0.3366 acc: 74.23\n",
      "ep:  600 loss: 0.3036 acc: 75.77\n",
      "ep:  700 loss: 0.2728 acc: 75.26\n",
      "ep:  800 loss: 0.2501 acc: 73.71\n",
      "ep:  900 loss: 0.2320 acc: 73.71\n",
      "ep: 1000 loss: 0.2205 acc: 74.74\n",
      "ep: 1100 loss: 0.2106 acc: 76.29\n",
      "ep: 1200 loss: 0.2022 acc: 76.80\n",
      "ep: 1300 loss: 0.1948 acc: 75.77\n",
      "ep: 1400 loss: 0.1884 acc: 75.77\n",
      "ep: 1500 loss: 0.1827 acc: 76.29\n",
      "ep: 1600 loss: 0.1777 acc: 76.29\n",
      "ep: 1700 loss: 0.1729 acc: 76.29\n",
      "ep: 1800 loss: 0.1677 acc: 77.32\n",
      "ep: 1900 loss: 0.1493 acc: 78.87\n",
      "ep: 2000 loss: 0.1348 acc: 79.90\n",
      "ep: 2100 loss: 0.1277 acc: 80.41\n",
      "ep: 2200 loss: 0.1227 acc: 80.93\n",
      "ep: 2300 loss: 0.1187 acc: 80.93\n",
      "ep: 2400 loss: 0.1151 acc: 80.93\n",
      "ep: 2500 loss: 0.1117 acc: 80.41\n",
      "ep: 2600 loss: 0.1086 acc: 80.93\n",
      "ep: 2700 loss: 0.1058 acc: 80.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 2800 loss: 0.1033 acc: 80.41\n",
      "ep: 2900 loss: 0.1009 acc: 79.90\n",
      "ep: 3000 loss: 0.0986 acc: 79.38\n",
      "ep: 3100 loss: 0.0963 acc: 79.90\n",
      "ep: 3200 loss: 0.0941 acc: 80.41\n",
      "ep: 3300 loss: 0.0924 acc: 80.41\n",
      "ep: 3400 loss: 0.0911 acc: 80.41\n",
      "ep: 3500 loss: 0.0901 acc: 80.41\n",
      "ep: 3600 loss: 0.0898 acc: 81.44\n",
      "ep: 3700 loss: 0.0911 acc: 82.99\n",
      "ep: 3800 loss: 0.0899 acc: 82.47\n",
      "ep: 3900 loss: 0.0880 acc: 84.54\n",
      "ep: 4000 loss: 0.0860 acc: 82.99\n",
      "ep: 4100 loss: 0.0832 acc: 84.02\n",
      "ep: 4200 loss: 0.0802 acc: 83.51\n",
      "ep: 4300 loss: 0.0777 acc: 84.02\n",
      "ep: 4400 loss: 0.0755 acc: 84.02\n",
      "ep: 4500 loss: 0.0736 acc: 84.02\n",
      "ep: 4600 loss: 0.0720 acc: 84.02\n",
      "ep: 4700 loss: 0.0707 acc: 84.02\n",
      "ep: 4800 loss: 0.0695 acc: 84.54\n",
      "ep: 4900 loss: 0.0686 acc: 84.54\n",
      "ep: 5000 loss: 0.0677 acc: 85.05\n",
      "ep: 5100 loss: 0.0670 acc: 85.05\n",
      "ep: 5200 loss: 0.0664 acc: 85.05\n",
      "ep: 5300 loss: 0.0659 acc: 85.57\n",
      "ep: 5400 loss: 0.0654 acc: 85.57\n",
      "ep: 5500 loss: 0.0648 acc: 86.08\n",
      "ep: 5600 loss: 0.0643 acc: 86.08\n",
      "ep: 5700 loss: 0.0639 acc: 86.08\n",
      "ep: 5800 loss: 0.0635 acc: 86.08\n",
      "ep: 5900 loss: 0.0632 acc: 86.60\n",
      "ep: 6000 loss: 0.0628 acc: 86.60\n",
      "ep: 6100 loss: 0.0638 acc: 86.60\n",
      "ep: 6200 loss: 0.0622 acc: 86.60\n",
      "ep: 6300 loss: 0.0619 acc: 87.11\n",
      "ep: 6400 loss: 0.0616 acc: 87.11\n",
      "ep: 6500 loss: 0.0614 acc: 87.11\n",
      "ep: 6600 loss: 0.0613 acc: 87.63\n",
      "init:0.6, acc:87.62886810302734 , epoch:6631\n",
      "ep:  100 loss: 0.5938 acc: 58.76\n",
      "ep:  200 loss: 0.5288 acc: 65.98\n",
      "ep:  300 loss: 0.4742 acc: 67.01\n",
      "ep:  400 loss: 0.4064 acc: 68.56\n",
      "ep:  500 loss: 0.3410 acc: 70.62\n",
      "ep:  600 loss: 0.2939 acc: 72.16\n",
      "ep:  700 loss: 0.2637 acc: 72.68\n",
      "ep:  800 loss: 0.2390 acc: 72.16\n",
      "ep:  900 loss: 0.2187 acc: 75.77\n",
      "ep: 1000 loss: 0.2010 acc: 74.74\n",
      "ep: 1100 loss: 0.1811 acc: 74.23\n",
      "ep: 1200 loss: 0.1644 acc: 74.74\n",
      "ep: 1300 loss: 0.1537 acc: 75.26\n",
      "ep: 1400 loss: 0.1441 acc: 77.84\n",
      "ep: 1500 loss: 0.1352 acc: 78.87\n",
      "ep: 1600 loss: 0.1280 acc: 78.87\n",
      "ep: 1700 loss: 0.1217 acc: 80.93\n",
      "ep: 1800 loss: 0.1159 acc: 80.93\n",
      "ep: 1900 loss: 0.1107 acc: 81.96\n",
      "ep: 2000 loss: 0.1061 acc: 81.44\n",
      "ep: 2100 loss: 0.1021 acc: 82.99\n",
      "ep: 2200 loss: 0.0986 acc: 82.99\n",
      "ep: 2300 loss: 0.0956 acc: 82.99\n",
      "ep: 2400 loss: 0.0929 acc: 82.99\n",
      "ep: 2500 loss: 0.0905 acc: 83.51\n",
      "ep: 2600 loss: 0.0884 acc: 83.51\n",
      "ep: 2700 loss: 0.0864 acc: 84.02\n",
      "ep: 2800 loss: 0.0848 acc: 84.02\n",
      "ep: 2900 loss: 0.0832 acc: 84.02\n",
      "ep: 3000 loss: 0.0818 acc: 84.02\n",
      "ep: 3100 loss: 0.0805 acc: 83.51\n",
      "ep: 3200 loss: 0.0793 acc: 83.51\n",
      "ep: 3300 loss: 0.0780 acc: 83.51\n",
      "ep: 3400 loss: 0.0764 acc: 84.02\n",
      "ep: 3500 loss: 0.0744 acc: 84.02\n",
      "ep: 3600 loss: 0.0673 acc: 85.57\n",
      "ep: 3700 loss: 0.0650 acc: 85.57\n",
      "ep: 3800 loss: 0.0637 acc: 84.54\n",
      "ep: 3900 loss: 0.0627 acc: 85.05\n",
      "ep: 4000 loss: 0.0619 acc: 85.05\n",
      "ep: 4100 loss: 0.0612 acc: 85.05\n",
      "ep: 4200 loss: 0.0606 acc: 85.05\n",
      "ep: 4300 loss: 0.0600 acc: 85.57\n",
      "ep: 4400 loss: 0.0594 acc: 85.05\n",
      "ep: 4500 loss: 0.0586 acc: 85.05\n",
      "ep: 4600 loss: 0.0578 acc: 85.05\n",
      "ep: 4700 loss: 0.0569 acc: 85.57\n",
      "ep: 4800 loss: 0.0560 acc: 85.57\n",
      "ep: 4900 loss: 0.0551 acc: 85.57\n",
      "ep: 5000 loss: 0.0543 acc: 85.57\n",
      "ep: 5100 loss: 0.0535 acc: 86.08\n",
      "ep: 5200 loss: 0.0528 acc: 86.08\n",
      "ep: 5300 loss: 0.0522 acc: 86.08\n",
      "ep: 5400 loss: 0.0515 acc: 86.08\n",
      "ep: 5500 loss: 0.0508 acc: 86.08\n",
      "ep: 5600 loss: 0.0503 acc: 86.08\n",
      "ep: 5700 loss: 0.0500 acc: 85.57\n",
      "ep: 5800 loss: 0.0497 acc: 85.57\n",
      "ep: 5900 loss: 0.0493 acc: 85.57\n",
      "ep: 6000 loss: 0.0490 acc: 85.05\n",
      "ep: 6100 loss: 0.0488 acc: 85.05\n",
      "ep: 6200 loss: 0.0487 acc: 84.54\n",
      "ep: 6300 loss: 0.0487 acc: 84.54\n",
      "ep: 6400 loss: 0.0486 acc: 84.54\n",
      "ep: 6500 loss: 0.0485 acc: 84.54\n",
      "ep: 6600 loss: 0.0484 acc: 84.54\n",
      "init:0.7000000000000001, acc:84.53607940673828 , epoch:6630\n",
      "ep:  100 loss: 0.5807 acc: 61.34\n",
      "ep:  200 loss: 0.4744 acc: 67.53\n",
      "ep:  300 loss: 0.3896 acc: 71.65\n",
      "ep:  400 loss: 0.3333 acc: 72.68\n",
      "ep:  500 loss: 0.2941 acc: 74.74\n",
      "ep:  600 loss: 0.2622 acc: 75.77\n",
      "ep:  700 loss: 0.2400 acc: 78.35\n",
      "ep:  800 loss: 0.2234 acc: 79.90\n",
      "ep:  900 loss: 0.2135 acc: 80.41\n",
      "ep: 1000 loss: 0.2080 acc: 79.90\n",
      "ep: 1100 loss: 0.2000 acc: 80.41\n",
      "ep: 1200 loss: 0.1928 acc: 80.41\n",
      "ep: 1300 loss: 0.1853 acc: 80.41\n",
      "ep: 1400 loss: 0.1798 acc: 80.41\n",
      "ep: 1500 loss: 0.1756 acc: 80.93\n",
      "ep: 1600 loss: 0.1720 acc: 81.44\n",
      "ep: 1700 loss: 0.1687 acc: 81.44\n",
      "ep: 1800 loss: 0.1654 acc: 81.96\n",
      "ep: 1900 loss: 0.1624 acc: 81.96\n",
      "ep: 2000 loss: 0.1605 acc: 81.96\n",
      "ep: 2100 loss: 0.1592 acc: 81.96\n",
      "ep: 2200 loss: 0.1580 acc: 81.96\n",
      "ep: 2300 loss: 0.1567 acc: 81.96\n",
      "ep: 2400 loss: 0.1560 acc: 81.96\n",
      "ep: 2500 loss: 0.1547 acc: 82.47\n",
      "ep: 2600 loss: 0.1534 acc: 82.47\n",
      "ep: 2700 loss: 0.1515 acc: 81.96\n",
      "ep: 2800 loss: 0.1471 acc: 81.96\n",
      "ep: 2900 loss: 0.1400 acc: 83.51\n",
      "ep: 3000 loss: 0.1326 acc: 83.51\n",
      "ep: 3100 loss: 0.1273 acc: 84.02\n",
      "ep: 3200 loss: 0.1229 acc: 83.51\n",
      "ep: 3300 loss: 0.1186 acc: 82.47\n",
      "ep: 3400 loss: 0.1140 acc: 82.47\n",
      "ep: 3500 loss: 0.1107 acc: 82.99\n",
      "ep: 3600 loss: 0.1083 acc: 84.02\n",
      "ep: 3700 loss: 0.1063 acc: 84.54\n",
      "ep: 3800 loss: 0.1043 acc: 84.54\n",
      "ep: 3900 loss: 0.1019 acc: 84.02\n",
      "ep: 4000 loss: 0.0993 acc: 85.05\n",
      "ep: 4100 loss: 0.0969 acc: 85.57\n",
      "ep: 4200 loss: 0.0950 acc: 84.54\n",
      "ep: 4300 loss: 0.0933 acc: 85.57\n",
      "ep: 4400 loss: 0.0920 acc: 85.57\n",
      "ep: 4500 loss: 0.0907 acc: 86.08\n",
      "ep: 4600 loss: 0.0897 acc: 86.60\n",
      "ep: 4700 loss: 0.0887 acc: 86.60\n",
      "ep: 4800 loss: 0.0879 acc: 86.60\n",
      "ep: 4900 loss: 0.0871 acc: 86.60\n",
      "ep: 5000 loss: 0.0864 acc: 86.60\n",
      "ep: 5100 loss: 0.0857 acc: 86.60\n",
      "ep: 5200 loss: 0.0851 acc: 86.60\n",
      "ep: 5300 loss: 0.0846 acc: 86.60\n",
      "ep: 5400 loss: 0.0841 acc: 86.08\n",
      "ep: 5500 loss: 0.0836 acc: 86.08\n",
      "ep: 5600 loss: 0.0831 acc: 86.08\n",
      "ep: 5700 loss: 0.0825 acc: 85.57\n",
      "ep: 5800 loss: 0.0819 acc: 85.57\n",
      "ep: 5900 loss: 0.0812 acc: 85.57\n",
      "ep: 6000 loss: 0.0805 acc: 86.08\n",
      "ep: 6100 loss: 0.0797 acc: 86.08\n",
      "ep: 6200 loss: 0.0788 acc: 86.08\n",
      "ep: 6300 loss: 0.0779 acc: 86.08\n",
      "ep: 6400 loss: 0.0772 acc: 86.08\n",
      "ep: 6500 loss: 0.0766 acc: 86.08\n",
      "ep: 6600 loss: 0.0761 acc: 86.08\n",
      "init:0.8, acc:86.08247375488281 , epoch:6629\n",
      "ep:  100 loss: 0.6124 acc: 58.76\n",
      "ep:  200 loss: 0.5452 acc: 62.37\n",
      "ep:  300 loss: 0.4566 acc: 70.10\n",
      "ep:  400 loss: 0.3582 acc: 72.16\n",
      "ep:  500 loss: 0.2764 acc: 77.32\n",
      "ep:  600 loss: 0.2291 acc: 80.93\n",
      "ep:  700 loss: 0.1903 acc: 82.99\n",
      "ep:  800 loss: 0.1642 acc: 84.02\n",
      "ep:  900 loss: 0.1445 acc: 84.54\n",
      "ep: 1000 loss: 0.1295 acc: 84.54\n",
      "ep: 1100 loss: 0.1118 acc: 84.54\n",
      "ep: 1200 loss: 0.0971 acc: 85.57\n",
      "ep: 1300 loss: 0.0879 acc: 86.08\n",
      "ep: 1400 loss: 0.0812 acc: 86.08\n",
      "ep: 1500 loss: 0.0765 acc: 86.60\n",
      "ep: 1600 loss: 0.0768 acc: 90.21\n",
      "ep: 1700 loss: 0.0726 acc: 90.21\n",
      "ep: 1800 loss: 0.0688 acc: 90.72\n",
      "ep: 1900 loss: 0.0654 acc: 91.24\n",
      "ep: 2000 loss: 0.0628 acc: 91.24\n",
      "ep: 2100 loss: 0.0606 acc: 91.24\n",
      "ep: 2200 loss: 0.0585 acc: 91.24\n",
      "ep: 2300 loss: 0.0553 acc: 91.24\n",
      "ep: 2400 loss: 0.0519 acc: 91.75\n",
      "ep: 2500 loss: 0.0497 acc: 91.75\n",
      "ep: 2600 loss: 0.0482 acc: 91.75\n",
      "ep: 2700 loss: 0.0468 acc: 92.27\n",
      "ep: 2800 loss: 0.0455 acc: 92.27\n",
      "ep: 2900 loss: 0.0438 acc: 92.78\n",
      "ep: 3000 loss: 0.0420 acc: 92.78\n",
      "ep: 3100 loss: 0.0405 acc: 92.27\n",
      "ep: 3200 loss: 0.0396 acc: 91.75\n",
      "ep: 3300 loss: 0.0390 acc: 91.75\n",
      "ep: 3400 loss: 0.0387 acc: 91.75\n",
      "ep: 3500 loss: 0.0388 acc: 92.27\n",
      "ep: 3600 loss: 0.0393 acc: 92.78\n",
      "ep: 3700 loss: 0.0400 acc: 92.78\n",
      "ep: 3800 loss: 0.0404 acc: 93.30\n",
      "ep: 3900 loss: 0.0406 acc: 94.33\n",
      "ep: 4000 loss: 0.0407 acc: 94.33\n",
      "ep: 4100 loss: 0.0407 acc: 94.33\n",
      "ep: 4200 loss: 0.0407 acc: 94.33\n",
      "ep: 4300 loss: 0.0404 acc: 94.33\n",
      "ep: 4400 loss: 0.0375 acc: 94.85\n",
      "ep: 4500 loss: 0.0357 acc: 94.85\n",
      "ep: 4600 loss: 0.0343 acc: 94.85\n",
      "ep: 4700 loss: 0.0329 acc: 94.85\n",
      "ep: 4800 loss: 0.0317 acc: 94.85\n",
      "ep: 4900 loss: 0.0304 acc: 94.85\n",
      "ep: 5000 loss: 0.0283 acc: 95.88\n",
      "ep: 5100 loss: 0.0254 acc: 95.88\n",
      "ep: 5200 loss: 0.0234 acc: 95.88\n",
      "ep: 5300 loss: 0.0221 acc: 96.39\n",
      "ep: 5400 loss: 0.0211 acc: 96.39\n",
      "ep: 5500 loss: 0.0204 acc: 96.39\n",
      "ep: 5600 loss: 0.0275 acc: 95.88\n",
      "ep: 5700 loss: 0.0196 acc: 96.39\n",
      "ep: 5800 loss: 0.0195 acc: 96.39\n",
      "ep: 5900 loss: 0.0194 acc: 96.39\n",
      "ep: 6000 loss: 0.0193 acc: 96.39\n",
      "ep: 6100 loss: 0.0192 acc: 96.39\n",
      "ep: 6200 loss: 0.0191 acc: 96.39\n",
      "ep: 6300 loss: 0.0190 acc: 96.39\n",
      "ep: 6400 loss: 0.0189 acc: 96.39\n",
      "ep: 6500 loss: 0.0188 acc: 96.39\n",
      "ep: 6600 loss: 0.0187 acc: 96.39\n",
      "init:0.9, acc:96.39175415039062 , epoch:6628\n",
      "ep:  100 loss: 0.6424 acc: 59.28\n",
      "ep:  200 loss: 0.5790 acc: 59.28\n",
      "ep:  300 loss: 0.5498 acc: 62.89\n",
      "ep:  400 loss: 0.5224 acc: 66.49\n",
      "ep:  500 loss: 0.4853 acc: 64.95\n",
      "ep:  600 loss: 0.4268 acc: 64.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:  700 loss: 0.3807 acc: 68.56\n",
      "ep:  800 loss: 0.3462 acc: 69.07\n",
      "ep:  900 loss: 0.3167 acc: 69.59\n",
      "ep: 1000 loss: 0.2966 acc: 70.62\n",
      "ep: 1100 loss: 0.2800 acc: 73.20\n",
      "ep: 1200 loss: 0.2609 acc: 72.68\n",
      "ep: 1300 loss: 0.2430 acc: 73.71\n",
      "ep: 1400 loss: 0.2269 acc: 73.71\n",
      "ep: 1500 loss: 0.2134 acc: 74.74\n",
      "ep: 1600 loss: 0.2026 acc: 77.32\n",
      "ep: 1700 loss: 0.1944 acc: 75.26\n",
      "ep: 1800 loss: 0.1880 acc: 75.26\n",
      "ep: 1900 loss: 0.1821 acc: 77.84\n",
      "ep: 2000 loss: 0.1772 acc: 79.38\n",
      "ep: 2100 loss: 0.1758 acc: 81.96\n",
      "ep: 2200 loss: 0.1691 acc: 83.51\n",
      "ep: 2300 loss: 0.1657 acc: 82.47\n",
      "ep: 2400 loss: 0.1616 acc: 85.57\n",
      "ep: 2500 loss: 0.1572 acc: 87.63\n",
      "ep: 2600 loss: 0.1528 acc: 87.63\n",
      "ep: 2700 loss: 0.1482 acc: 87.11\n",
      "ep: 2800 loss: 0.1430 acc: 87.63\n",
      "ep: 2900 loss: 0.1387 acc: 87.63\n",
      "ep: 3000 loss: 0.1338 acc: 87.63\n",
      "ep: 3100 loss: 0.1291 acc: 88.66\n",
      "ep: 3200 loss: 0.1250 acc: 88.66\n",
      "ep: 3300 loss: 0.1213 acc: 88.66\n",
      "ep: 3400 loss: 0.1194 acc: 89.69\n",
      "ep: 3500 loss: 0.1174 acc: 90.21\n",
      "ep: 3600 loss: 0.1153 acc: 90.21\n",
      "ep: 3700 loss: 0.1131 acc: 90.21\n",
      "ep: 3800 loss: 0.1111 acc: 90.21\n",
      "ep: 3900 loss: 0.1090 acc: 90.72\n",
      "ep: 4000 loss: 0.1066 acc: 90.72\n",
      "ep: 4100 loss: 0.1032 acc: 91.24\n",
      "ep: 4200 loss: 0.0997 acc: 91.24\n",
      "ep: 4300 loss: 0.0970 acc: 91.75\n",
      "ep: 4400 loss: 0.0947 acc: 91.75\n",
      "ep: 4500 loss: 0.0926 acc: 91.75\n",
      "ep: 4600 loss: 0.0902 acc: 91.75\n",
      "ep: 4700 loss: 0.0674 acc: 92.27\n",
      "ep: 4800 loss: 0.0602 acc: 92.78\n",
      "ep: 4900 loss: 0.0525 acc: 92.78\n",
      "ep: 5000 loss: 0.0472 acc: 92.78\n",
      "ep: 5100 loss: 0.0440 acc: 93.30\n",
      "ep: 5200 loss: 0.0415 acc: 93.30\n",
      "ep: 5300 loss: 0.0393 acc: 93.30\n",
      "ep: 5400 loss: 0.0373 acc: 93.30\n",
      "ep: 5500 loss: 0.0356 acc: 93.30\n",
      "ep: 5600 loss: 0.0343 acc: 93.30\n",
      "ep: 5700 loss: 0.0331 acc: 93.30\n",
      "ep: 5800 loss: 0.0322 acc: 93.30\n",
      "ep: 5900 loss: 0.0312 acc: 93.30\n",
      "ep: 6000 loss: 0.0303 acc: 93.30\n",
      "ep: 6100 loss: 0.0294 acc: 93.30\n",
      "ep: 6200 loss: 0.0287 acc: 93.30\n",
      "ep: 6300 loss: 0.0280 acc: 93.30\n",
      "ep: 6400 loss: 0.0274 acc: 93.30\n",
      "ep: 6500 loss: 0.0268 acc: 93.30\n",
      "ep: 6600 loss: 0.0263 acc: 93.30\n",
      "init:1.0, acc:93.29896545410156 , epoch:6627\n",
      "ep:  100 loss: 0.6027 acc: 59.28\n",
      "ep:  200 loss: 0.5467 acc: 61.86\n",
      "ep:  300 loss: 0.4920 acc: 64.43\n",
      "ep:  400 loss: 0.4449 acc: 66.49\n",
      "ep:  500 loss: 0.4113 acc: 67.53\n",
      "ep:  600 loss: 0.3794 acc: 69.07\n",
      "ep:  700 loss: 0.3555 acc: 71.65\n",
      "ep:  800 loss: 0.3385 acc: 71.65\n",
      "ep:  900 loss: 0.3242 acc: 73.20\n",
      "ep: 1000 loss: 0.3126 acc: 75.26\n",
      "ep: 1100 loss: 0.3031 acc: 75.77\n",
      "ep: 1200 loss: 0.2968 acc: 74.74\n",
      "ep: 1300 loss: 0.2888 acc: 74.74\n",
      "ep: 1400 loss: 0.2819 acc: 75.26\n",
      "ep: 1500 loss: 0.2761 acc: 75.77\n",
      "ep: 1600 loss: 0.2710 acc: 76.80\n",
      "ep: 1700 loss: 0.2666 acc: 77.32\n",
      "ep: 1800 loss: 0.2625 acc: 76.29\n",
      "ep: 1900 loss: 0.2583 acc: 76.80\n",
      "ep: 2000 loss: 0.2538 acc: 77.32\n",
      "ep: 2100 loss: 0.2491 acc: 77.84\n",
      "ep: 2200 loss: 0.2438 acc: 78.35\n",
      "ep: 2300 loss: 0.2376 acc: 78.35\n",
      "ep: 2400 loss: 0.2302 acc: 77.32\n",
      "ep: 2500 loss: 0.2227 acc: 76.80\n",
      "ep: 2600 loss: 0.2147 acc: 77.84\n",
      "ep: 2700 loss: 0.2083 acc: 78.35\n",
      "ep: 2800 loss: 0.1992 acc: 77.32\n",
      "ep: 2900 loss: 0.1848 acc: 77.84\n",
      "ep: 3000 loss: 0.1800 acc: 78.87\n",
      "ep: 3100 loss: 0.1629 acc: 79.38\n",
      "ep: 3200 loss: 0.1492 acc: 82.47\n",
      "ep: 3300 loss: 0.1425 acc: 82.47\n",
      "ep: 3400 loss: 0.1361 acc: 82.47\n",
      "ep: 3500 loss: 0.1293 acc: 82.47\n",
      "ep: 3600 loss: 0.1254 acc: 81.96\n",
      "ep: 3700 loss: 0.1217 acc: 81.96\n",
      "ep: 3800 loss: 0.1186 acc: 82.47\n",
      "ep: 3900 loss: 0.1115 acc: 84.54\n",
      "ep: 4000 loss: 0.1047 acc: 85.05\n",
      "ep: 4100 loss: 0.1015 acc: 84.54\n",
      "ep: 4200 loss: 0.0993 acc: 85.05\n",
      "ep: 4300 loss: 0.0975 acc: 85.05\n",
      "ep: 4400 loss: 0.0960 acc: 84.54\n",
      "ep: 4500 loss: 0.0949 acc: 84.54\n",
      "ep: 4600 loss: 0.0940 acc: 84.54\n",
      "ep: 4700 loss: 0.0931 acc: 84.54\n",
      "ep: 4800 loss: 0.0922 acc: 84.54\n",
      "ep: 4900 loss: 0.0913 acc: 84.02\n",
      "ep: 5000 loss: 0.0904 acc: 84.54\n",
      "ep: 5100 loss: 0.0897 acc: 84.54\n",
      "ep: 5200 loss: 0.0890 acc: 84.54\n",
      "ep: 5300 loss: 0.0884 acc: 84.54\n",
      "ep: 5400 loss: 0.0878 acc: 85.05\n",
      "ep: 5500 loss: 0.0872 acc: 85.05\n",
      "ep: 5600 loss: 0.0867 acc: 84.54\n",
      "ep: 5700 loss: 0.0864 acc: 84.54\n",
      "ep: 5800 loss: 0.0861 acc: 85.05\n",
      "ep: 5900 loss: 0.0858 acc: 86.08\n",
      "ep: 6000 loss: 0.0856 acc: 86.60\n",
      "ep: 6100 loss: 0.0853 acc: 86.60\n",
      "ep: 6200 loss: 0.0850 acc: 87.11\n",
      "ep: 6300 loss: 0.0847 acc: 87.11\n",
      "ep: 6400 loss: 0.0844 acc: 87.11\n",
      "ep: 6500 loss: 0.0839 acc: 87.63\n",
      "ep: 6600 loss: 0.0835 acc: 88.14\n",
      "init:1.1, acc:88.14433288574219 , epoch:6626\n",
      "ep:  100 loss: 0.5768 acc: 62.37\n",
      "ep:  200 loss: 0.5013 acc: 64.95\n",
      "ep:  300 loss: 0.4590 acc: 63.92\n",
      "ep:  400 loss: 0.4270 acc: 64.95\n",
      "ep:  500 loss: 0.3974 acc: 68.56\n",
      "ep:  600 loss: 0.3746 acc: 73.20\n",
      "ep:  700 loss: 0.3531 acc: 75.26\n",
      "ep:  800 loss: 0.3363 acc: 75.77\n",
      "ep:  900 loss: 0.3243 acc: 75.77\n",
      "ep: 1000 loss: 0.3118 acc: 75.26\n",
      "ep: 1100 loss: 0.3012 acc: 74.74\n",
      "ep: 1200 loss: 0.2879 acc: 76.29\n",
      "ep: 1300 loss: 0.2771 acc: 77.32\n",
      "ep: 1400 loss: 0.2678 acc: 75.77\n",
      "ep: 1500 loss: 0.2608 acc: 76.29\n",
      "ep: 1600 loss: 0.2551 acc: 77.84\n",
      "ep: 1700 loss: 0.2504 acc: 78.35\n",
      "ep: 1800 loss: 0.2462 acc: 78.35\n",
      "ep: 1900 loss: 0.2425 acc: 78.35\n",
      "ep: 2000 loss: 0.2391 acc: 78.35\n",
      "ep: 2100 loss: 0.2360 acc: 78.35\n",
      "ep: 2200 loss: 0.2332 acc: 77.84\n",
      "ep: 2300 loss: 0.2305 acc: 76.80\n",
      "ep: 2400 loss: 0.2275 acc: 78.35\n",
      "ep: 2500 loss: 0.2235 acc: 78.87\n",
      "ep: 2600 loss: 0.2180 acc: 78.35\n",
      "ep: 2700 loss: 0.2119 acc: 78.87\n",
      "ep: 2800 loss: 0.2055 acc: 78.87\n",
      "ep: 2900 loss: 0.1989 acc: 78.87\n",
      "ep: 3000 loss: 0.1881 acc: 80.41\n",
      "ep: 3100 loss: 0.1762 acc: 80.93\n",
      "ep: 3200 loss: 0.1679 acc: 81.96\n",
      "ep: 3300 loss: 0.1619 acc: 81.96\n",
      "ep: 3400 loss: 0.1572 acc: 81.96\n",
      "ep: 3500 loss: 0.1531 acc: 81.96\n",
      "ep: 3600 loss: 0.1496 acc: 82.99\n",
      "ep: 3700 loss: 0.1463 acc: 83.51\n",
      "ep: 3800 loss: 0.1431 acc: 83.51\n",
      "ep: 3900 loss: 0.1391 acc: 82.47\n",
      "ep: 4000 loss: 0.1286 acc: 84.54\n",
      "ep: 4100 loss: 0.1198 acc: 84.54\n",
      "ep: 4200 loss: 0.1161 acc: 84.54\n",
      "ep: 4300 loss: 0.1132 acc: 84.54\n",
      "ep: 4400 loss: 0.1108 acc: 84.54\n",
      "ep: 4500 loss: 0.1088 acc: 84.02\n",
      "ep: 4600 loss: 0.1071 acc: 84.02\n",
      "ep: 4700 loss: 0.1056 acc: 84.02\n",
      "ep: 4800 loss: 0.1043 acc: 84.02\n",
      "ep: 4900 loss: 0.1031 acc: 84.02\n",
      "ep: 5000 loss: 0.1020 acc: 84.02\n",
      "ep: 5100 loss: 0.1011 acc: 84.02\n",
      "ep: 5200 loss: 0.1002 acc: 84.02\n",
      "ep: 5300 loss: 0.0993 acc: 84.02\n",
      "ep: 5400 loss: 0.0985 acc: 83.51\n",
      "ep: 5500 loss: 0.0977 acc: 83.51\n",
      "ep: 5600 loss: 0.0969 acc: 84.02\n",
      "ep: 5700 loss: 0.0962 acc: 84.02\n",
      "ep: 5800 loss: 0.0956 acc: 84.02\n",
      "ep: 5900 loss: 0.0950 acc: 84.02\n",
      "ep: 6000 loss: 0.0945 acc: 84.02\n",
      "ep: 6100 loss: 0.0941 acc: 84.02\n",
      "ep: 6200 loss: 0.0936 acc: 84.02\n",
      "ep: 6300 loss: 0.0933 acc: 84.02\n",
      "ep: 6400 loss: 0.0933 acc: 84.02\n",
      "ep: 6500 loss: 0.0949 acc: 84.54\n",
      "ep: 6600 loss: 0.0917 acc: 84.54\n",
      "init:1.2000000000000002, acc:84.53607940673828 , epoch:6625\n",
      "ep:  100 loss: 0.5657 acc: 62.37\n",
      "ep:  200 loss: 0.4958 acc: 62.37\n",
      "ep:  300 loss: 0.4594 acc: 61.86\n",
      "ep:  400 loss: 0.4395 acc: 64.95\n",
      "ep:  500 loss: 0.4203 acc: 68.04\n",
      "ep:  600 loss: 0.4062 acc: 70.10\n",
      "ep:  700 loss: 0.3915 acc: 74.23\n",
      "ep:  800 loss: 0.3751 acc: 74.74\n",
      "ep:  900 loss: 0.3405 acc: 77.32\n",
      "ep: 1000 loss: 0.3123 acc: 76.80\n",
      "ep: 1100 loss: 0.2879 acc: 79.90\n",
      "ep: 1200 loss: 0.2645 acc: 81.96\n",
      "ep: 1300 loss: 0.2464 acc: 81.44\n",
      "ep: 1400 loss: 0.2323 acc: 81.96\n",
      "ep: 1500 loss: 0.2208 acc: 80.93\n",
      "ep: 1600 loss: 0.2114 acc: 81.44\n",
      "ep: 1700 loss: 0.2046 acc: 85.05\n",
      "ep: 1800 loss: 0.2021 acc: 85.05\n",
      "ep: 1900 loss: 0.1935 acc: 87.11\n",
      "ep: 2000 loss: 0.1847 acc: 87.63\n",
      "ep: 2100 loss: 0.1764 acc: 87.63\n",
      "ep: 2200 loss: 0.1693 acc: 87.11\n",
      "ep: 2300 loss: 0.1630 acc: 87.11\n",
      "ep: 2400 loss: 0.1570 acc: 87.11\n",
      "ep: 2500 loss: 0.1520 acc: 87.11\n",
      "ep: 2600 loss: 0.1468 acc: 86.60\n",
      "ep: 2700 loss: 0.1417 acc: 86.60\n",
      "ep: 2800 loss: 0.1368 acc: 88.14\n",
      "ep: 2900 loss: 0.1322 acc: 87.63\n",
      "ep: 3000 loss: 0.1280 acc: 87.63\n",
      "ep: 3100 loss: 0.1242 acc: 87.11\n",
      "ep: 3200 loss: 0.1208 acc: 87.63\n",
      "ep: 3300 loss: 0.1184 acc: 89.18\n",
      "ep: 3400 loss: 0.1175 acc: 90.21\n",
      "ep: 3500 loss: 0.1158 acc: 89.18\n",
      "ep: 3600 loss: 0.1136 acc: 90.21\n",
      "ep: 3700 loss: 0.1113 acc: 90.21\n",
      "ep: 3800 loss: 0.1090 acc: 90.21\n",
      "ep: 3900 loss: 0.1073 acc: 89.69\n",
      "ep: 4000 loss: 0.1061 acc: 90.21\n",
      "ep: 4100 loss: 0.1047 acc: 89.69\n",
      "ep: 4200 loss: 0.1031 acc: 90.21\n",
      "ep: 4300 loss: 0.1015 acc: 90.21\n",
      "ep: 4400 loss: 0.1004 acc: 90.21\n",
      "ep: 4500 loss: 0.1027 acc: 91.24\n",
      "ep: 4600 loss: 0.1011 acc: 91.75\n",
      "ep: 4700 loss: 0.0999 acc: 91.75\n",
      "ep: 4800 loss: 0.0986 acc: 91.75\n",
      "ep: 4900 loss: 0.0967 acc: 91.75\n",
      "ep: 5000 loss: 0.0951 acc: 91.75\n",
      "ep: 5100 loss: 0.0940 acc: 91.75\n",
      "ep: 5200 loss: 0.0931 acc: 91.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 5300 loss: 0.0922 acc: 91.75\n",
      "ep: 5400 loss: 0.0912 acc: 91.75\n",
      "ep: 5500 loss: 0.0900 acc: 92.27\n",
      "ep: 5600 loss: 0.0888 acc: 92.27\n",
      "ep: 5700 loss: 0.0878 acc: 92.27\n",
      "ep: 5800 loss: 0.0866 acc: 92.27\n",
      "ep: 5900 loss: 0.0857 acc: 92.27\n",
      "ep: 6000 loss: 0.0849 acc: 92.27\n",
      "ep: 6100 loss: 0.0841 acc: 92.27\n",
      "ep: 6200 loss: 0.0835 acc: 92.78\n",
      "ep: 6300 loss: 0.0829 acc: 92.78\n",
      "ep: 6400 loss: 0.0824 acc: 92.78\n",
      "ep: 6500 loss: 0.0819 acc: 92.78\n",
      "ep: 6600 loss: 0.0814 acc: 92.78\n",
      "init:1.3000000000000003, acc:92.78350830078125 , epoch:6624\n",
      "ep:  100 loss: 0.5693 acc: 62.37\n",
      "ep:  200 loss: 0.4984 acc: 65.98\n",
      "ep:  300 loss: 0.4398 acc: 69.59\n",
      "ep:  400 loss: 0.3964 acc: 72.68\n",
      "ep:  500 loss: 0.3634 acc: 72.68\n",
      "ep:  600 loss: 0.3345 acc: 72.68\n",
      "ep:  700 loss: 0.3064 acc: 74.23\n",
      "ep:  800 loss: 0.2794 acc: 75.26\n",
      "ep:  900 loss: 0.2563 acc: 76.29\n",
      "ep: 1000 loss: 0.2382 acc: 79.38\n",
      "ep: 1100 loss: 0.2180 acc: 79.90\n",
      "ep: 1200 loss: 0.2029 acc: 81.44\n",
      "ep: 1300 loss: 0.1912 acc: 82.99\n",
      "ep: 1400 loss: 0.1728 acc: 84.54\n",
      "ep: 1500 loss: 0.1599 acc: 85.57\n",
      "ep: 1600 loss: 0.1491 acc: 86.60\n",
      "ep: 1700 loss: 0.1392 acc: 86.08\n",
      "ep: 1800 loss: 0.1305 acc: 86.60\n",
      "ep: 1900 loss: 0.1210 acc: 86.60\n",
      "ep: 2000 loss: 0.1126 acc: 87.11\n",
      "ep: 2100 loss: 0.1055 acc: 87.11\n",
      "ep: 2200 loss: 0.0993 acc: 87.63\n",
      "ep: 2300 loss: 0.0938 acc: 87.63\n",
      "ep: 2400 loss: 0.0889 acc: 87.11\n",
      "ep: 2500 loss: 0.0845 acc: 87.11\n",
      "ep: 2600 loss: 0.0806 acc: 88.14\n",
      "ep: 2700 loss: 0.0772 acc: 88.14\n",
      "ep: 2800 loss: 0.0744 acc: 88.66\n",
      "ep: 2900 loss: 0.0717 acc: 88.66\n",
      "ep: 3000 loss: 0.0694 acc: 88.66\n",
      "ep: 3100 loss: 0.0671 acc: 88.14\n",
      "ep: 3200 loss: 0.0648 acc: 88.14\n",
      "ep: 3300 loss: 0.0623 acc: 89.18\n",
      "ep: 3400 loss: 0.0598 acc: 88.66\n",
      "ep: 3500 loss: 0.0565 acc: 89.69\n",
      "ep: 3600 loss: 0.0523 acc: 89.69\n",
      "ep: 3700 loss: 0.0481 acc: 90.72\n",
      "ep: 3800 loss: 0.0447 acc: 91.75\n",
      "ep: 3900 loss: 0.0409 acc: 91.75\n",
      "ep: 4000 loss: 0.0378 acc: 91.24\n",
      "ep: 4100 loss: 0.0355 acc: 90.72\n",
      "ep: 4200 loss: 0.0337 acc: 90.72\n",
      "ep: 4300 loss: 0.0323 acc: 90.72\n",
      "ep: 4400 loss: 0.0310 acc: 91.24\n",
      "ep: 4500 loss: 0.0298 acc: 91.24\n",
      "ep: 4600 loss: 0.0279 acc: 91.24\n",
      "ep: 4700 loss: 0.0253 acc: 92.78\n",
      "ep: 4800 loss: 0.0236 acc: 92.78\n",
      "ep: 4900 loss: 0.0226 acc: 92.78\n",
      "ep: 5000 loss: 0.0218 acc: 93.30\n",
      "ep: 5100 loss: 0.0211 acc: 93.30\n",
      "ep: 5200 loss: 0.0205 acc: 92.78\n",
      "ep: 5300 loss: 0.0200 acc: 93.30\n",
      "ep: 5400 loss: 0.0196 acc: 93.81\n",
      "ep: 5500 loss: 0.0201 acc: 93.81\n",
      "ep: 5600 loss: 0.0197 acc: 94.85\n",
      "ep: 5700 loss: 0.0194 acc: 94.85\n",
      "ep: 5800 loss: 0.0187 acc: 94.85\n",
      "ep: 5900 loss: 0.0186 acc: 94.85\n",
      "ep: 6000 loss: 0.0181 acc: 94.85\n",
      "ep: 6100 loss: 0.0174 acc: 94.85\n",
      "ep: 6200 loss: 0.0171 acc: 94.85\n",
      "ep: 6300 loss: 0.0170 acc: 94.33\n",
      "ep: 6400 loss: 0.0165 acc: 94.85\n",
      "ep: 6500 loss: 0.0164 acc: 95.36\n",
      "ep: 6600 loss: 0.0163 acc: 94.33\n",
      "init:1.4000000000000001, acc:95.36082458496094 , epoch:6623\n",
      "ep:  100 loss: 0.6126 acc: 58.76\n",
      "ep:  200 loss: 0.5012 acc: 62.89\n",
      "ep:  300 loss: 0.4470 acc: 65.46\n",
      "ep:  400 loss: 0.4115 acc: 70.10\n",
      "ep:  500 loss: 0.3864 acc: 71.13\n",
      "ep:  600 loss: 0.3695 acc: 73.20\n",
      "ep:  700 loss: 0.3582 acc: 72.68\n",
      "ep:  800 loss: 0.3478 acc: 74.74\n",
      "ep:  900 loss: 0.3410 acc: 74.74\n",
      "ep: 1000 loss: 0.3347 acc: 76.29\n",
      "ep: 1100 loss: 0.3248 acc: 76.80\n",
      "ep: 1200 loss: 0.3140 acc: 77.32\n",
      "ep: 1300 loss: 0.3046 acc: 77.32\n",
      "ep: 1400 loss: 0.2972 acc: 77.84\n",
      "ep: 1500 loss: 0.2913 acc: 78.35\n",
      "ep: 1600 loss: 0.2860 acc: 79.38\n",
      "ep: 1700 loss: 0.2810 acc: 80.93\n",
      "ep: 1800 loss: 0.2761 acc: 79.38\n",
      "ep: 1900 loss: 0.2682 acc: 77.84\n",
      "ep: 2000 loss: 0.2602 acc: 77.32\n",
      "ep: 2100 loss: 0.2532 acc: 77.32\n",
      "ep: 2200 loss: 0.2468 acc: 78.35\n",
      "ep: 2300 loss: 0.2408 acc: 78.35\n",
      "ep: 2400 loss: 0.2361 acc: 79.38\n",
      "ep: 2500 loss: 0.2312 acc: 79.38\n",
      "ep: 2600 loss: 0.2262 acc: 81.96\n",
      "ep: 2700 loss: 0.2217 acc: 82.47\n",
      "ep: 2800 loss: 0.2178 acc: 82.47\n",
      "ep: 2900 loss: 0.2144 acc: 82.47\n",
      "ep: 3000 loss: 0.2114 acc: 82.99\n",
      "ep: 3100 loss: 0.2088 acc: 82.99\n",
      "ep: 3200 loss: 0.2064 acc: 82.99\n",
      "ep: 3300 loss: 0.2042 acc: 82.99\n",
      "ep: 3400 loss: 0.2022 acc: 83.51\n",
      "ep: 3500 loss: 0.2004 acc: 84.02\n",
      "ep: 3600 loss: 0.1986 acc: 84.02\n",
      "ep: 3700 loss: 0.1967 acc: 83.51\n",
      "ep: 3800 loss: 0.1941 acc: 83.51\n",
      "ep: 3900 loss: 0.1920 acc: 83.51\n",
      "ep: 4000 loss: 0.1902 acc: 84.02\n",
      "ep: 4100 loss: 0.1888 acc: 84.02\n",
      "ep: 4200 loss: 0.1875 acc: 84.02\n",
      "ep: 4300 loss: 0.1863 acc: 83.51\n",
      "ep: 4400 loss: 0.1851 acc: 83.51\n",
      "ep: 4500 loss: 0.1840 acc: 83.51\n",
      "ep: 4600 loss: 0.1829 acc: 83.51\n",
      "ep: 4700 loss: 0.1818 acc: 83.51\n",
      "ep: 4800 loss: 0.1806 acc: 83.51\n",
      "ep: 4900 loss: 0.1795 acc: 83.51\n",
      "ep: 5000 loss: 0.1783 acc: 83.51\n",
      "ep: 5100 loss: 0.1771 acc: 83.51\n",
      "ep: 5200 loss: 0.1758 acc: 84.02\n",
      "ep: 5300 loss: 0.1745 acc: 84.02\n",
      "ep: 5400 loss: 0.1731 acc: 84.54\n",
      "ep: 5500 loss: 0.1716 acc: 84.54\n",
      "ep: 5600 loss: 0.1701 acc: 84.02\n",
      "ep: 5700 loss: 0.1685 acc: 83.51\n",
      "ep: 5800 loss: 0.1668 acc: 84.02\n",
      "ep: 5900 loss: 0.1650 acc: 83.51\n",
      "ep: 6000 loss: 0.1629 acc: 83.51\n",
      "ep: 6100 loss: 0.1604 acc: 83.51\n",
      "ep: 6200 loss: 0.1565 acc: 83.51\n",
      "ep: 6300 loss: 0.1515 acc: 83.51\n",
      "ep: 6400 loss: 0.1462 acc: 84.02\n",
      "ep: 6500 loss: 0.1371 acc: 84.02\n",
      "ep: 6600 loss: 0.1308 acc: 84.02\n",
      "init:1.5000000000000002, acc:84.02062225341797 , epoch:6622\n",
      "ep:  100 loss: 0.4768 acc: 65.46\n",
      "ep:  200 loss: 0.3999 acc: 69.07\n",
      "ep:  300 loss: 0.3585 acc: 70.10\n",
      "ep:  400 loss: 0.3204 acc: 71.65\n",
      "ep:  500 loss: 0.2974 acc: 72.68\n",
      "ep:  600 loss: 0.2818 acc: 74.74\n",
      "ep:  700 loss: 0.2675 acc: 73.71\n",
      "ep:  800 loss: 0.2549 acc: 75.77\n",
      "ep:  900 loss: 0.2446 acc: 76.29\n",
      "ep: 1000 loss: 0.2354 acc: 76.80\n",
      "ep: 1100 loss: 0.2272 acc: 76.80\n",
      "ep: 1200 loss: 0.2201 acc: 77.32\n",
      "ep: 1300 loss: 0.2138 acc: 76.80\n",
      "ep: 1400 loss: 0.2081 acc: 78.87\n",
      "ep: 1500 loss: 0.2031 acc: 79.90\n",
      "ep: 1600 loss: 0.1994 acc: 79.38\n",
      "ep: 1700 loss: 0.1954 acc: 79.38\n",
      "ep: 1800 loss: 0.1909 acc: 78.87\n",
      "ep: 1900 loss: 0.1870 acc: 80.41\n",
      "ep: 2000 loss: 0.1829 acc: 80.41\n",
      "ep: 2100 loss: 0.1791 acc: 80.41\n",
      "ep: 2200 loss: 0.1749 acc: 80.41\n",
      "ep: 2300 loss: 0.1718 acc: 79.90\n",
      "ep: 2400 loss: 0.1695 acc: 80.41\n",
      "ep: 2500 loss: 0.1676 acc: 80.93\n",
      "ep: 2600 loss: 0.1658 acc: 80.93\n",
      "ep: 2700 loss: 0.1639 acc: 80.93\n",
      "ep: 2800 loss: 0.1623 acc: 80.93\n",
      "ep: 2900 loss: 0.1612 acc: 81.44\n",
      "ep: 3000 loss: 0.1602 acc: 81.96\n",
      "ep: 3100 loss: 0.1587 acc: 81.96\n",
      "ep: 3200 loss: 0.1564 acc: 81.96\n",
      "ep: 3300 loss: 0.1549 acc: 81.44\n",
      "ep: 3400 loss: 0.1538 acc: 81.44\n",
      "ep: 3500 loss: 0.1532 acc: 81.96\n",
      "ep: 3600 loss: 0.1530 acc: 81.96\n",
      "ep: 3700 loss: 0.1526 acc: 82.47\n",
      "ep: 3800 loss: 0.1522 acc: 81.96\n",
      "ep: 3900 loss: 0.1517 acc: 81.96\n",
      "ep: 4000 loss: 0.1513 acc: 81.96\n",
      "ep: 4100 loss: 0.1510 acc: 81.96\n",
      "ep: 4200 loss: 0.1507 acc: 81.96\n",
      "ep: 4300 loss: 0.1504 acc: 81.96\n",
      "ep: 4400 loss: 0.1502 acc: 81.96\n",
      "ep: 4500 loss: 0.1500 acc: 81.96\n",
      "ep: 4600 loss: 0.1498 acc: 81.96\n",
      "ep: 4700 loss: 0.1497 acc: 82.47\n",
      "ep: 4800 loss: 0.1496 acc: 82.47\n",
      "ep: 4900 loss: 0.1495 acc: 82.47\n",
      "ep: 5000 loss: 0.1494 acc: 82.47\n",
      "ep: 5100 loss: 0.1494 acc: 82.47\n",
      "ep: 5200 loss: 0.1493 acc: 82.47\n",
      "ep: 5300 loss: 0.1493 acc: 82.47\n",
      "ep: 5400 loss: 0.1492 acc: 82.47\n",
      "ep: 5500 loss: 0.1491 acc: 81.96\n",
      "ep: 5600 loss: 0.1489 acc: 81.96\n",
      "ep: 5700 loss: 0.1488 acc: 81.44\n",
      "ep: 5800 loss: 0.1485 acc: 81.44\n",
      "ep: 5900 loss: 0.1483 acc: 81.96\n",
      "ep: 6000 loss: 0.1479 acc: 81.96\n",
      "ep: 6100 loss: 0.1474 acc: 81.96\n",
      "ep: 6200 loss: 0.1467 acc: 82.47\n",
      "ep: 6300 loss: 0.1455 acc: 82.47\n",
      "ep: 6400 loss: 0.1437 acc: 82.99\n",
      "ep: 6500 loss: 0.1411 acc: 82.99\n",
      "ep: 6600 loss: 0.1382 acc: 83.51\n",
      "init:1.6, acc:83.50515747070312 , epoch:6621\n",
      "ep:  100 loss: 0.5489 acc: 64.95\n",
      "ep:  200 loss: 0.5062 acc: 66.49\n",
      "ep:  300 loss: 0.4623 acc: 65.46\n",
      "ep:  400 loss: 0.4215 acc: 68.04\n",
      "ep:  500 loss: 0.3990 acc: 68.56\n",
      "ep:  600 loss: 0.3729 acc: 70.62\n",
      "ep:  700 loss: 0.3464 acc: 71.13\n",
      "ep:  800 loss: 0.3166 acc: 68.56\n",
      "ep:  900 loss: 0.2959 acc: 66.49\n",
      "ep: 1000 loss: 0.2811 acc: 69.59\n",
      "ep: 1100 loss: 0.2635 acc: 72.68\n",
      "ep: 1200 loss: 0.2552 acc: 71.65\n",
      "ep: 1300 loss: 0.2490 acc: 72.16\n",
      "ep: 1400 loss: 0.2437 acc: 72.16\n",
      "ep: 1500 loss: 0.2382 acc: 72.68\n",
      "ep: 1600 loss: 0.2332 acc: 74.23\n",
      "ep: 1700 loss: 0.2285 acc: 74.23\n",
      "ep: 1800 loss: 0.2224 acc: 74.23\n",
      "ep: 1900 loss: 0.2150 acc: 76.80\n",
      "ep: 2000 loss: 0.2102 acc: 77.32\n",
      "ep: 2100 loss: 0.2052 acc: 76.80\n",
      "ep: 2200 loss: 0.1997 acc: 76.80\n",
      "ep: 2300 loss: 0.1933 acc: 78.87\n",
      "ep: 2400 loss: 0.1864 acc: 79.90\n",
      "ep: 2500 loss: 0.1801 acc: 79.38\n",
      "ep: 2600 loss: 0.1745 acc: 79.90\n",
      "ep: 2700 loss: 0.1691 acc: 80.41\n",
      "ep: 2800 loss: 0.1633 acc: 80.93\n",
      "ep: 2900 loss: 0.1582 acc: 80.41\n",
      "ep: 3000 loss: 0.1544 acc: 81.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 3100 loss: 0.1511 acc: 82.47\n",
      "ep: 3200 loss: 0.1479 acc: 81.96\n",
      "ep: 3300 loss: 0.1460 acc: 83.51\n",
      "ep: 3400 loss: 0.1438 acc: 82.47\n",
      "ep: 3500 loss: 0.1420 acc: 82.99\n",
      "ep: 3600 loss: 0.1374 acc: 86.08\n",
      "ep: 3700 loss: 0.1346 acc: 86.08\n",
      "ep: 3800 loss: 0.1301 acc: 86.08\n",
      "ep: 3900 loss: 0.1259 acc: 85.05\n",
      "ep: 4000 loss: 0.1227 acc: 85.57\n",
      "ep: 4100 loss: 0.1201 acc: 85.05\n",
      "ep: 4200 loss: 0.1180 acc: 85.05\n",
      "ep: 4300 loss: 0.1161 acc: 84.54\n",
      "ep: 4400 loss: 0.1145 acc: 85.05\n",
      "ep: 4500 loss: 0.1131 acc: 85.05\n",
      "ep: 4600 loss: 0.1119 acc: 85.05\n",
      "ep: 4700 loss: 0.1108 acc: 85.57\n",
      "ep: 4800 loss: 0.1098 acc: 85.57\n",
      "ep: 4900 loss: 0.1089 acc: 85.57\n",
      "ep: 5000 loss: 0.1080 acc: 85.57\n",
      "ep: 5100 loss: 0.1072 acc: 85.57\n",
      "ep: 5200 loss: 0.1064 acc: 85.05\n",
      "ep: 5300 loss: 0.1057 acc: 85.57\n",
      "ep: 5400 loss: 0.1051 acc: 85.57\n",
      "ep: 5500 loss: 0.1045 acc: 85.57\n",
      "ep: 5600 loss: 0.1040 acc: 85.57\n",
      "ep: 5700 loss: 0.1035 acc: 85.57\n",
      "ep: 5800 loss: 0.1031 acc: 86.08\n",
      "ep: 5900 loss: 0.1026 acc: 86.08\n",
      "ep: 6000 loss: 0.1022 acc: 86.60\n",
      "ep: 6100 loss: 0.1018 acc: 86.08\n",
      "ep: 6200 loss: 0.1013 acc: 86.08\n",
      "ep: 6300 loss: 0.1009 acc: 86.08\n",
      "ep: 6400 loss: 0.1004 acc: 86.08\n",
      "ep: 6500 loss: 0.0998 acc: 86.08\n",
      "ep: 6600 loss: 0.0992 acc: 86.08\n",
      "init:1.7000000000000002, acc:86.08247375488281 , epoch:6620\n",
      "ep:  100 loss: 0.5660 acc: 60.31\n",
      "ep:  200 loss: 0.5004 acc: 62.37\n",
      "ep:  300 loss: 0.4696 acc: 63.92\n",
      "ep:  400 loss: 0.4345 acc: 62.89\n",
      "ep:  500 loss: 0.3955 acc: 65.98\n",
      "ep:  600 loss: 0.3621 acc: 68.56\n",
      "ep:  700 loss: 0.3417 acc: 69.59\n",
      "ep:  800 loss: 0.3258 acc: 71.13\n",
      "ep:  900 loss: 0.3130 acc: 71.13\n",
      "ep: 1000 loss: 0.3022 acc: 71.65\n",
      "ep: 1100 loss: 0.2926 acc: 72.16\n",
      "ep: 1200 loss: 0.2834 acc: 72.68\n",
      "ep: 1300 loss: 0.2739 acc: 72.16\n",
      "ep: 1400 loss: 0.2633 acc: 72.68\n",
      "ep: 1500 loss: 0.2531 acc: 72.68\n",
      "ep: 1600 loss: 0.2441 acc: 73.20\n",
      "ep: 1700 loss: 0.2358 acc: 74.74\n",
      "ep: 1800 loss: 0.2275 acc: 74.74\n",
      "ep: 1900 loss: 0.2191 acc: 74.74\n",
      "ep: 2000 loss: 0.2114 acc: 74.74\n",
      "ep: 2100 loss: 0.2051 acc: 74.74\n",
      "ep: 2200 loss: 0.1989 acc: 75.77\n",
      "ep: 2300 loss: 0.1880 acc: 76.29\n",
      "ep: 2400 loss: 0.1800 acc: 77.32\n",
      "ep: 2500 loss: 0.1754 acc: 77.84\n",
      "ep: 2600 loss: 0.1722 acc: 77.84\n",
      "ep: 2700 loss: 0.1696 acc: 77.32\n",
      "ep: 2800 loss: 0.1675 acc: 77.32\n",
      "ep: 2900 loss: 0.1656 acc: 77.32\n",
      "ep: 3000 loss: 0.1639 acc: 77.32\n",
      "ep: 3100 loss: 0.1624 acc: 77.32\n",
      "ep: 3200 loss: 0.1610 acc: 77.32\n",
      "ep: 3300 loss: 0.1594 acc: 77.32\n",
      "ep: 3400 loss: 0.1574 acc: 76.80\n",
      "ep: 3500 loss: 0.1554 acc: 76.80\n",
      "ep: 3600 loss: 0.1536 acc: 76.80\n",
      "ep: 3700 loss: 0.1521 acc: 76.80\n",
      "ep: 3800 loss: 0.1508 acc: 77.32\n",
      "ep: 3900 loss: 0.1502 acc: 77.32\n",
      "ep: 4000 loss: 0.1499 acc: 75.77\n",
      "ep: 4100 loss: 0.1491 acc: 76.80\n",
      "ep: 4200 loss: 0.1482 acc: 76.80\n",
      "ep: 4300 loss: 0.1471 acc: 76.80\n",
      "ep: 4400 loss: 0.1460 acc: 76.80\n",
      "ep: 4500 loss: 0.1449 acc: 76.80\n",
      "ep: 4600 loss: 0.1438 acc: 76.80\n",
      "ep: 4700 loss: 0.1428 acc: 77.32\n",
      "ep: 4800 loss: 0.1418 acc: 77.32\n",
      "ep: 4900 loss: 0.1409 acc: 77.32\n",
      "ep: 5000 loss: 0.1400 acc: 77.84\n",
      "ep: 5100 loss: 0.1391 acc: 78.35\n",
      "ep: 5200 loss: 0.1384 acc: 78.87\n",
      "ep: 5300 loss: 0.1376 acc: 78.87\n",
      "ep: 5400 loss: 0.1370 acc: 78.87\n",
      "ep: 5500 loss: 0.1364 acc: 78.87\n",
      "ep: 5600 loss: 0.1358 acc: 78.87\n",
      "ep: 5700 loss: 0.1353 acc: 78.87\n",
      "ep: 5800 loss: 0.1348 acc: 78.87\n",
      "ep: 5900 loss: 0.1343 acc: 78.87\n",
      "ep: 6000 loss: 0.1339 acc: 78.87\n",
      "ep: 6100 loss: 0.1335 acc: 78.87\n",
      "ep: 6200 loss: 0.1331 acc: 78.87\n",
      "ep: 6300 loss: 0.1328 acc: 78.87\n",
      "ep: 6400 loss: 0.1325 acc: 78.87\n",
      "ep: 6500 loss: 0.1323 acc: 78.87\n",
      "ep: 6600 loss: 0.1320 acc: 78.87\n",
      "init:1.8000000000000003, acc:78.86598205566406 , epoch:6619\n",
      "ep:  100 loss: 0.5019 acc: 67.01\n",
      "ep:  200 loss: 0.4466 acc: 67.01\n",
      "ep:  300 loss: 0.4256 acc: 67.53\n",
      "ep:  400 loss: 0.4000 acc: 69.07\n",
      "ep:  500 loss: 0.3765 acc: 72.16\n",
      "ep:  600 loss: 0.3591 acc: 73.20\n",
      "ep:  700 loss: 0.3438 acc: 74.74\n",
      "ep:  800 loss: 0.3302 acc: 74.23\n",
      "ep:  900 loss: 0.3176 acc: 73.20\n",
      "ep: 1000 loss: 0.3058 acc: 73.71\n",
      "ep: 1100 loss: 0.2966 acc: 73.71\n",
      "ep: 1200 loss: 0.2892 acc: 74.74\n",
      "ep: 1300 loss: 0.2828 acc: 74.23\n",
      "ep: 1400 loss: 0.2769 acc: 73.71\n",
      "ep: 1500 loss: 0.2719 acc: 73.71\n",
      "ep: 1600 loss: 0.2681 acc: 74.74\n",
      "ep: 1700 loss: 0.2639 acc: 75.77\n",
      "ep: 1800 loss: 0.2593 acc: 75.26\n",
      "ep: 1900 loss: 0.2553 acc: 74.74\n",
      "ep: 2000 loss: 0.2517 acc: 75.77\n",
      "ep: 2100 loss: 0.2486 acc: 75.77\n",
      "ep: 2200 loss: 0.2455 acc: 77.32\n",
      "ep: 2300 loss: 0.2425 acc: 76.29\n",
      "ep: 2400 loss: 0.2394 acc: 77.32\n",
      "ep: 2500 loss: 0.2365 acc: 77.84\n",
      "ep: 2600 loss: 0.2339 acc: 78.87\n",
      "ep: 2700 loss: 0.2315 acc: 78.35\n",
      "ep: 2800 loss: 0.2287 acc: 78.35\n",
      "ep: 2900 loss: 0.2261 acc: 79.90\n",
      "ep: 3000 loss: 0.2238 acc: 80.93\n",
      "ep: 3100 loss: 0.2217 acc: 80.93\n",
      "ep: 3200 loss: 0.2200 acc: 80.93\n",
      "ep: 3300 loss: 0.2185 acc: 80.93\n",
      "ep: 3400 loss: 0.2172 acc: 80.93\n",
      "ep: 3500 loss: 0.2162 acc: 80.41\n",
      "ep: 3600 loss: 0.2155 acc: 80.93\n",
      "ep: 3700 loss: 0.2152 acc: 81.96\n",
      "ep: 3800 loss: 0.2156 acc: 81.96\n",
      "ep: 3900 loss: 0.2171 acc: 80.41\n",
      "ep: 4000 loss: 0.2155 acc: 81.96\n",
      "ep: 4100 loss: 0.2137 acc: 81.96\n",
      "ep: 4200 loss: 0.2120 acc: 84.02\n",
      "ep: 4300 loss: 0.2107 acc: 84.02\n",
      "ep: 4400 loss: 0.2102 acc: 84.02\n",
      "ep: 4500 loss: 0.2094 acc: 84.02\n",
      "ep: 4600 loss: 0.2084 acc: 84.02\n",
      "ep: 4700 loss: 0.2075 acc: 84.02\n",
      "ep: 4800 loss: 0.2064 acc: 84.02\n",
      "ep: 4900 loss: 0.2054 acc: 83.51\n",
      "ep: 5000 loss: 0.2046 acc: 83.51\n",
      "ep: 5100 loss: 0.2040 acc: 83.51\n",
      "ep: 5200 loss: 0.2035 acc: 83.51\n",
      "ep: 5300 loss: 0.2029 acc: 83.51\n",
      "ep: 5400 loss: 0.2025 acc: 83.51\n",
      "ep: 5500 loss: 0.2020 acc: 83.51\n",
      "ep: 5600 loss: 0.2016 acc: 83.51\n",
      "ep: 5700 loss: 0.2013 acc: 82.99\n",
      "ep: 5800 loss: 0.2007 acc: 82.99\n",
      "ep: 5900 loss: 0.2003 acc: 82.99\n",
      "ep: 6000 loss: 0.1997 acc: 82.99\n",
      "ep: 6100 loss: 0.1992 acc: 82.99\n",
      "ep: 6200 loss: 0.1985 acc: 82.99\n",
      "ep: 6300 loss: 0.1978 acc: 82.99\n",
      "ep: 6400 loss: 0.1969 acc: 82.99\n",
      "ep: 6500 loss: 0.1960 acc: 82.99\n",
      "ep: 6600 loss: 0.1949 acc: 82.99\n",
      "init:1.9000000000000001, acc:82.98969268798828 , epoch:6618\n"
     ]
    }
   ],
   "source": [
    "num_input = data.shape[1] - 1\n",
    "\n",
    "full_input  = data[:,0:num_input]\n",
    "full_target = data[:,num_input:num_input+1]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(full_input,full_target)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset,batch_size=97)\n",
    "\n",
    "epoch = 20000\n",
    "y_epoch_converge = []\n",
    "x_init_vals = []\n",
    "z_accuracy = []\n",
    "for init in p:\n",
    "    net = RawNet(10)\n",
    "\n",
    "    if list(net.parameters()):\n",
    "        # initialize weight values\n",
    "        for m in list(net.parameters()):\n",
    "            m.data.normal_(0,init)\n",
    "\n",
    "        # use Adam optimizer\n",
    "        optimizer = torch.optim.Adam(net.parameters(),eps=0.000001,lr=.01,\n",
    "                                     betas=(0.9,0.999),weight_decay=0.0001)\n",
    "        # training loop\n",
    "        for epoch in range(1, epoch):\n",
    "            accuracy = train(net, train_loader, optimizer)\n",
    "            if accuracy == 100:\n",
    "                print(\"Achieves 100%\")\n",
    "                break\n",
    "\n",
    "    y_epoch_converge.append(epoch)\n",
    "    x_init_vals.append(init)\n",
    "    z_accuracy.append(accuracy)\n",
    "    print(f\"init:{init}, acc:{accuracy} , epoch:{epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6683, 6682, 6681, 6680, 6679, 6678, 6677, 6676, 6675]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_epoch_converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_init_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'epoch')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_init_vals,y_epoch_converge)\n",
    "plt.xlabel('Initial weights')\n",
    "plt.ylabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.active1[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97])\n",
      "torch.Size([97])\n",
      "1875300\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(net.active1[:,0].shape)\n",
    "    net.eval() \n",
    "    print(net.active1[:,0].shape)\n",
    "    output = net(grid)\n",
    "    print(net.active1[:,0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # suppress updating of gradients\n",
    "    # plot function computed by model\n",
    "    net.eval()\n",
    "    net(grid)\n",
    "    hid = net.hidlayer[0]\n",
    "    pred = (hid[:,0].view(hid.shape[0],1) >= 0.5).float()\n",
    "    plt.clf()\n",
    "    plt.pcolormesh(xrange,yrange,pred.cpu().view(yrange.size()[0],xrange.size()[0]), cmap='inferno')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1875300, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6938,  0.6930,  0.6921,  ..., -0.6984, -0.6992, -0.6999])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.active1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1875300, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " with torch.no_grad(): # suppress updating of gradients\n",
    "    net.eval()        # toggle batch norm, dropout\n",
    "    output = net(grid)\n",
    "    net.train() # toggle batch norm, dropout back again\n",
    "\n",
    "    pred = (output >= 0.5).float()\n",
    "    \n",
    "pred.shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0000, -6.9900, -6.9800,  ...,  7.0700,  7.0800,  7.0900])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x1bfe46c6048>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.pcolormesh(xrange,yrange,pred.cpu().view(yrange.size()[0],xrange.size()[0]), cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1330, 1410]' is invalid for input of size 937650000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-47be79e5d057>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myrange\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxrange\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1330, 1410]' is invalid for input of size 937650000"
     ]
    }
   ],
   "source": [
    "net.active1.cpu().view(yrange.size()[0],xrange.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.active1[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.active1[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (net.active1[:,0] >= 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolarNet(torch.nn.Module):\n",
    "    def __init__(self, num_hid):\n",
    "        super(PolarNet, self).__init__()\n",
    "        self.feed = nn.Linear(2,num_hid) \n",
    "        self.hid = nn.Linear(num_hid,1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input[:,0]\n",
    "        y = input[:,1]\n",
    "        out = torch.zeros(input.shape,dtype=torch.float32)\n",
    "        out[:,0] = torch.sqrt(x*x + y*y) #tranform data\n",
    "        out[:,1] = torch.atan2(y,x)\n",
    "        #here we start the forward pass\n",
    "        self.active1 = torch.tanh(self.feed(out))\n",
    "        self.active2 = torch.tanh(self.hid(self.active1))\n",
    "        self.hidlayer = [self.active1]\n",
    "        return F.sigmoid(self.active2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:  100 loss: 0.6552 acc: 62.37\n",
      "ep:  200 loss: 0.5080 acc: 66.49\n",
      "ep:  300 loss: 0.4440 acc: 67.53\n",
      "ep:  400 loss: 0.3876 acc: 76.29\n",
      "ep:  500 loss: 0.3642 acc: 76.29\n",
      "ep:  600 loss: 0.3554 acc: 75.77\n",
      "ep:  700 loss: 0.3514 acc: 75.77\n",
      "ep:  800 loss: 0.3494 acc: 75.26\n",
      "ep:  900 loss: 0.3483 acc: 75.26\n",
      "ep: 1000 loss: 0.3480 acc: 75.26\n",
      "ep: 1100 loss: 0.3476 acc: 75.26\n",
      "ep: 1200 loss: 0.3471 acc: 75.26\n",
      "ep: 1300 loss: 0.3464 acc: 75.77\n",
      "ep: 1400 loss: 0.3481 acc: 77.84\n",
      "ep: 1500 loss: 0.3387 acc: 82.99\n",
      "ep: 1600 loss: 0.3363 acc: 90.21\n",
      "ep: 1700 loss: 0.3340 acc: 90.72\n",
      "ep: 1800 loss: 0.3325 acc: 91.24\n",
      "ep: 1900 loss: 0.3319 acc: 98.45\n",
      "ep: 2000 loss: 0.3308 acc: 99.48\n",
      "ep: 2100 loss: 0.3294 acc: 99.48\n",
      "ep: 2200 loss: 0.3283 acc: 99.48\n",
      "ep: 2300 loss: 0.3277 acc: 99.48\n",
      "ep: 2400 loss: 0.3273 acc: 99.48\n",
      "ep: 2500 loss: 0.3270 acc: 99.48\n",
      "ep: 2600 loss: 0.3267 acc: 99.48\n",
      "ep: 2700 loss: 0.3266 acc: 99.48\n",
      "ep: 2800 loss: 0.3265 acc: 99.48\n",
      "ep: 2900 loss: 0.3264 acc: 99.48\n",
      "ep: 3000 loss: 0.3263 acc: 99.48\n",
      "ep: 3100 loss: 0.3262 acc: 99.48\n",
      "ep: 3200 loss: 0.3262 acc: 99.48\n",
      "ep: 3300 loss: 0.3261 acc: 99.48\n",
      "ep: 3400 loss: 0.3261 acc: 99.48\n",
      "ep: 3500 loss: 0.3261 acc: 99.48\n",
      "ep: 3600 loss: 0.3260 acc: 99.48\n",
      "ep: 3700 loss: 0.3260 acc: 99.48\n",
      "ep: 3800 loss: 0.3260 acc: 99.48\n",
      "ep: 3900 loss: 0.3260 acc: 99.48\n",
      "ep: 4000 loss: 0.3259 acc: 99.48\n",
      "ep: 4100 loss: 0.3259 acc: 99.48\n",
      "ep: 4200 loss: 0.3259 acc: 99.48\n",
      "ep: 4300 loss: 0.3259 acc: 99.48\n",
      "ep: 4400 loss: 0.3259 acc: 99.48\n",
      "ep: 4500 loss: 0.3259 acc: 99.48\n",
      "ep: 4600 loss: 0.3259 acc: 99.48\n",
      "ep: 4700 loss: 0.3259 acc: 99.48\n",
      "ep: 4800 loss: 0.3259 acc: 99.48\n",
      "ep: 4900 loss: 0.3259 acc: 99.48\n",
      "ep: 5000 loss: 0.3259 acc: 99.48\n",
      "ep: 5100 loss: 0.3259 acc: 99.48\n",
      "ep: 5200 loss: 0.3259 acc: 99.48\n",
      "ep: 5300 loss: 0.3259 acc: 99.48\n",
      "ep: 5400 loss: 0.3259 acc: 99.48\n",
      "ep: 5500 loss: 0.3259 acc: 99.48\n",
      "ep: 5600 loss: 0.3259 acc: 99.48\n",
      "ep: 5700 loss: 0.3259 acc: 99.48\n",
      "ep: 5800 loss: 0.3259 acc: 99.48\n",
      "ep: 5900 loss: 0.3259 acc: 99.48\n",
      "ep: 6000 loss: 0.3259 acc: 99.48\n",
      "ep: 6100 loss: 0.3259 acc: 99.48\n",
      "ep: 6200 loss: 0.3259 acc: 99.48\n",
      "ep: 6300 loss: 0.3259 acc: 99.48\n",
      "ep: 6400 loss: 0.3259 acc: 99.48\n",
      "ep: 6500 loss: 0.3259 acc: 99.48\n",
      "ep: 6600 loss: 0.3259 acc: 99.48\n",
      "ep: 6700 loss: 0.3259 acc: 99.48\n",
      "ep: 6800 loss: 0.3259 acc: 99.48\n",
      "ep: 6900 loss: 0.3259 acc: 99.48\n",
      "ep: 7000 loss: 0.3259 acc: 99.48\n",
      "ep: 7100 loss: 0.3259 acc: 99.48\n",
      "ep: 7200 loss: 0.3259 acc: 99.48\n",
      "ep: 7300 loss: 0.3259 acc: 99.48\n",
      "ep: 7400 loss: 0.3259 acc: 99.48\n",
      "ep: 7500 loss: 0.3259 acc: 99.48\n",
      "ep: 7600 loss: 0.3259 acc: 99.48\n",
      "ep: 7700 loss: 0.3261 acc: 99.48\n",
      "ep: 7800 loss: 0.3259 acc: 99.48\n",
      "ep: 7900 loss: 0.3259 acc: 99.48\n",
      "ep: 8000 loss: 0.3259 acc: 99.48\n",
      "ep: 8100 loss: 0.3259 acc: 99.48\n",
      "ep: 8200 loss: 0.3259 acc: 99.48\n",
      "ep: 8300 loss: 0.3259 acc: 99.48\n",
      "ep: 8400 loss: 0.3258 acc: 99.48\n",
      "ep: 8500 loss: 0.3259 acc: 99.48\n",
      "ep: 8600 loss: 0.3259 acc: 99.48\n",
      "ep: 8700 loss: 0.3259 acc: 99.48\n",
      "ep: 8800 loss: 0.3259 acc: 99.48\n",
      "ep: 8900 loss: 0.3259 acc: 99.48\n",
      "ep: 9000 loss: 0.3259 acc: 99.48\n",
      "ep: 9100 loss: 0.3259 acc: 99.48\n",
      "ep: 9200 loss: 0.3259 acc: 99.48\n",
      "ep: 9300 loss: 0.3259 acc: 99.48\n",
      "ep: 9400 loss: 0.3259 acc: 99.48\n",
      "ep: 9500 loss: 0.3259 acc: 99.48\n",
      "ep: 9600 loss: 0.3259 acc: 99.48\n",
      "ep: 9700 loss: 0.3259 acc: 99.48\n",
      "ep: 9800 loss: 0.3259 acc: 99.48\n",
      "ep: 9900 loss: 0.3259 acc: 99.48\n",
      "ep:10000 loss: 0.3259 acc: 99.48\n",
      "ep:10100 loss: 0.3259 acc: 99.48\n",
      "ep:10200 loss: 0.3259 acc: 99.48\n",
      "ep:10300 loss: 0.3259 acc: 99.48\n",
      "ep:10400 loss: 0.3259 acc: 99.48\n",
      "ep:10500 loss: 0.3259 acc: 99.48\n",
      "ep:10600 loss: 0.3259 acc: 99.48\n",
      "ep:10700 loss: 0.3259 acc: 99.48\n",
      "ep:10800 loss: 0.3259 acc: 99.48\n",
      "ep:10900 loss: 0.3259 acc: 99.48\n",
      "ep:11000 loss: 0.3259 acc: 99.48\n",
      "ep:11100 loss: 0.3259 acc: 99.48\n",
      "ep:11200 loss: 0.3259 acc: 99.48\n",
      "ep:11300 loss: 0.3259 acc: 99.48\n",
      "ep:11400 loss: 0.3259 acc: 99.48\n",
      "ep:11500 loss: 0.3259 acc: 99.48\n",
      "ep:11600 loss: 0.3259 acc: 99.48\n",
      "ep:11700 loss: 0.3259 acc: 99.48\n",
      "ep:11800 loss: 0.3259 acc: 99.48\n",
      "ep:11900 loss: 0.3259 acc: 99.48\n",
      "ep:12000 loss: 0.3259 acc: 99.48\n",
      "ep:12100 loss: 0.3259 acc: 99.48\n",
      "ep:12200 loss: 0.3259 acc: 99.48\n",
      "ep:12300 loss: 0.3259 acc: 99.48\n",
      "ep:12400 loss: 0.3259 acc: 99.48\n",
      "ep:12500 loss: 0.3259 acc: 99.48\n",
      "ep:12600 loss: 0.3259 acc: 99.48\n",
      "ep:12700 loss: 0.3258 acc: 99.48\n",
      "ep:12800 loss: 0.3259 acc: 99.48\n",
      "ep:12900 loss: 0.3259 acc: 99.48\n",
      "ep:13000 loss: 0.3259 acc: 99.48\n",
      "ep:13100 loss: 0.3259 acc: 99.48\n",
      "ep:13200 loss: 0.3259 acc: 99.48\n",
      "ep:13300 loss: 0.3260 acc: 99.48\n",
      "ep:13400 loss: 0.3259 acc: 99.48\n",
      "ep:13500 loss: 0.3259 acc: 99.48\n",
      "ep:13600 loss: 0.3259 acc: 99.48\n",
      "ep:13700 loss: 0.3259 acc: 99.48\n",
      "ep:13800 loss: 0.3259 acc: 99.48\n",
      "ep:13900 loss: 0.3259 acc: 99.48\n",
      "ep:14000 loss: 0.3258 acc: 99.48\n",
      "ep:14100 loss: 0.3259 acc: 99.48\n",
      "ep:14200 loss: 0.3259 acc: 99.48\n",
      "ep:14300 loss: 0.3259 acc: 99.48\n",
      "ep:14400 loss: 0.3259 acc: 99.48\n",
      "ep:14500 loss: 0.3259 acc: 99.48\n",
      "ep:14600 loss: 0.3259 acc: 99.48\n",
      "ep:14700 loss: 0.3258 acc: 99.48\n",
      "ep:14800 loss: 0.3259 acc: 99.48\n",
      "ep:14900 loss: 0.3259 acc: 99.48\n",
      "ep:15000 loss: 0.3259 acc: 99.48\n",
      "ep:15100 loss: 0.3259 acc: 99.48\n",
      "ep:15200 loss: 0.3259 acc: 99.48\n",
      "ep:15300 loss: 0.3258 acc: 99.48\n",
      "ep:15400 loss: 0.3259 acc: 99.48\n",
      "ep:15500 loss: 0.3259 acc: 99.48\n",
      "ep:15600 loss: 0.3259 acc: 99.48\n",
      "ep:15700 loss: 0.3259 acc: 99.48\n",
      "ep:15800 loss: 0.3259 acc: 99.48\n",
      "ep:15900 loss: 0.3258 acc: 99.48\n",
      "ep:16000 loss: 0.3259 acc: 99.48\n",
      "ep:16100 loss: 0.3259 acc: 99.48\n",
      "ep:16200 loss: 0.3259 acc: 99.48\n",
      "ep:16300 loss: 0.3258 acc: 99.48\n",
      "ep:16400 loss: 0.3259 acc: 99.48\n",
      "ep:16500 loss: 0.3259 acc: 99.48\n",
      "ep:16600 loss: 0.3259 acc: 99.48\n",
      "ep:16700 loss: 0.3259 acc: 99.48\n",
      "ep:16800 loss: 0.3259 acc: 99.48\n",
      "ep:16900 loss: 0.3259 acc: 99.48\n",
      "ep:17000 loss: 0.3259 acc: 99.48\n",
      "ep:17100 loss: 0.3259 acc: 99.48\n",
      "ep:17200 loss: 0.3259 acc: 99.48\n",
      "ep:17300 loss: 0.3259 acc: 99.48\n",
      "ep:17400 loss: 0.3259 acc: 99.48\n",
      "ep:17500 loss: 0.3259 acc: 99.48\n",
      "ep:17600 loss: 0.3259 acc: 99.48\n",
      "ep:17700 loss: 0.3259 acc: 99.48\n",
      "ep:17800 loss: 0.3259 acc: 99.48\n",
      "ep:17900 loss: 0.3259 acc: 99.48\n",
      "ep:18000 loss: 0.3259 acc: 99.48\n",
      "ep:18100 loss: 0.3259 acc: 99.48\n",
      "ep:18200 loss: 0.3259 acc: 99.48\n",
      "ep:18300 loss: 0.3259 acc: 99.48\n",
      "ep:18400 loss: 0.3259 acc: 99.48\n",
      "ep:18500 loss: 0.3259 acc: 99.48\n",
      "ep:18600 loss: 0.3259 acc: 99.48\n",
      "ep:18700 loss: 0.3259 acc: 99.48\n",
      "ep:18800 loss: 0.3259 acc: 99.48\n",
      "ep:18900 loss: 0.3259 acc: 99.48\n",
      "ep:19000 loss: 0.3259 acc: 99.48\n",
      "ep:19100 loss: 0.3259 acc: 99.48\n",
      "ep:19200 loss: 0.3259 acc: 99.48\n",
      "ep:19300 loss: 0.3260 acc: 99.48\n",
      "ep:19400 loss: 0.3259 acc: 99.48\n",
      "ep:19500 loss: 0.3259 acc: 99.48\n",
      "ep:19600 loss: 0.3259 acc: 99.48\n",
      "ep:19700 loss: 0.3259 acc: 99.48\n",
      "ep:19800 loss: 0.3259 acc: 99.48\n",
      "ep:19900 loss: 0.3259 acc: 99.48\n",
      "hidden:15, acc:99.48453521728516 , epoch:19999\n",
      "ep:  100 loss: 0.6415 acc: 61.34\n",
      "ep:  200 loss: 0.5579 acc: 66.49\n",
      "ep:  300 loss: 0.4676 acc: 71.65\n",
      "ep:  400 loss: 0.4198 acc: 75.77\n",
      "ep:  500 loss: 0.3846 acc: 76.80\n",
      "ep:  600 loss: 0.3661 acc: 76.80\n",
      "ep:  700 loss: 0.3578 acc: 76.29\n",
      "ep:  800 loss: 0.3536 acc: 76.29\n",
      "ep:  900 loss: 0.3509 acc: 76.29\n",
      "ep: 1000 loss: 0.3497 acc: 75.77\n",
      "ep: 1100 loss: 0.3489 acc: 75.26\n",
      "ep: 1200 loss: 0.3484 acc: 75.26\n",
      "ep: 1300 loss: 0.3477 acc: 75.26\n",
      "ep: 1400 loss: 0.3469 acc: 77.32\n",
      "ep: 1500 loss: 0.3463 acc: 78.35\n",
      "ep: 1600 loss: 0.3455 acc: 78.87\n",
      "ep: 1700 loss: 0.3446 acc: 78.87\n",
      "ep: 1800 loss: 0.3435 acc: 79.90\n",
      "ep: 1900 loss: 0.3420 acc: 81.44\n",
      "ep: 2000 loss: 0.3396 acc: 81.44\n",
      "ep: 2100 loss: 0.3365 acc: 81.44\n",
      "ep: 2200 loss: 0.3337 acc: 81.44\n",
      "ep: 2300 loss: 0.3344 acc: 87.11\n",
      "ep: 2400 loss: 0.3328 acc: 91.24\n",
      "ep: 2500 loss: 0.3303 acc: 99.48\n",
      "ep: 2600 loss: 0.3285 acc: 99.48\n",
      "ep: 2700 loss: 0.3275 acc: 99.48\n",
      "ep: 2800 loss: 0.3269 acc: 99.48\n",
      "ep: 2900 loss: 0.3267 acc: 99.48\n",
      "ep: 3000 loss: 0.3265 acc: 99.48\n",
      "ep: 3100 loss: 0.3264 acc: 99.48\n",
      "ep: 3200 loss: 0.3263 acc: 99.48\n",
      "ep: 3300 loss: 0.3263 acc: 99.48\n",
      "ep: 3400 loss: 0.3263 acc: 99.48\n",
      "ep: 3500 loss: 0.3262 acc: 99.48\n",
      "ep: 3600 loss: 0.3262 acc: 99.48\n",
      "ep: 3700 loss: 0.3262 acc: 99.48\n",
      "ep: 3800 loss: 0.3262 acc: 99.48\n",
      "ep: 3900 loss: 0.3261 acc: 99.48\n",
      "ep: 4000 loss: 0.3261 acc: 99.48\n",
      "ep: 4100 loss: 0.3261 acc: 99.48\n",
      "ep: 4200 loss: 0.3261 acc: 99.48\n",
      "ep: 4300 loss: 0.3261 acc: 99.48\n",
      "ep: 4400 loss: 0.3260 acc: 99.48\n",
      "ep: 4500 loss: 0.3260 acc: 99.48\n",
      "ep: 4600 loss: 0.3260 acc: 99.48\n",
      "ep: 4700 loss: 0.3260 acc: 99.48\n",
      "ep: 4800 loss: 0.3260 acc: 99.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4900 loss: 0.3260 acc: 99.48\n",
      "ep: 5000 loss: 0.3260 acc: 99.48\n",
      "ep: 5100 loss: 0.3261 acc: 99.48\n",
      "ep: 5200 loss: 0.3259 acc: 99.48\n",
      "ep: 5300 loss: 0.3259 acc: 99.48\n",
      "ep: 5400 loss: 0.3259 acc: 99.48\n",
      "ep: 5500 loss: 0.3259 acc: 99.48\n",
      "ep: 5600 loss: 0.3259 acc: 99.48\n",
      "ep: 5700 loss: 0.3259 acc: 99.48\n",
      "ep: 5800 loss: 0.3260 acc: 99.48\n",
      "ep: 5900 loss: 0.3259 acc: 99.48\n",
      "ep: 6000 loss: 0.3259 acc: 99.48\n",
      "ep: 6100 loss: 0.3259 acc: 99.48\n",
      "ep: 6200 loss: 0.3259 acc: 99.48\n",
      "ep: 6300 loss: 0.3259 acc: 99.48\n",
      "ep: 6400 loss: 0.3260 acc: 99.48\n",
      "ep: 6500 loss: 0.3259 acc: 99.48\n",
      "ep: 6600 loss: 0.3260 acc: 99.48\n",
      "ep: 6700 loss: 0.3260 acc: 99.48\n",
      "ep: 6800 loss: 0.3260 acc: 99.48\n",
      "ep: 6900 loss: 0.3260 acc: 99.48\n",
      "ep: 7000 loss: 0.3260 acc: 99.48\n",
      "ep: 7100 loss: 0.3259 acc: 99.48\n",
      "ep: 7200 loss: 0.3259 acc: 99.48\n",
      "ep: 7300 loss: 0.3259 acc: 99.48\n",
      "ep: 7400 loss: 0.3259 acc: 99.48\n",
      "ep: 7500 loss: 0.3259 acc: 99.48\n",
      "ep: 7600 loss: 0.3259 acc: 99.48\n",
      "ep: 7700 loss: 0.3259 acc: 99.48\n",
      "ep: 7800 loss: 0.3259 acc: 99.48\n",
      "ep: 7900 loss: 0.3259 acc: 99.48\n",
      "ep: 8000 loss: 0.3259 acc: 99.48\n",
      "ep: 8100 loss: 0.3259 acc: 99.48\n",
      "ep: 8200 loss: 0.3259 acc: 99.48\n",
      "ep: 8300 loss: 0.3259 acc: 99.48\n",
      "ep: 8400 loss: 0.3259 acc: 99.48\n",
      "ep: 8500 loss: 0.3259 acc: 99.48\n",
      "ep: 8600 loss: 0.3261 acc: 99.48\n",
      "ep: 8700 loss: 0.3259 acc: 99.48\n",
      "ep: 8800 loss: 0.3259 acc: 99.48\n",
      "ep: 8900 loss: 0.3259 acc: 99.48\n",
      "ep: 9000 loss: 0.3259 acc: 99.48\n",
      "ep: 9100 loss: 0.3258 acc: 99.48\n",
      "ep: 9200 loss: 0.3259 acc: 99.48\n",
      "ep: 9300 loss: 0.3259 acc: 99.48\n",
      "ep: 9400 loss: 0.3259 acc: 99.48\n",
      "ep: 9500 loss: 0.3259 acc: 99.48\n",
      "ep: 9600 loss: 0.3259 acc: 99.48\n",
      "ep: 9700 loss: 0.3259 acc: 99.48\n",
      "ep: 9800 loss: 0.3259 acc: 99.48\n",
      "ep: 9900 loss: 0.3259 acc: 99.48\n",
      "ep:10000 loss: 0.3259 acc: 99.48\n",
      "ep:10100 loss: 0.3259 acc: 99.48\n",
      "ep:10200 loss: 0.3259 acc: 99.48\n",
      "ep:10300 loss: 0.3259 acc: 99.48\n",
      "ep:10400 loss: 0.3259 acc: 99.48\n",
      "ep:10500 loss: 0.3259 acc: 99.48\n",
      "ep:10600 loss: 0.3259 acc: 99.48\n",
      "ep:10700 loss: 0.3259 acc: 99.48\n",
      "ep:10800 loss: 0.3259 acc: 99.48\n",
      "ep:10900 loss: 0.3259 acc: 99.48\n",
      "ep:11000 loss: 0.3259 acc: 99.48\n",
      "ep:11100 loss: 0.3259 acc: 99.48\n",
      "ep:11200 loss: 0.3259 acc: 99.48\n",
      "ep:11300 loss: 0.3259 acc: 99.48\n",
      "ep:11400 loss: 0.3259 acc: 99.48\n",
      "ep:11500 loss: 0.3259 acc: 99.48\n",
      "ep:11600 loss: 0.3259 acc: 99.48\n",
      "ep:11700 loss: 0.3259 acc: 99.48\n",
      "ep:11800 loss: 0.3259 acc: 99.48\n",
      "ep:11900 loss: 0.3259 acc: 99.48\n",
      "ep:12000 loss: 0.3258 acc: 99.48\n",
      "ep:12100 loss: 0.3259 acc: 99.48\n",
      "ep:12200 loss: 0.3259 acc: 99.48\n",
      "ep:12300 loss: 0.3259 acc: 99.48\n",
      "ep:12400 loss: 0.3258 acc: 99.48\n",
      "ep:12500 loss: 0.3259 acc: 99.48\n",
      "ep:12600 loss: 0.3259 acc: 99.48\n",
      "ep:12700 loss: 0.3259 acc: 99.48\n",
      "ep:12800 loss: 0.3259 acc: 99.48\n",
      "ep:12900 loss: 0.3259 acc: 99.48\n",
      "ep:13000 loss: 0.3259 acc: 99.48\n",
      "ep:13100 loss: 0.3259 acc: 99.48\n",
      "ep:13200 loss: 0.3259 acc: 99.48\n",
      "ep:13300 loss: 0.3259 acc: 99.48\n",
      "ep:13400 loss: 0.3259 acc: 99.48\n",
      "ep:13500 loss: 0.3259 acc: 99.48\n",
      "ep:13600 loss: 0.3258 acc: 99.48\n",
      "ep:13700 loss: 0.3259 acc: 99.48\n",
      "ep:13800 loss: 0.3259 acc: 99.48\n",
      "ep:13900 loss: 0.3259 acc: 99.48\n",
      "ep:14000 loss: 0.3259 acc: 99.48\n",
      "ep:14100 loss: 0.3259 acc: 99.48\n",
      "ep:14200 loss: 0.3259 acc: 99.48\n",
      "ep:14300 loss: 0.3259 acc: 99.48\n",
      "ep:14400 loss: 0.3259 acc: 99.48\n",
      "ep:14500 loss: 0.3259 acc: 99.48\n",
      "ep:14600 loss: 0.3259 acc: 99.48\n",
      "ep:14700 loss: 0.3259 acc: 99.48\n",
      "ep:14800 loss: 0.3258 acc: 99.48\n",
      "ep:14900 loss: 0.3259 acc: 99.48\n",
      "ep:15000 loss: 0.3259 acc: 99.48\n",
      "ep:15100 loss: 0.3259 acc: 99.48\n",
      "ep:15200 loss: 0.3259 acc: 99.48\n",
      "ep:15300 loss: 0.3259 acc: 99.48\n",
      "ep:15400 loss: 0.3259 acc: 99.48\n",
      "ep:15500 loss: 0.3259 acc: 99.48\n",
      "ep:15600 loss: 0.3259 acc: 99.48\n",
      "ep:15700 loss: 0.3259 acc: 99.48\n",
      "ep:15800 loss: 0.3259 acc: 99.48\n",
      "ep:15900 loss: 0.3259 acc: 99.48\n",
      "ep:16000 loss: 0.3259 acc: 99.48\n",
      "ep:16100 loss: 0.3259 acc: 99.48\n",
      "ep:16200 loss: 0.3259 acc: 99.48\n",
      "ep:16300 loss: 0.3259 acc: 99.48\n",
      "ep:16400 loss: 0.3259 acc: 99.48\n",
      "ep:16500 loss: 0.3259 acc: 99.48\n",
      "ep:16600 loss: 0.3259 acc: 99.48\n",
      "ep:16700 loss: 0.3259 acc: 99.48\n",
      "ep:16800 loss: 0.3259 acc: 99.48\n",
      "ep:16900 loss: 0.3259 acc: 99.48\n",
      "ep:17000 loss: 0.3259 acc: 99.48\n",
      "ep:17100 loss: 0.3259 acc: 99.48\n",
      "ep:17200 loss: 0.3259 acc: 99.48\n",
      "ep:17300 loss: 0.3259 acc: 99.48\n",
      "ep:17400 loss: 0.3259 acc: 99.48\n",
      "ep:17500 loss: 0.3259 acc: 99.48\n",
      "ep:17600 loss: 0.3259 acc: 99.48\n",
      "ep:17700 loss: 0.3259 acc: 99.48\n",
      "ep:17800 loss: 0.3259 acc: 99.48\n",
      "ep:17900 loss: 0.3259 acc: 99.48\n",
      "ep:18000 loss: 0.3258 acc: 99.48\n",
      "ep:18100 loss: 0.3259 acc: 99.48\n",
      "ep:18200 loss: 0.3259 acc: 99.48\n",
      "ep:18300 loss: 0.3259 acc: 99.48\n",
      "ep:18400 loss: 0.3259 acc: 99.48\n",
      "ep:18500 loss: 0.3259 acc: 99.48\n",
      "ep:18600 loss: 0.3259 acc: 99.48\n",
      "ep:18700 loss: 0.3259 acc: 99.48\n",
      "ep:18800 loss: 0.3258 acc: 99.48\n",
      "ep:18900 loss: 0.3259 acc: 99.48\n",
      "ep:19000 loss: 0.3259 acc: 99.48\n",
      "ep:19100 loss: 0.3259 acc: 99.48\n",
      "ep:19200 loss: 0.3259 acc: 99.48\n",
      "ep:19300 loss: 0.3259 acc: 99.48\n",
      "ep:19400 loss: 0.3259 acc: 99.48\n",
      "ep:19500 loss: 0.3259 acc: 99.48\n",
      "ep:19600 loss: 0.3259 acc: 99.48\n",
      "ep:19700 loss: 0.3259 acc: 99.48\n",
      "ep:19800 loss: 0.3259 acc: 99.48\n",
      "ep:19900 loss: 0.3259 acc: 99.48\n",
      "hidden:16, acc:99.48453521728516 , epoch:19998\n",
      "ep:  100 loss: 0.6388 acc: 61.34\n",
      "ep:  200 loss: 0.5977 acc: 57.22\n",
      "ep:  300 loss: 0.4828 acc: 67.01\n",
      "ep:  400 loss: 0.4354 acc: 70.62\n",
      "ep:  500 loss: 0.3942 acc: 77.32\n",
      "ep:  600 loss: 0.3686 acc: 77.32\n",
      "ep:  700 loss: 0.3563 acc: 77.32\n",
      "ep:  800 loss: 0.3504 acc: 77.32\n",
      "ep:  900 loss: 0.3473 acc: 76.29\n",
      "ep: 1000 loss: 0.3466 acc: 76.29\n",
      "ep: 1100 loss: 0.3459 acc: 76.29\n",
      "ep: 1200 loss: 0.3454 acc: 76.29\n",
      "ep: 1300 loss: 0.3451 acc: 75.77\n",
      "ep: 1400 loss: 0.3451 acc: 77.84\n",
      "ep: 1500 loss: 0.3453 acc: 77.84\n",
      "ep: 1600 loss: 0.3452 acc: 78.35\n",
      "ep: 1700 loss: 0.3449 acc: 78.35\n",
      "ep: 1800 loss: 0.3446 acc: 78.35\n",
      "ep: 1900 loss: 0.3441 acc: 78.35\n",
      "ep: 2000 loss: 0.3436 acc: 78.35\n",
      "ep: 2100 loss: 0.3432 acc: 78.87\n",
      "ep: 2200 loss: 0.3425 acc: 78.87\n",
      "ep: 2300 loss: 0.3411 acc: 81.96\n",
      "ep: 2400 loss: 0.3393 acc: 82.47\n",
      "ep: 2500 loss: 0.3370 acc: 82.47\n",
      "ep: 2600 loss: 0.3346 acc: 81.96\n",
      "ep: 2700 loss: 0.3350 acc: 85.05\n",
      "ep: 2800 loss: 0.3345 acc: 90.72\n",
      "ep: 2900 loss: 0.3337 acc: 91.24\n",
      "ep: 3000 loss: 0.3320 acc: 98.45\n",
      "ep: 3100 loss: 0.3303 acc: 99.48\n",
      "ep: 3200 loss: 0.3288 acc: 99.48\n",
      "ep: 3300 loss: 0.3277 acc: 99.48\n",
      "ep: 3400 loss: 0.3271 acc: 99.48\n",
      "ep: 3500 loss: 0.3267 acc: 99.48\n",
      "ep: 3600 loss: 0.3265 acc: 99.48\n",
      "ep: 3700 loss: 0.3263 acc: 99.48\n",
      "ep: 3800 loss: 0.3262 acc: 99.48\n",
      "ep: 3900 loss: 0.3261 acc: 99.48\n",
      "ep: 4000 loss: 0.3261 acc: 99.48\n",
      "ep: 4100 loss: 0.3261 acc: 99.48\n",
      "ep: 4200 loss: 0.3260 acc: 99.48\n",
      "ep: 4300 loss: 0.3260 acc: 99.48\n",
      "ep: 4400 loss: 0.3260 acc: 99.48\n",
      "ep: 4500 loss: 0.3260 acc: 99.48\n",
      "ep: 4600 loss: 0.3260 acc: 99.48\n",
      "ep: 4700 loss: 0.3260 acc: 99.48\n",
      "ep: 4800 loss: 0.3260 acc: 99.48\n",
      "ep: 4900 loss: 0.3260 acc: 99.48\n",
      "ep: 5000 loss: 0.3260 acc: 99.48\n",
      "ep: 5100 loss: 0.3261 acc: 99.48\n",
      "ep: 5200 loss: 0.3261 acc: 99.48\n",
      "ep: 5300 loss: 0.3260 acc: 99.48\n",
      "ep: 5400 loss: 0.3260 acc: 99.48\n",
      "ep: 5500 loss: 0.3256 acc: 99.48\n",
      "ep: 5600 loss: 0.3258 acc: 99.48\n",
      "ep: 5700 loss: 0.3259 acc: 99.48\n",
      "ep: 5800 loss: 0.3259 acc: 99.48\n",
      "ep: 5900 loss: 0.3259 acc: 99.48\n",
      "ep: 6000 loss: 0.3259 acc: 99.48\n",
      "ep: 6100 loss: 0.3260 acc: 99.48\n",
      "ep: 6200 loss: 0.3260 acc: 99.48\n",
      "ep: 6300 loss: 0.3260 acc: 99.48\n",
      "ep: 6400 loss: 0.3260 acc: 99.48\n",
      "ep: 6500 loss: 0.3260 acc: 99.48\n",
      "ep: 6600 loss: 0.3260 acc: 99.48\n",
      "ep: 6700 loss: 0.3260 acc: 99.48\n",
      "ep: 6800 loss: 0.3260 acc: 99.48\n",
      "ep: 6900 loss: 0.3260 acc: 99.48\n",
      "ep: 7000 loss: 0.3260 acc: 99.48\n",
      "ep: 7100 loss: 0.3260 acc: 99.48\n",
      "ep: 7200 loss: 0.3260 acc: 99.48\n",
      "ep: 7300 loss: 0.3260 acc: 99.48\n",
      "ep: 7400 loss: 0.3260 acc: 99.48\n",
      "ep: 7500 loss: 0.3260 acc: 99.48\n",
      "ep: 7600 loss: 0.3260 acc: 99.48\n",
      "ep: 7700 loss: 0.3260 acc: 99.48\n",
      "ep: 7800 loss: 0.3260 acc: 99.48\n",
      "ep: 7900 loss: 0.3255 acc: 99.48\n",
      "ep: 8000 loss: 0.3257 acc: 99.48\n",
      "ep: 8100 loss: 0.3258 acc: 99.48\n",
      "ep: 8200 loss: 0.3259 acc: 99.48\n",
      "ep: 8300 loss: 0.3259 acc: 99.48\n",
      "ep: 8400 loss: 0.3259 acc: 99.48\n",
      "ep: 8500 loss: 0.3260 acc: 99.48\n",
      "ep: 8600 loss: 0.3260 acc: 99.48\n",
      "ep: 8700 loss: 0.3260 acc: 99.48\n",
      "ep: 8800 loss: 0.3260 acc: 99.48\n",
      "ep: 8900 loss: 0.3260 acc: 99.48\n",
      "ep: 9000 loss: 0.3260 acc: 99.48\n",
      "ep: 9100 loss: 0.3260 acc: 99.48\n",
      "ep: 9200 loss: 0.3260 acc: 99.48\n",
      "ep: 9300 loss: 0.3260 acc: 99.48\n",
      "ep: 9400 loss: 0.3260 acc: 99.48\n",
      "ep: 9500 loss: 0.3260 acc: 99.48\n",
      "ep: 9600 loss: 0.3260 acc: 99.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 9700 loss: 0.3260 acc: 99.48\n",
      "ep: 9800 loss: 0.3260 acc: 99.48\n",
      "ep: 9900 loss: 0.3260 acc: 99.48\n",
      "ep:10000 loss: 0.3259 acc: 99.48\n",
      "ep:10100 loss: 0.3259 acc: 99.48\n",
      "ep:10200 loss: 0.3259 acc: 99.48\n",
      "ep:10300 loss: 0.3259 acc: 99.48\n",
      "ep:10400 loss: 0.3256 acc: 99.48\n",
      "ep:10500 loss: 0.3258 acc: 99.48\n",
      "ep:10600 loss: 0.3258 acc: 99.48\n",
      "ep:10700 loss: 0.3258 acc: 99.48\n",
      "ep:10800 loss: 0.3259 acc: 99.48\n",
      "ep:10900 loss: 0.3259 acc: 99.48\n",
      "ep:11000 loss: 0.3259 acc: 99.48\n",
      "ep:11100 loss: 0.3259 acc: 99.48\n",
      "ep:11200 loss: 0.3259 acc: 99.48\n",
      "ep:11300 loss: 0.3259 acc: 99.48\n",
      "ep:11400 loss: 0.3259 acc: 99.48\n",
      "ep:11500 loss: 0.3259 acc: 99.48\n",
      "ep:11600 loss: 0.3259 acc: 99.48\n",
      "ep:11700 loss: 0.3259 acc: 99.48\n",
      "ep:11800 loss: 0.3259 acc: 99.48\n",
      "ep:11900 loss: 0.3259 acc: 99.48\n",
      "ep:12000 loss: 0.3259 acc: 99.48\n",
      "ep:12100 loss: 0.3259 acc: 99.48\n",
      "ep:12200 loss: 0.3259 acc: 99.48\n",
      "ep:12300 loss: 0.3259 acc: 99.48\n",
      "ep:12400 loss: 0.3259 acc: 99.48\n",
      "ep:12500 loss: 0.3259 acc: 99.48\n",
      "ep:12600 loss: 0.3259 acc: 99.48\n",
      "ep:12700 loss: 0.3259 acc: 99.48\n",
      "ep:12800 loss: 0.3259 acc: 99.48\n",
      "ep:12900 loss: 0.3258 acc: 99.48\n",
      "ep:13000 loss: 0.3259 acc: 99.48\n",
      "ep:13100 loss: 0.3259 acc: 99.48\n",
      "ep:13200 loss: 0.3259 acc: 99.48\n",
      "ep:13300 loss: 0.3259 acc: 99.48\n",
      "ep:13400 loss: 0.3259 acc: 99.48\n",
      "ep:13500 loss: 0.3259 acc: 99.48\n",
      "ep:13600 loss: 0.3259 acc: 99.48\n",
      "ep:13700 loss: 0.3259 acc: 99.48\n",
      "ep:13800 loss: 0.3259 acc: 99.48\n",
      "ep:13900 loss: 0.3259 acc: 99.48\n",
      "ep:14000 loss: 0.3259 acc: 99.48\n",
      "ep:14100 loss: 0.3259 acc: 99.48\n",
      "ep:14200 loss: 0.3260 acc: 99.48\n",
      "ep:14300 loss: 0.3259 acc: 99.48\n",
      "ep:14400 loss: 0.3259 acc: 99.48\n",
      "ep:14500 loss: 0.3259 acc: 99.48\n",
      "ep:14600 loss: 0.3259 acc: 99.48\n",
      "ep:14700 loss: 0.3259 acc: 99.48\n",
      "ep:14800 loss: 0.3259 acc: 99.48\n",
      "ep:14900 loss: 0.3259 acc: 99.48\n",
      "ep:15000 loss: 0.3259 acc: 99.48\n",
      "ep:15100 loss: 0.3259 acc: 99.48\n",
      "ep:15200 loss: 0.3259 acc: 99.48\n",
      "ep:15300 loss: 0.3259 acc: 99.48\n",
      "ep:15400 loss: 0.3259 acc: 99.48\n",
      "ep:15500 loss: 0.3260 acc: 99.48\n",
      "ep:15600 loss: 0.3259 acc: 99.48\n",
      "ep:15700 loss: 0.3259 acc: 99.48\n",
      "ep:15800 loss: 0.3259 acc: 99.48\n",
      "ep:15900 loss: 0.3259 acc: 99.48\n",
      "ep:16000 loss: 0.3259 acc: 99.48\n",
      "ep:16100 loss: 0.3259 acc: 99.48\n",
      "ep:16200 loss: 0.3259 acc: 99.48\n",
      "ep:16300 loss: 0.3259 acc: 99.48\n",
      "ep:16400 loss: 0.3259 acc: 99.48\n",
      "ep:16500 loss: 0.3259 acc: 99.48\n",
      "ep:16600 loss: 0.3259 acc: 99.48\n",
      "ep:16700 loss: 0.3258 acc: 99.48\n",
      "ep:16800 loss: 0.3259 acc: 99.48\n",
      "ep:16900 loss: 0.3259 acc: 99.48\n",
      "ep:17000 loss: 0.3259 acc: 99.48\n",
      "ep:17100 loss: 0.3259 acc: 99.48\n",
      "ep:17200 loss: 0.3259 acc: 99.48\n",
      "ep:17300 loss: 0.3259 acc: 99.48\n",
      "ep:17400 loss: 0.3259 acc: 99.48\n",
      "ep:17500 loss: 0.3259 acc: 99.48\n",
      "ep:17600 loss: 0.3259 acc: 99.48\n",
      "ep:17700 loss: 0.3259 acc: 99.48\n",
      "ep:17800 loss: 0.3259 acc: 99.48\n",
      "ep:17900 loss: 0.3259 acc: 99.48\n",
      "ep:18000 loss: 0.3259 acc: 99.48\n",
      "ep:18100 loss: 0.3259 acc: 99.48\n",
      "ep:18200 loss: 0.3259 acc: 99.48\n",
      "ep:18300 loss: 0.3259 acc: 99.48\n",
      "ep:18400 loss: 0.3259 acc: 99.48\n",
      "ep:18500 loss: 0.3259 acc: 99.48\n",
      "ep:18600 loss: 0.3259 acc: 99.48\n",
      "ep:18700 loss: 0.3258 acc: 99.48\n",
      "ep:18800 loss: 0.3259 acc: 99.48\n",
      "ep:18900 loss: 0.3259 acc: 99.48\n",
      "ep:19000 loss: 0.3259 acc: 99.48\n",
      "ep:19100 loss: 0.3259 acc: 99.48\n",
      "ep:19200 loss: 0.3259 acc: 99.48\n",
      "ep:19300 loss: 0.3258 acc: 99.48\n",
      "ep:19400 loss: 0.3259 acc: 99.48\n",
      "ep:19500 loss: 0.3259 acc: 99.48\n",
      "ep:19600 loss: 0.3259 acc: 99.48\n",
      "ep:19700 loss: 0.3259 acc: 99.48\n",
      "ep:19800 loss: 0.3259 acc: 99.48\n",
      "ep:19900 loss: 0.3259 acc: 99.48\n",
      "hidden:17, acc:99.48453521728516 , epoch:19997\n",
      "ep:  100 loss: 0.6466 acc: 62.37\n",
      "ep:  200 loss: 0.5775 acc: 61.86\n",
      "ep:  300 loss: 0.4845 acc: 66.49\n",
      "ep:  400 loss: 0.4509 acc: 73.20\n",
      "ep:  500 loss: 0.4096 acc: 77.84\n",
      "ep:  600 loss: 0.3727 acc: 76.80\n",
      "ep:  700 loss: 0.3578 acc: 76.80\n",
      "ep:  800 loss: 0.3519 acc: 76.29\n",
      "ep:  900 loss: 0.3493 acc: 75.77\n",
      "ep: 1000 loss: 0.3480 acc: 75.77\n",
      "ep: 1100 loss: 0.3472 acc: 75.77\n",
      "ep: 1200 loss: 0.3468 acc: 75.77\n",
      "ep: 1300 loss: 0.3466 acc: 75.26\n",
      "ep: 1400 loss: 0.3462 acc: 75.26\n",
      "ep: 1500 loss: 0.3458 acc: 75.26\n",
      "ep: 1600 loss: 0.3454 acc: 77.32\n",
      "ep: 1700 loss: 0.3451 acc: 77.84\n",
      "ep: 1800 loss: 0.3448 acc: 78.35\n",
      "ep: 1900 loss: 0.3444 acc: 78.87\n",
      "ep: 2000 loss: 0.3441 acc: 78.35\n",
      "ep: 2100 loss: 0.3436 acc: 78.87\n",
      "ep: 2200 loss: 0.3431 acc: 78.87\n",
      "ep: 2300 loss: 0.3425 acc: 79.38\n",
      "ep: 2400 loss: 0.3413 acc: 80.41\n",
      "ep: 2500 loss: 0.3394 acc: 82.47\n",
      "ep: 2600 loss: 0.3370 acc: 82.99\n",
      "ep: 2700 loss: 0.3356 acc: 82.47\n",
      "ep: 2800 loss: 0.3334 acc: 85.05\n",
      "ep: 2900 loss: 0.3313 acc: 90.21\n",
      "ep: 3000 loss: 0.3300 acc: 90.72\n",
      "ep: 3100 loss: 0.3306 acc: 90.72\n",
      "ep: 3200 loss: 0.3304 acc: 98.45\n",
      "ep: 3300 loss: 0.3293 acc: 99.48\n",
      "ep: 3400 loss: 0.3282 acc: 99.48\n",
      "ep: 3500 loss: 0.3276 acc: 99.48\n",
      "ep: 3600 loss: 0.3271 acc: 99.48\n",
      "ep: 3700 loss: 0.3268 acc: 99.48\n",
      "ep: 3800 loss: 0.3265 acc: 99.48\n",
      "ep: 3900 loss: 0.3264 acc: 99.48\n",
      "ep: 4000 loss: 0.3263 acc: 99.48\n",
      "ep: 4100 loss: 0.3262 acc: 99.48\n",
      "ep: 4200 loss: 0.3262 acc: 99.48\n",
      "ep: 4300 loss: 0.3261 acc: 99.48\n",
      "ep: 4400 loss: 0.3261 acc: 99.48\n",
      "ep: 4500 loss: 0.3261 acc: 99.48\n",
      "ep: 4600 loss: 0.3260 acc: 99.48\n",
      "ep: 4700 loss: 0.3260 acc: 99.48\n",
      "ep: 4800 loss: 0.3260 acc: 99.48\n",
      "ep: 4900 loss: 0.3260 acc: 99.48\n",
      "ep: 5000 loss: 0.3260 acc: 99.48\n",
      "ep: 5100 loss: 0.3259 acc: 99.48\n",
      "ep: 5200 loss: 0.3259 acc: 99.48\n",
      "ep: 5300 loss: 0.3259 acc: 99.48\n",
      "ep: 5400 loss: 0.3259 acc: 99.48\n",
      "ep: 5500 loss: 0.3259 acc: 99.48\n",
      "ep: 5600 loss: 0.3259 acc: 99.48\n",
      "ep: 5700 loss: 0.3259 acc: 99.48\n",
      "ep: 5800 loss: 0.3259 acc: 99.48\n",
      "ep: 5900 loss: 0.3259 acc: 99.48\n",
      "ep: 6000 loss: 0.3258 acc: 99.48\n",
      "ep: 6100 loss: 0.3258 acc: 99.48\n",
      "ep: 6200 loss: 0.3259 acc: 99.48\n",
      "ep: 6300 loss: 0.3259 acc: 99.48\n",
      "ep: 6400 loss: 0.3259 acc: 99.48\n",
      "ep: 6500 loss: 0.3259 acc: 99.48\n",
      "ep: 6600 loss: 0.3259 acc: 99.48\n",
      "ep: 6700 loss: 0.3259 acc: 99.48\n",
      "ep: 6800 loss: 0.3259 acc: 99.48\n",
      "ep: 6900 loss: 0.3259 acc: 99.48\n",
      "ep: 7000 loss: 0.3259 acc: 99.48\n",
      "ep: 7100 loss: 0.3259 acc: 99.48\n",
      "ep: 7200 loss: 0.3259 acc: 99.48\n",
      "ep: 7300 loss: 0.3259 acc: 99.48\n",
      "ep: 7400 loss: 0.3259 acc: 99.48\n",
      "ep: 7500 loss: 0.3259 acc: 99.48\n",
      "ep: 7600 loss: 0.3259 acc: 99.48\n",
      "ep: 7700 loss: 0.3259 acc: 99.48\n",
      "ep: 7800 loss: 0.3259 acc: 99.48\n",
      "ep: 7900 loss: 0.3259 acc: 99.48\n",
      "ep: 8000 loss: 0.3259 acc: 99.48\n",
      "ep: 8100 loss: 0.3259 acc: 99.48\n",
      "ep: 8200 loss: 0.3259 acc: 99.48\n",
      "ep: 8300 loss: 0.3259 acc: 99.48\n",
      "ep: 8400 loss: 0.3254 acc: 99.48\n",
      "ep: 8500 loss: 0.3257 acc: 99.48\n",
      "ep: 8600 loss: 0.3258 acc: 99.48\n",
      "ep: 8700 loss: 0.3258 acc: 99.48\n",
      "ep: 8800 loss: 0.3258 acc: 99.48\n",
      "ep: 8900 loss: 0.3259 acc: 99.48\n",
      "ep: 9000 loss: 0.3259 acc: 99.48\n",
      "ep: 9100 loss: 0.3259 acc: 99.48\n",
      "ep: 9200 loss: 0.3259 acc: 99.48\n",
      "ep: 9300 loss: 0.3259 acc: 99.48\n",
      "ep: 9400 loss: 0.3259 acc: 99.48\n",
      "ep: 9500 loss: 0.3259 acc: 99.48\n",
      "ep: 9600 loss: 0.3259 acc: 99.48\n",
      "ep: 9700 loss: 0.3259 acc: 99.48\n",
      "ep: 9800 loss: 0.3259 acc: 99.48\n",
      "ep: 9900 loss: 0.3259 acc: 99.48\n",
      "ep:10000 loss: 0.3259 acc: 99.48\n",
      "ep:10100 loss: 0.3259 acc: 99.48\n",
      "ep:10200 loss: 0.3259 acc: 99.48\n",
      "ep:10300 loss: 0.3259 acc: 99.48\n",
      "ep:10400 loss: 0.3259 acc: 99.48\n",
      "ep:10500 loss: 0.3259 acc: 99.48\n",
      "ep:10600 loss: 0.3259 acc: 99.48\n",
      "ep:10700 loss: 0.3259 acc: 99.48\n",
      "ep:10800 loss: 0.3259 acc: 99.48\n",
      "ep:10900 loss: 0.3259 acc: 99.48\n",
      "ep:11000 loss: 0.3259 acc: 99.48\n",
      "ep:11100 loss: 0.3259 acc: 99.48\n",
      "ep:11200 loss: 0.3259 acc: 99.48\n",
      "ep:11300 loss: 0.3259 acc: 99.48\n",
      "ep:11400 loss: 0.3259 acc: 99.48\n",
      "ep:11500 loss: 0.3259 acc: 99.48\n",
      "ep:11600 loss: 0.3259 acc: 99.48\n",
      "ep:11700 loss: 0.3259 acc: 99.48\n",
      "ep:11800 loss: 0.3259 acc: 99.48\n",
      "ep:11900 loss: 0.3259 acc: 99.48\n",
      "ep:12000 loss: 0.3259 acc: 99.48\n",
      "ep:12100 loss: 0.3256 acc: 99.48\n",
      "ep:12200 loss: 0.3258 acc: 99.48\n",
      "ep:12300 loss: 0.3259 acc: 99.48\n",
      "ep:12400 loss: 0.3259 acc: 99.48\n",
      "ep:12500 loss: 0.3259 acc: 99.48\n",
      "ep:12600 loss: 0.3259 acc: 99.48\n",
      "ep:12700 loss: 0.3259 acc: 99.48\n",
      "ep:12800 loss: 0.3259 acc: 99.48\n",
      "ep:12900 loss: 0.3259 acc: 99.48\n",
      "ep:13000 loss: 0.3259 acc: 99.48\n",
      "ep:13100 loss: 0.3259 acc: 99.48\n",
      "ep:13200 loss: 0.3259 acc: 99.48\n",
      "ep:13300 loss: 0.3259 acc: 99.48\n",
      "ep:13400 loss: 0.3259 acc: 99.48\n",
      "ep:13500 loss: 0.3259 acc: 99.48\n",
      "ep:13600 loss: 0.3259 acc: 99.48\n",
      "ep:13700 loss: 0.3259 acc: 99.48\n",
      "ep:13800 loss: 0.3259 acc: 99.48\n",
      "ep:13900 loss: 0.3259 acc: 99.48\n",
      "ep:14000 loss: 0.3259 acc: 99.48\n",
      "ep:14100 loss: 0.3259 acc: 99.48\n",
      "ep:14200 loss: 0.3259 acc: 99.48\n",
      "ep:14300 loss: 0.3259 acc: 99.48\n",
      "ep:14400 loss: 0.3259 acc: 99.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:14500 loss: 0.3258 acc: 99.48\n",
      "ep:14600 loss: 0.3259 acc: 99.48\n",
      "ep:14700 loss: 0.3259 acc: 99.48\n",
      "ep:14800 loss: 0.3259 acc: 99.48\n",
      "ep:14900 loss: 0.3259 acc: 99.48\n",
      "ep:15000 loss: 0.3259 acc: 99.48\n",
      "ep:15100 loss: 0.3259 acc: 99.48\n",
      "ep:15200 loss: 0.3259 acc: 99.48\n",
      "ep:15300 loss: 0.3259 acc: 99.48\n",
      "ep:15400 loss: 0.3259 acc: 99.48\n",
      "ep:15500 loss: 0.3259 acc: 99.48\n",
      "ep:15600 loss: 0.3259 acc: 99.48\n",
      "ep:15700 loss: 0.3259 acc: 99.48\n",
      "ep:15800 loss: 0.3259 acc: 99.48\n",
      "ep:15900 loss: 0.3259 acc: 99.48\n",
      "ep:16000 loss: 0.3259 acc: 99.48\n",
      "ep:16100 loss: 0.3259 acc: 99.48\n",
      "ep:16200 loss: 0.3259 acc: 99.48\n",
      "ep:16300 loss: 0.3259 acc: 99.48\n",
      "ep:16400 loss: 0.3260 acc: 99.48\n",
      "ep:16500 loss: 0.3259 acc: 99.48\n",
      "ep:16600 loss: 0.3259 acc: 99.48\n",
      "ep:16700 loss: 0.3259 acc: 99.48\n",
      "ep:16800 loss: 0.3259 acc: 99.48\n",
      "ep:16900 loss: 0.3259 acc: 99.48\n",
      "ep:17000 loss: 0.3259 acc: 99.48\n",
      "ep:17100 loss: 0.3259 acc: 99.48\n",
      "ep:17200 loss: 0.3259 acc: 99.48\n",
      "ep:17300 loss: 0.3259 acc: 99.48\n",
      "ep:17400 loss: 0.3259 acc: 99.48\n",
      "ep:17500 loss: 0.3259 acc: 99.48\n",
      "ep:17600 loss: 0.3259 acc: 99.48\n",
      "ep:17700 loss: 0.3260 acc: 99.48\n",
      "ep:17800 loss: 0.3259 acc: 99.48\n",
      "ep:17900 loss: 0.3259 acc: 99.48\n",
      "ep:18000 loss: 0.3259 acc: 99.48\n",
      "ep:18100 loss: 0.3259 acc: 99.48\n",
      "ep:18200 loss: 0.3259 acc: 99.48\n",
      "ep:18300 loss: 0.3259 acc: 99.48\n",
      "ep:18400 loss: 0.3259 acc: 99.48\n",
      "ep:18500 loss: 0.3259 acc: 99.48\n",
      "ep:18600 loss: 0.3259 acc: 99.48\n",
      "ep:18700 loss: 0.3259 acc: 99.48\n",
      "ep:18800 loss: 0.3259 acc: 99.48\n",
      "ep:18900 loss: 0.3259 acc: 99.48\n",
      "ep:19000 loss: 0.3259 acc: 99.48\n",
      "ep:19100 loss: 0.3259 acc: 99.48\n",
      "ep:19200 loss: 0.3259 acc: 99.48\n",
      "ep:19300 loss: 0.3259 acc: 99.48\n",
      "ep:19400 loss: 0.3259 acc: 99.48\n",
      "ep:19500 loss: 0.3259 acc: 99.48\n",
      "ep:19600 loss: 0.3259 acc: 99.48\n",
      "ep:19700 loss: 0.3259 acc: 99.48\n",
      "ep:19800 loss: 0.3259 acc: 99.48\n",
      "ep:19900 loss: 0.3259 acc: 99.48\n",
      "hidden:18, acc:99.48453521728516 , epoch:19996\n",
      "ep:  100 loss: 0.6345 acc: 62.37\n",
      "ep:  200 loss: 0.4863 acc: 65.98\n",
      "ep:  300 loss: 0.4092 acc: 75.77\n",
      "ep:  400 loss: 0.3678 acc: 75.77\n",
      "ep:  500 loss: 0.3547 acc: 75.77\n",
      "ep:  600 loss: 0.3502 acc: 75.77\n",
      "ep:  700 loss: 0.3483 acc: 75.77\n",
      "ep:  800 loss: 0.3475 acc: 75.77\n",
      "ep:  900 loss: 0.3468 acc: 75.26\n",
      "ep: 1000 loss: 0.3463 acc: 75.26\n",
      "ep: 1100 loss: 0.3459 acc: 75.26\n",
      "ep: 1200 loss: 0.3456 acc: 77.32\n",
      "ep: 1300 loss: 0.3454 acc: 77.84\n",
      "ep: 1400 loss: 0.3452 acc: 78.35\n",
      "ep: 1500 loss: 0.3450 acc: 78.35\n",
      "ep: 1600 loss: 0.3447 acc: 78.35\n",
      "ep: 1700 loss: 0.3442 acc: 78.87\n",
      "ep: 1800 loss: 0.3350 acc: 82.99\n",
      "ep: 1900 loss: 0.3331 acc: 90.21\n",
      "ep: 2000 loss: 0.3319 acc: 90.72\n",
      "ep: 2100 loss: 0.3308 acc: 91.75\n",
      "ep: 2200 loss: 0.3291 acc: 99.48\n",
      "converged:19\n",
      "hidden:19, acc:100.0 , epoch:2295\n",
      "Done with task!, found optimal : 19\n"
     ]
    }
   ],
   "source": [
    "num_input = data.shape[1] - 1\n",
    "\n",
    "full_input  = data[:,0:num_input]\n",
    "full_target = data[:,num_input:num_input+1]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(full_input,full_target)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset,batch_size=97)\n",
    "\n",
    "epoch = 20000\n",
    "converge = False\n",
    "hid_layers = 15\n",
    "while not converge:\n",
    "    net = PolarNet(hid_layers)\n",
    "    if list(net.parameters()):\n",
    "        # initialize weight values\n",
    "        for m in list(net.parameters()):\n",
    "            m.data.normal_(0,.1)\n",
    "        # use Adam optimizer\n",
    "        optimizer = torch.optim.Adam(net.parameters(),eps=0.000001,lr=.01,\n",
    "                                     betas=(0.9,0.999),weight_decay=0.0001)\n",
    "        # training loop\n",
    "        for epoch in range(1, epoch):\n",
    "            accuracy = train(net, train_loader, optimizer)\n",
    "            if accuracy == 100:\n",
    "                print(f\"converged:{hid_layers}\")\n",
    "                converge = True\n",
    "                break\n",
    "    print(f\"hidden:{hid_layers}, acc:{accuracy} , epoch:{epoch}\")\n",
    "    hid_layers += 1\n",
    "print(f\"Done with task!, found optimal : {hid_layers - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "star16 = torch.Tensor(\n",
    "    [[1,1,0,0,0,0,0,0],\n",
    "     [0,1,1,0,0,0,0,0],\n",
    "     [0,0,1,1,0,0,0,0],\n",
    "     [0,0,0,1,1,0,0,0],\n",
    "     [0,0,0,0,1,1,0,0],\n",
    "     [0,0,0,0,0,1,1,0],\n",
    "     [0,0,0,0,0,0,1,1],\n",
    "     [1,0,0,0,0,0,0,1],\n",
    "     [1,1,0,0,0,0,0,1],\n",
    "     [1,1,1,0,0,0,0,0],\n",
    "     [0,1,1,1,0,0,0,0],\n",
    "     [0,0,1,1,1,0,0,0],\n",
    "     [0,0,0,1,1,1,0,0],\n",
    "     [0,0,0,0,1,1,1,0],\n",
    "     [0,0,0,0,0,1,1,1],\n",
    "     [1,0,0,0,0,0,1,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = star16\n",
    "num_in  = target.size()[0]\n",
    "num_out = target.size()[1]\n",
    "\n",
    "# input is one-hot with same number of rows as target\n",
    "input = torch.eye(num_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star16.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart18 =  torch.Tensor([\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,1,0,0,0,0,0,1,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = heart18\n",
    "num_in  = target.size()[0]\n",
    "num_out = target.size()[1]\n",
    "\n",
    "# input is one-hot with same number of rows as target\n",
    "input = torch.eye(num_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
