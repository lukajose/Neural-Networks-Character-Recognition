{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "# fetch and load training data\n",
    "trainset = datasets.KMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # fetch and load test data\n",
    "testset = datasets.KMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this means we have 64 images of 28x28 pixels in grayscale (1 channel)\n",
    "\n",
    "example_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.view(example_data.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape\n",
    "x = example_data\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(x.shape[0], -1)#flatten image into 784x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spirals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.50000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.31380</td>\n",
       "      <td>1.25590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.31380</td>\n",
       "      <td>-1.25590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.88973</td>\n",
       "      <td>2.43961</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-5.88973</td>\n",
       "      <td>-2.43961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.24865</td>\n",
       "      <td>3.50704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-5.24865</td>\n",
       "      <td>-3.50704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.41941</td>\n",
       "      <td>4.41943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-4.41941</td>\n",
       "      <td>-4.41943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_coord   y_coord  class\n",
       "0  6.50000   0.00000      0\n",
       "1 -6.50000  -0.00000      1\n",
       "2  6.31380   1.25590      0\n",
       "3 -6.31380  -1.25590      1\n",
       "4  5.88973   2.43961      0\n",
       "5 -5.88973  -2.43961      1\n",
       "6  5.24865   3.50704      0\n",
       "7 -5.24865  -3.50704      1\n",
       "8  4.41941   4.41943      0\n",
       "9 -4.41941  -4.41943      1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(df.values,dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-6.5000e+00, -0.0000e+00,  1.0000e+00],\n",
       "        [ 6.3138e+00,  1.2559e+00,  0.0000e+00],\n",
       "        [-6.3138e+00, -1.2559e+00,  1.0000e+00],\n",
       "        [ 5.8897e+00,  2.4396e+00,  0.0000e+00],\n",
       "        [-5.8897e+00, -2.4396e+00,  1.0000e+00],\n",
       "        [ 5.2487e+00,  3.5070e+00,  0.0000e+00],\n",
       "        [-5.2487e+00, -3.5070e+00,  1.0000e+00],\n",
       "        [ 4.4194e+00,  4.4194e+00,  0.0000e+00],\n",
       "        [-4.4194e+00, -4.4194e+00,  1.0000e+00],\n",
       "        [ 3.4376e+00,  5.1447e+00,  0.0000e+00],\n",
       "        [-3.4376e+00, -5.1447e+00,  1.0000e+00],\n",
       "        [ 2.3439e+00,  5.6588e+00,  0.0000e+00],\n",
       "        [-2.3439e+00, -5.6588e+00,  1.0000e+00],\n",
       "        [ 1.1827e+00,  5.9460e+00,  0.0000e+00],\n",
       "        [-1.1827e+00, -5.9460e+00,  1.0000e+00],\n",
       "        [-2.0000e-05,  6.0000e+00,  0.0000e+00],\n",
       "        [ 2.0000e-05, -6.0000e+00,  1.0000e+00],\n",
       "        [-1.1584e+00,  5.8234e+00,  0.0000e+00],\n",
       "        [ 1.1584e+00, -5.8234e+00,  1.0000e+00],\n",
       "        [-2.2483e+00,  5.4278e+00,  0.0000e+00],\n",
       "        [ 2.2483e+00, -5.4278e+00,  1.0000e+00],\n",
       "        [-3.2293e+00,  4.8329e+00,  0.0000e+00],\n",
       "        [ 3.2293e+00, -4.8329e+00,  1.0000e+00],\n",
       "        [-4.0659e+00,  4.0658e+00,  0.0000e+00],\n",
       "        [ 4.0659e+00, -4.0658e+00,  1.0000e+00],\n",
       "        [-4.7290e+00,  3.1598e+00,  0.0000e+00],\n",
       "        [ 4.7290e+00, -3.1598e+00,  1.0000e+00],\n",
       "        [-5.1968e+00,  2.1526e+00,  0.0000e+00],\n",
       "        [ 5.1968e+00, -2.1526e+00,  1.0000e+00],\n",
       "        [-5.4556e+00,  1.0852e+00,  0.0000e+00],\n",
       "        [ 5.4556e+00, -1.0852e+00,  1.0000e+00],\n",
       "        [-5.5000e+00, -4.0000e-05,  0.0000e+00],\n",
       "        [ 5.5000e+00,  4.0000e-05,  1.0000e+00],\n",
       "        [-5.3330e+00, -1.0609e+00,  0.0000e+00],\n",
       "        [ 5.3330e+00,  1.0609e+00,  1.0000e+00],\n",
       "        [-4.9658e+00, -2.0570e+00,  0.0000e+00],\n",
       "        [ 4.9658e+00,  2.0570e+00,  1.0000e+00],\n",
       "        [-4.4172e+00, -2.9515e+00,  0.0000e+00],\n",
       "        [ 4.4172e+00,  2.9515e+00,  1.0000e+00],\n",
       "        [-3.7123e+00, -3.7123e+00,  0.0000e+00],\n",
       "        [ 3.7123e+00,  3.7123e+00,  1.0000e+00],\n",
       "        [-2.8820e+00, -4.3133e+00,  0.0000e+00],\n",
       "        [ 2.8820e+00,  4.3133e+00,  1.0000e+00],\n",
       "        [-1.9612e+00, -4.7349e+00,  0.0000e+00],\n",
       "        [ 1.9612e+00,  4.7349e+00,  1.0000e+00],\n",
       "        [-9.8759e-01, -4.9652e+00,  0.0000e+00],\n",
       "        [ 9.8759e-01,  4.9652e+00,  1.0000e+00],\n",
       "        [ 6.0000e-05, -5.0000e+00,  0.0000e+00],\n",
       "        [-6.0000e-05,  5.0000e+00,  1.0000e+00],\n",
       "        [ 9.6331e-01, -4.8426e+00,  0.0000e+00],\n",
       "        [-9.6331e-01,  4.8426e+00,  1.0000e+00],\n",
       "        [ 1.8656e+00, -4.5039e+00,  0.0000e+00],\n",
       "        [-1.8656e+00,  4.5039e+00,  1.0000e+00],\n",
       "        [ 2.6737e+00, -4.0014e+00,  0.0000e+00],\n",
       "        [-2.6737e+00,  4.0014e+00,  1.0000e+00],\n",
       "        [ 3.3588e+00, -3.3587e+00,  0.0000e+00],\n",
       "        [-3.3588e+00,  3.3587e+00,  1.0000e+00],\n",
       "        [ 3.8976e+00, -2.6042e+00,  0.0000e+00],\n",
       "        [-3.8976e+00,  2.6042e+00,  1.0000e+00],\n",
       "        [ 4.2730e+00, -1.7699e+00,  0.0000e+00],\n",
       "        [-4.2730e+00,  1.7699e+00,  1.0000e+00],\n",
       "        [ 4.4749e+00, -8.9004e-01,  0.0000e+00],\n",
       "        [-4.4749e+00,  8.9004e-01,  1.0000e+00],\n",
       "        [ 4.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-4.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 4.3522e+00,  8.6578e-01,  0.0000e+00],\n",
       "        [-4.3522e+00, -8.6578e-01,  1.0000e+00],\n",
       "        [ 4.0420e+00,  1.6743e+00,  0.0000e+00],\n",
       "        [-4.0420e+00, -1.6743e+00,  1.0000e+00],\n",
       "        [ 3.5857e+00,  2.3960e+00,  0.0000e+00],\n",
       "        [-3.5857e+00, -2.3960e+00,  1.0000e+00],\n",
       "        [ 3.0052e+00,  3.0052e+00,  0.0000e+00],\n",
       "        [-3.0052e+00, -3.0052e+00,  1.0000e+00],\n",
       "        [ 2.3264e+00,  3.4818e+00,  0.0000e+00],\n",
       "        [-2.3264e+00, -3.4818e+00,  1.0000e+00],\n",
       "        [ 1.5785e+00,  3.8110e+00,  0.0000e+00],\n",
       "        [-1.5785e+00, -3.8110e+00,  1.0000e+00],\n",
       "        [ 7.9248e-01,  3.9845e+00,  0.0000e+00],\n",
       "        [-7.9248e-01, -3.9845e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  4.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -4.0000e+00,  1.0000e+00],\n",
       "        [-7.6824e-01,  3.8618e+00,  0.0000e+00],\n",
       "        [ 7.6824e-01, -3.8618e+00,  1.0000e+00],\n",
       "        [-1.4830e+00,  3.5800e+00,  0.0000e+00],\n",
       "        [ 1.4830e+00, -3.5800e+00,  1.0000e+00],\n",
       "        [-2.1182e+00,  3.1699e+00,  0.0000e+00],\n",
       "        [ 2.1182e+00, -3.1699e+00,  1.0000e+00],\n",
       "        [-2.6517e+00,  2.6516e+00,  0.0000e+00],\n",
       "        [ 2.6517e+00, -2.6516e+00,  1.0000e+00],\n",
       "        [-3.0661e+00,  2.0486e+00,  0.0000e+00],\n",
       "        [ 3.0661e+00, -2.0486e+00,  1.0000e+00],\n",
       "        [-3.3491e+00,  1.3872e+00,  0.0000e+00],\n",
       "        [ 3.3491e+00, -1.3872e+00,  1.0000e+00],\n",
       "        [-3.4941e+00,  6.9493e-01,  0.0000e+00],\n",
       "        [ 3.4941e+00, -6.9493e-01,  1.0000e+00],\n",
       "        [-3.5000e+00, -8.0000e-05,  0.0000e+00],\n",
       "        [ 3.5000e+00,  8.0000e-05,  1.0000e+00],\n",
       "        [-3.3714e+00, -6.7070e-01,  0.0000e+00],\n",
       "        [ 3.3714e+00,  6.7070e-01,  1.0000e+00],\n",
       "        [-3.1181e+00, -1.2916e+00,  0.0000e+00],\n",
       "        [ 3.1181e+00,  1.2916e+00,  1.0000e+00],\n",
       "        [-2.7542e+00, -1.8404e+00,  0.0000e+00],\n",
       "        [ 2.7542e+00,  1.8404e+00,  1.0000e+00],\n",
       "        [-2.2980e+00, -2.2982e+00,  0.0000e+00],\n",
       "        [ 2.2980e+00,  2.2982e+00,  1.0000e+00],\n",
       "        [-1.7708e+00, -2.6504e+00,  0.0000e+00],\n",
       "        [ 1.7708e+00,  2.6504e+00,  1.0000e+00],\n",
       "        [-1.1958e+00, -2.8872e+00,  0.0000e+00],\n",
       "        [ 1.1958e+00,  2.8872e+00,  1.0000e+00],\n",
       "        [-5.9739e-01, -3.0037e+00,  0.0000e+00],\n",
       "        [ 5.9739e-01,  3.0037e+00,  1.0000e+00],\n",
       "        [ 8.0000e-05, -3.0000e+00,  0.0000e+00],\n",
       "        [-8.0000e-05,  3.0000e+00,  1.0000e+00],\n",
       "        [ 5.7315e-01, -2.8810e+00,  0.0000e+00],\n",
       "        [-5.7315e-01,  2.8810e+00,  1.0000e+00],\n",
       "        [ 1.1003e+00, -2.6561e+00,  0.0000e+00],\n",
       "        [-1.1003e+00,  2.6561e+00,  1.0000e+00],\n",
       "        [ 1.5626e+00, -2.3385e+00,  0.0000e+00],\n",
       "        [-1.5626e+00,  2.3385e+00,  1.0000e+00],\n",
       "        [ 1.9446e+00, -1.9445e+00,  0.0000e+00],\n",
       "        [-1.9446e+00,  1.9445e+00,  1.0000e+00],\n",
       "        [ 2.2346e+00, -1.4930e+00,  0.0000e+00],\n",
       "        [-2.2346e+00,  1.4930e+00,  1.0000e+00],\n",
       "        [ 2.4252e+00, -1.0045e+00,  0.0000e+00],\n",
       "        [-2.4252e+00,  1.0045e+00,  1.0000e+00],\n",
       "        [ 2.5133e+00, -4.9985e-01,  0.0000e+00],\n",
       "        [-2.5133e+00,  4.9985e-01,  1.0000e+00],\n",
       "        [ 2.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-2.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 2.3907e+00,  4.7560e-01,  0.0000e+00],\n",
       "        [-2.3907e+00, -4.7560e-01,  1.0000e+00],\n",
       "        [ 2.1942e+00,  9.0894e-01,  0.0000e+00],\n",
       "        [-2.1942e+00, -9.0894e-01,  1.0000e+00],\n",
       "        [ 1.9227e+00,  1.2848e+00,  0.0000e+00],\n",
       "        [-1.9227e+00, -1.2848e+00,  1.0000e+00],\n",
       "        [ 1.5909e+00,  1.5910e+00,  0.0000e+00],\n",
       "        [-1.5909e+00, -1.5910e+00,  1.0000e+00],\n",
       "        [ 1.2153e+00,  1.8189e+00,  0.0000e+00],\n",
       "        [-1.2153e+00, -1.8189e+00,  1.0000e+00],\n",
       "        [ 8.1314e-01,  1.9633e+00,  0.0000e+00],\n",
       "        [-8.1314e-01, -1.9633e+00,  1.0000e+00],\n",
       "        [ 4.0231e-01,  2.0229e+00,  0.0000e+00],\n",
       "        [-4.0231e-01, -2.0229e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  2.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -2.0000e+00,  1.0000e+00],\n",
       "        [-3.7805e-01,  1.9003e+00,  0.0000e+00],\n",
       "        [ 3.7805e-01, -1.9003e+00,  1.0000e+00],\n",
       "        [-7.1759e-01,  1.7322e+00,  0.0000e+00],\n",
       "        [ 7.1759e-01, -1.7322e+00,  1.0000e+00],\n",
       "        [-1.0070e+00,  1.5070e+00,  0.0000e+00],\n",
       "        [ 1.0070e+00, -1.5070e+00,  1.0000e+00],\n",
       "        [-1.2375e+00,  1.2374e+00,  0.0000e+00],\n",
       "        [ 1.2375e+00, -1.2374e+00,  1.0000e+00],\n",
       "        [-1.4031e+00,  9.3748e-01,  0.0000e+00],\n",
       "        [ 1.4031e+00, -9.3748e-01,  1.0000e+00],\n",
       "        [-1.5013e+00,  6.2181e-01,  0.0000e+00],\n",
       "        [ 1.5013e+00, -6.2181e-01,  1.0000e+00],\n",
       "        [-1.5325e+00,  3.0477e-01,  0.0000e+00],\n",
       "        [ 1.5325e+00, -3.0477e-01,  1.0000e+00],\n",
       "        [-1.5000e+00, -6.0000e-05,  0.0000e+00],\n",
       "        [ 1.5000e+00,  6.0000e-05,  1.0000e+00],\n",
       "        [-1.4099e+00, -2.8049e-01,  0.0000e+00],\n",
       "        [ 1.4099e+00,  2.8049e-01,  1.0000e+00],\n",
       "        [-1.2703e+00, -5.2624e-01,  0.0000e+00],\n",
       "        [ 1.2703e+00,  5.2624e-01,  1.0000e+00],\n",
       "        [-1.0913e+00, -7.2923e-01,  0.0000e+00],\n",
       "        [ 1.0913e+00,  7.2923e-01,  1.0000e+00],\n",
       "        [-8.8385e-01, -8.8392e-01,  0.0000e+00],\n",
       "        [ 8.8385e-01,  8.8392e-01,  1.0000e+00],\n",
       "        [-6.5970e-01, -9.8740e-01,  0.0000e+00],\n",
       "        [ 6.5970e-01,  9.8740e-01,  1.0000e+00],\n",
       "        [-4.3048e-01, -1.0394e+00,  0.0000e+00],\n",
       "        [ 4.3048e-01,  1.0394e+00,  1.0000e+00],\n",
       "        [-2.0724e-01, -1.0421e+00,  0.0000e+00],\n",
       "        [ 2.0724e-01,  1.0421e+00,  1.0000e+00],\n",
       "        [ 4.0000e-05, -1.0000e+00,  0.0000e+00],\n",
       "        [-4.0000e-05,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.8293e-01, -9.1948e-01,  0.0000e+00],\n",
       "        [-1.8293e-01,  9.1948e-01,  1.0000e+00],\n",
       "        [ 3.3488e-01, -8.0838e-01,  0.0000e+00],\n",
       "        [-3.3488e-01,  8.0838e-01,  1.0000e+00],\n",
       "        [ 4.5143e-01, -6.7555e-01,  0.0000e+00],\n",
       "        [-4.5143e-01,  6.7555e-01,  1.0000e+00],\n",
       "        [ 5.3035e-01, -5.3031e-01,  0.0000e+00],\n",
       "        [-5.3035e-01,  5.3031e-01,  1.0000e+00],\n",
       "        [ 5.7165e-01, -3.8193e-01,  0.0000e+00],\n",
       "        [-5.7165e-01,  3.8193e-01,  1.0000e+00],\n",
       "        [ 5.7744e-01, -2.3915e-01,  0.0000e+00],\n",
       "        [-5.7744e-01,  2.3915e-01,  1.0000e+00],\n",
       "        [ 5.5170e-01, -1.0971e-01,  0.0000e+00],\n",
       "        [-5.5170e-01,  1.0971e-01,  1.0000e+00],\n",
       "        [ 5.0000e-01,  2.0000e-05,  0.0000e+00],\n",
       "        [-5.0000e-01, -2.0000e-05,  1.0000e+00]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data #transform all data into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_input # number of features without the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data\n",
    "full_input  = data[:,0:num_input]\n",
    "full_target = data[:,num_input:num_input+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000e+00,  0.0000e+00],\n",
       "        [-6.5000e+00, -0.0000e+00],\n",
       "        [ 6.3138e+00,  1.2559e+00],\n",
       "        [-6.3138e+00, -1.2559e+00],\n",
       "        [ 5.8897e+00,  2.4396e+00],\n",
       "        [-5.8897e+00, -2.4396e+00],\n",
       "        [ 5.2487e+00,  3.5070e+00],\n",
       "        [-5.2487e+00, -3.5070e+00],\n",
       "        [ 4.4194e+00,  4.4194e+00],\n",
       "        [-4.4194e+00, -4.4194e+00],\n",
       "        [ 3.4376e+00,  5.1447e+00],\n",
       "        [-3.4376e+00, -5.1447e+00],\n",
       "        [ 2.3439e+00,  5.6588e+00],\n",
       "        [-2.3439e+00, -5.6588e+00],\n",
       "        [ 1.1827e+00,  5.9460e+00],\n",
       "        [-1.1827e+00, -5.9460e+00],\n",
       "        [-2.0000e-05,  6.0000e+00],\n",
       "        [ 2.0000e-05, -6.0000e+00],\n",
       "        [-1.1584e+00,  5.8234e+00],\n",
       "        [ 1.1584e+00, -5.8234e+00],\n",
       "        [-2.2483e+00,  5.4278e+00],\n",
       "        [ 2.2483e+00, -5.4278e+00],\n",
       "        [-3.2293e+00,  4.8329e+00],\n",
       "        [ 3.2293e+00, -4.8329e+00],\n",
       "        [-4.0659e+00,  4.0658e+00],\n",
       "        [ 4.0659e+00, -4.0658e+00],\n",
       "        [-4.7290e+00,  3.1598e+00],\n",
       "        [ 4.7290e+00, -3.1598e+00],\n",
       "        [-5.1968e+00,  2.1526e+00],\n",
       "        [ 5.1968e+00, -2.1526e+00],\n",
       "        [-5.4556e+00,  1.0852e+00],\n",
       "        [ 5.4556e+00, -1.0852e+00],\n",
       "        [-5.5000e+00, -4.0000e-05],\n",
       "        [ 5.5000e+00,  4.0000e-05],\n",
       "        [-5.3330e+00, -1.0609e+00],\n",
       "        [ 5.3330e+00,  1.0609e+00],\n",
       "        [-4.9658e+00, -2.0570e+00],\n",
       "        [ 4.9658e+00,  2.0570e+00],\n",
       "        [-4.4172e+00, -2.9515e+00],\n",
       "        [ 4.4172e+00,  2.9515e+00],\n",
       "        [-3.7123e+00, -3.7123e+00],\n",
       "        [ 3.7123e+00,  3.7123e+00],\n",
       "        [-2.8820e+00, -4.3133e+00],\n",
       "        [ 2.8820e+00,  4.3133e+00],\n",
       "        [-1.9612e+00, -4.7349e+00],\n",
       "        [ 1.9612e+00,  4.7349e+00],\n",
       "        [-9.8759e-01, -4.9652e+00],\n",
       "        [ 9.8759e-01,  4.9652e+00],\n",
       "        [ 6.0000e-05, -5.0000e+00],\n",
       "        [-6.0000e-05,  5.0000e+00],\n",
       "        [ 9.6331e-01, -4.8426e+00],\n",
       "        [-9.6331e-01,  4.8426e+00],\n",
       "        [ 1.8656e+00, -4.5039e+00],\n",
       "        [-1.8656e+00,  4.5039e+00],\n",
       "        [ 2.6737e+00, -4.0014e+00],\n",
       "        [-2.6737e+00,  4.0014e+00],\n",
       "        [ 3.3588e+00, -3.3587e+00],\n",
       "        [-3.3588e+00,  3.3587e+00],\n",
       "        [ 3.8976e+00, -2.6042e+00],\n",
       "        [-3.8976e+00,  2.6042e+00],\n",
       "        [ 4.2730e+00, -1.7699e+00],\n",
       "        [-4.2730e+00,  1.7699e+00],\n",
       "        [ 4.4749e+00, -8.9004e-01],\n",
       "        [-4.4749e+00,  8.9004e-01],\n",
       "        [ 4.5000e+00,  7.0000e-05],\n",
       "        [-4.5000e+00, -7.0000e-05],\n",
       "        [ 4.3522e+00,  8.6578e-01],\n",
       "        [-4.3522e+00, -8.6578e-01],\n",
       "        [ 4.0420e+00,  1.6743e+00],\n",
       "        [-4.0420e+00, -1.6743e+00],\n",
       "        [ 3.5857e+00,  2.3960e+00],\n",
       "        [-3.5857e+00, -2.3960e+00],\n",
       "        [ 3.0052e+00,  3.0052e+00],\n",
       "        [-3.0052e+00, -3.0052e+00],\n",
       "        [ 2.3264e+00,  3.4818e+00],\n",
       "        [-2.3264e+00, -3.4818e+00],\n",
       "        [ 1.5785e+00,  3.8110e+00],\n",
       "        [-1.5785e+00, -3.8110e+00],\n",
       "        [ 7.9248e-01,  3.9845e+00],\n",
       "        [-7.9248e-01, -3.9845e+00],\n",
       "        [-7.0000e-05,  4.0000e+00],\n",
       "        [ 7.0000e-05, -4.0000e+00],\n",
       "        [-7.6824e-01,  3.8618e+00],\n",
       "        [ 7.6824e-01, -3.8618e+00],\n",
       "        [-1.4830e+00,  3.5800e+00],\n",
       "        [ 1.4830e+00, -3.5800e+00],\n",
       "        [-2.1182e+00,  3.1699e+00],\n",
       "        [ 2.1182e+00, -3.1699e+00],\n",
       "        [-2.6517e+00,  2.6516e+00],\n",
       "        [ 2.6517e+00, -2.6516e+00],\n",
       "        [-3.0661e+00,  2.0486e+00],\n",
       "        [ 3.0661e+00, -2.0486e+00],\n",
       "        [-3.3491e+00,  1.3872e+00],\n",
       "        [ 3.3491e+00, -1.3872e+00],\n",
       "        [-3.4941e+00,  6.9493e-01],\n",
       "        [ 3.4941e+00, -6.9493e-01],\n",
       "        [-3.5000e+00, -8.0000e-05],\n",
       "        [ 3.5000e+00,  8.0000e-05],\n",
       "        [-3.3714e+00, -6.7070e-01],\n",
       "        [ 3.3714e+00,  6.7070e-01],\n",
       "        [-3.1181e+00, -1.2916e+00],\n",
       "        [ 3.1181e+00,  1.2916e+00],\n",
       "        [-2.7542e+00, -1.8404e+00],\n",
       "        [ 2.7542e+00,  1.8404e+00],\n",
       "        [-2.2980e+00, -2.2982e+00],\n",
       "        [ 2.2980e+00,  2.2982e+00],\n",
       "        [-1.7708e+00, -2.6504e+00],\n",
       "        [ 1.7708e+00,  2.6504e+00],\n",
       "        [-1.1958e+00, -2.8872e+00],\n",
       "        [ 1.1958e+00,  2.8872e+00],\n",
       "        [-5.9739e-01, -3.0037e+00],\n",
       "        [ 5.9739e-01,  3.0037e+00],\n",
       "        [ 8.0000e-05, -3.0000e+00],\n",
       "        [-8.0000e-05,  3.0000e+00],\n",
       "        [ 5.7315e-01, -2.8810e+00],\n",
       "        [-5.7315e-01,  2.8810e+00],\n",
       "        [ 1.1003e+00, -2.6561e+00],\n",
       "        [-1.1003e+00,  2.6561e+00],\n",
       "        [ 1.5626e+00, -2.3385e+00],\n",
       "        [-1.5626e+00,  2.3385e+00],\n",
       "        [ 1.9446e+00, -1.9445e+00],\n",
       "        [-1.9446e+00,  1.9445e+00],\n",
       "        [ 2.2346e+00, -1.4930e+00],\n",
       "        [-2.2346e+00,  1.4930e+00],\n",
       "        [ 2.4252e+00, -1.0045e+00],\n",
       "        [-2.4252e+00,  1.0045e+00],\n",
       "        [ 2.5133e+00, -4.9985e-01],\n",
       "        [-2.5133e+00,  4.9985e-01],\n",
       "        [ 2.5000e+00,  7.0000e-05],\n",
       "        [-2.5000e+00, -7.0000e-05],\n",
       "        [ 2.3907e+00,  4.7560e-01],\n",
       "        [-2.3907e+00, -4.7560e-01],\n",
       "        [ 2.1942e+00,  9.0894e-01],\n",
       "        [-2.1942e+00, -9.0894e-01],\n",
       "        [ 1.9227e+00,  1.2848e+00],\n",
       "        [-1.9227e+00, -1.2848e+00],\n",
       "        [ 1.5909e+00,  1.5910e+00],\n",
       "        [-1.5909e+00, -1.5910e+00],\n",
       "        [ 1.2153e+00,  1.8189e+00],\n",
       "        [-1.2153e+00, -1.8189e+00],\n",
       "        [ 8.1314e-01,  1.9633e+00],\n",
       "        [-8.1314e-01, -1.9633e+00],\n",
       "        [ 4.0231e-01,  2.0229e+00],\n",
       "        [-4.0231e-01, -2.0229e+00],\n",
       "        [-7.0000e-05,  2.0000e+00],\n",
       "        [ 7.0000e-05, -2.0000e+00],\n",
       "        [-3.7805e-01,  1.9003e+00],\n",
       "        [ 3.7805e-01, -1.9003e+00],\n",
       "        [-7.1759e-01,  1.7322e+00],\n",
       "        [ 7.1759e-01, -1.7322e+00],\n",
       "        [-1.0070e+00,  1.5070e+00],\n",
       "        [ 1.0070e+00, -1.5070e+00],\n",
       "        [-1.2375e+00,  1.2374e+00],\n",
       "        [ 1.2375e+00, -1.2374e+00],\n",
       "        [-1.4031e+00,  9.3748e-01],\n",
       "        [ 1.4031e+00, -9.3748e-01],\n",
       "        [-1.5013e+00,  6.2181e-01],\n",
       "        [ 1.5013e+00, -6.2181e-01],\n",
       "        [-1.5325e+00,  3.0477e-01],\n",
       "        [ 1.5325e+00, -3.0477e-01],\n",
       "        [-1.5000e+00, -6.0000e-05],\n",
       "        [ 1.5000e+00,  6.0000e-05],\n",
       "        [-1.4099e+00, -2.8049e-01],\n",
       "        [ 1.4099e+00,  2.8049e-01],\n",
       "        [-1.2703e+00, -5.2624e-01],\n",
       "        [ 1.2703e+00,  5.2624e-01],\n",
       "        [-1.0913e+00, -7.2923e-01],\n",
       "        [ 1.0913e+00,  7.2923e-01],\n",
       "        [-8.8385e-01, -8.8392e-01],\n",
       "        [ 8.8385e-01,  8.8392e-01],\n",
       "        [-6.5970e-01, -9.8740e-01],\n",
       "        [ 6.5970e-01,  9.8740e-01],\n",
       "        [-4.3048e-01, -1.0394e+00],\n",
       "        [ 4.3048e-01,  1.0394e+00],\n",
       "        [-2.0724e-01, -1.0421e+00],\n",
       "        [ 2.0724e-01,  1.0421e+00],\n",
       "        [ 4.0000e-05, -1.0000e+00],\n",
       "        [-4.0000e-05,  1.0000e+00],\n",
       "        [ 1.8293e-01, -9.1948e-01],\n",
       "        [-1.8293e-01,  9.1948e-01],\n",
       "        [ 3.3488e-01, -8.0838e-01],\n",
       "        [-3.3488e-01,  8.0838e-01],\n",
       "        [ 4.5143e-01, -6.7555e-01],\n",
       "        [-4.5143e-01,  6.7555e-01],\n",
       "        [ 5.3035e-01, -5.3031e-01],\n",
       "        [-5.3035e-01,  5.3031e-01],\n",
       "        [ 5.7165e-01, -3.8193e-01],\n",
       "        [-5.7165e-01,  3.8193e-01],\n",
       "        [ 5.7744e-01, -2.3915e-01],\n",
       "        [-5.7744e-01,  2.3915e-01],\n",
       "        [ 5.5170e-01, -1.0971e-01],\n",
       "        [-5.5170e-01,  1.0971e-01],\n",
       "        [ 5.0000e-01,  2.0000e-05],\n",
       "        [-5.0000e-01, -2.0000e-05]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_input #this will be fed in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_target #this will be used for the output loss calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(full_input,full_target)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset,batch_size=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x17b2e91e0b8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset #transforming to speed training in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x17b2e91e080>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 6.5000e+00,  0.0000e+00],\n",
      "        [-6.5000e+00, -0.0000e+00],\n",
      "        [ 6.3138e+00,  1.2559e+00],\n",
      "        [-6.3138e+00, -1.2559e+00],\n",
      "        [ 5.8897e+00,  2.4396e+00],\n",
      "        [-5.8897e+00, -2.4396e+00],\n",
      "        [ 5.2487e+00,  3.5070e+00],\n",
      "        [-5.2487e+00, -3.5070e+00],\n",
      "        [ 4.4194e+00,  4.4194e+00],\n",
      "        [-4.4194e+00, -4.4194e+00],\n",
      "        [ 3.4376e+00,  5.1447e+00],\n",
      "        [-3.4376e+00, -5.1447e+00],\n",
      "        [ 2.3439e+00,  5.6588e+00],\n",
      "        [-2.3439e+00, -5.6588e+00],\n",
      "        [ 1.1827e+00,  5.9460e+00],\n",
      "        [-1.1827e+00, -5.9460e+00],\n",
      "        [-2.0000e-05,  6.0000e+00],\n",
      "        [ 2.0000e-05, -6.0000e+00],\n",
      "        [-1.1584e+00,  5.8234e+00],\n",
      "        [ 1.1584e+00, -5.8234e+00],\n",
      "        [-2.2483e+00,  5.4278e+00],\n",
      "        [ 2.2483e+00, -5.4278e+00],\n",
      "        [-3.2293e+00,  4.8329e+00],\n",
      "        [ 3.2293e+00, -4.8329e+00],\n",
      "        [-4.0659e+00,  4.0658e+00],\n",
      "        [ 4.0659e+00, -4.0658e+00],\n",
      "        [-4.7290e+00,  3.1598e+00],\n",
      "        [ 4.7290e+00, -3.1598e+00],\n",
      "        [-5.1968e+00,  2.1526e+00],\n",
      "        [ 5.1968e+00, -2.1526e+00],\n",
      "        [-5.4556e+00,  1.0852e+00],\n",
      "        [ 5.4556e+00, -1.0852e+00],\n",
      "        [-5.5000e+00, -4.0000e-05],\n",
      "        [ 5.5000e+00,  4.0000e-05],\n",
      "        [-5.3330e+00, -1.0609e+00],\n",
      "        [ 5.3330e+00,  1.0609e+00],\n",
      "        [-4.9658e+00, -2.0570e+00],\n",
      "        [ 4.9658e+00,  2.0570e+00],\n",
      "        [-4.4172e+00, -2.9515e+00],\n",
      "        [ 4.4172e+00,  2.9515e+00],\n",
      "        [-3.7123e+00, -3.7123e+00],\n",
      "        [ 3.7123e+00,  3.7123e+00],\n",
      "        [-2.8820e+00, -4.3133e+00],\n",
      "        [ 2.8820e+00,  4.3133e+00],\n",
      "        [-1.9612e+00, -4.7349e+00],\n",
      "        [ 1.9612e+00,  4.7349e+00],\n",
      "        [-9.8759e-01, -4.9652e+00],\n",
      "        [ 9.8759e-01,  4.9652e+00],\n",
      "        [ 6.0000e-05, -5.0000e+00],\n",
      "        [-6.0000e-05,  5.0000e+00],\n",
      "        [ 9.6331e-01, -4.8426e+00],\n",
      "        [-9.6331e-01,  4.8426e+00],\n",
      "        [ 1.8656e+00, -4.5039e+00],\n",
      "        [-1.8656e+00,  4.5039e+00],\n",
      "        [ 2.6737e+00, -4.0014e+00],\n",
      "        [-2.6737e+00,  4.0014e+00],\n",
      "        [ 3.3588e+00, -3.3587e+00],\n",
      "        [-3.3588e+00,  3.3587e+00],\n",
      "        [ 3.8976e+00, -2.6042e+00],\n",
      "        [-3.8976e+00,  2.6042e+00],\n",
      "        [ 4.2730e+00, -1.7699e+00],\n",
      "        [-4.2730e+00,  1.7699e+00],\n",
      "        [ 4.4749e+00, -8.9004e-01],\n",
      "        [-4.4749e+00,  8.9004e-01],\n",
      "        [ 4.5000e+00,  7.0000e-05],\n",
      "        [-4.5000e+00, -7.0000e-05],\n",
      "        [ 4.3522e+00,  8.6578e-01],\n",
      "        [-4.3522e+00, -8.6578e-01],\n",
      "        [ 4.0420e+00,  1.6743e+00],\n",
      "        [-4.0420e+00, -1.6743e+00],\n",
      "        [ 3.5857e+00,  2.3960e+00],\n",
      "        [-3.5857e+00, -2.3960e+00],\n",
      "        [ 3.0052e+00,  3.0052e+00],\n",
      "        [-3.0052e+00, -3.0052e+00],\n",
      "        [ 2.3264e+00,  3.4818e+00],\n",
      "        [-2.3264e+00, -3.4818e+00],\n",
      "        [ 1.5785e+00,  3.8110e+00],\n",
      "        [-1.5785e+00, -3.8110e+00],\n",
      "        [ 7.9248e-01,  3.9845e+00],\n",
      "        [-7.9248e-01, -3.9845e+00],\n",
      "        [-7.0000e-05,  4.0000e+00],\n",
      "        [ 7.0000e-05, -4.0000e+00],\n",
      "        [-7.6824e-01,  3.8618e+00],\n",
      "        [ 7.6824e-01, -3.8618e+00],\n",
      "        [-1.4830e+00,  3.5800e+00],\n",
      "        [ 1.4830e+00, -3.5800e+00],\n",
      "        [-2.1182e+00,  3.1699e+00],\n",
      "        [ 2.1182e+00, -3.1699e+00],\n",
      "        [-2.6517e+00,  2.6516e+00],\n",
      "        [ 2.6517e+00, -2.6516e+00],\n",
      "        [-3.0661e+00,  2.0486e+00],\n",
      "        [ 3.0661e+00, -2.0486e+00],\n",
      "        [-3.3491e+00,  1.3872e+00],\n",
      "        [ 3.3491e+00, -1.3872e+00],\n",
      "        [-3.4941e+00,  6.9493e-01],\n",
      "        [ 3.4941e+00, -6.9493e-01],\n",
      "        [-3.5000e+00, -8.0000e-05]])\n"
     ]
    }
   ],
   "source": [
    "for _, (data_feed,target) in enumerate(train_loader):\n",
    "    print('x:',data_feed)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.5000, 0.0000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.5000e+00, -6.5000e+00,  6.3138e+00, -6.3138e+00,  5.8897e+00,\n",
       "        -5.8897e+00,  5.2487e+00, -5.2487e+00,  4.4194e+00, -4.4194e+00,\n",
       "         3.4376e+00, -3.4376e+00,  2.3439e+00, -2.3439e+00,  1.1827e+00,\n",
       "        -1.1827e+00, -2.0000e-05,  2.0000e-05, -1.1584e+00,  1.1584e+00,\n",
       "        -2.2483e+00,  2.2483e+00, -3.2293e+00,  3.2293e+00, -4.0659e+00,\n",
       "         4.0659e+00, -4.7290e+00,  4.7290e+00, -5.1968e+00,  5.1968e+00,\n",
       "        -5.4556e+00,  5.4556e+00, -5.5000e+00,  5.5000e+00, -5.3330e+00,\n",
       "         5.3330e+00, -4.9658e+00,  4.9658e+00, -4.4172e+00,  4.4172e+00,\n",
       "        -3.7123e+00,  3.7123e+00, -2.8820e+00,  2.8820e+00, -1.9612e+00,\n",
       "         1.9612e+00, -9.8759e-01,  9.8759e-01,  6.0000e-05, -6.0000e-05,\n",
       "         9.6331e-01, -9.6331e-01,  1.8656e+00, -1.8656e+00,  2.6737e+00,\n",
       "        -2.6737e+00,  3.3588e+00, -3.3588e+00,  3.8976e+00, -3.8976e+00,\n",
       "         4.2730e+00, -4.2730e+00,  4.4749e+00, -4.4749e+00,  4.5000e+00,\n",
       "        -4.5000e+00,  4.3522e+00, -4.3522e+00,  4.0420e+00, -4.0420e+00,\n",
       "         3.5857e+00, -3.5857e+00,  3.0052e+00, -3.0052e+00,  2.3264e+00,\n",
       "        -2.3264e+00,  1.5785e+00, -1.5785e+00,  7.9248e-01, -7.9248e-01,\n",
       "        -7.0000e-05,  7.0000e-05, -7.6824e-01,  7.6824e-01, -1.4830e+00,\n",
       "         1.4830e+00, -2.1182e+00,  2.1182e+00, -2.6517e+00,  2.6517e+00,\n",
       "        -3.0661e+00,  3.0661e+00, -3.3491e+00,  3.3491e+00, -3.4941e+00,\n",
       "         3.4941e+00, -3.5000e+00])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_feed[:,0] # x vector\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00, -0.0000e+00,  1.2559e+00, -1.2559e+00,  2.4396e+00,\n",
       "        -2.4396e+00,  3.5070e+00, -3.5070e+00,  4.4194e+00, -4.4194e+00,\n",
       "         5.1447e+00, -5.1447e+00,  5.6588e+00, -5.6588e+00,  5.9460e+00,\n",
       "        -5.9460e+00,  6.0000e+00, -6.0000e+00,  5.8234e+00, -5.8234e+00,\n",
       "         5.4278e+00, -5.4278e+00,  4.8329e+00, -4.8329e+00,  4.0658e+00,\n",
       "        -4.0658e+00,  3.1598e+00, -3.1598e+00,  2.1526e+00, -2.1526e+00,\n",
       "         1.0852e+00, -1.0852e+00, -4.0000e-05,  4.0000e-05, -1.0609e+00,\n",
       "         1.0609e+00, -2.0570e+00,  2.0570e+00, -2.9515e+00,  2.9515e+00,\n",
       "        -3.7123e+00,  3.7123e+00, -4.3133e+00,  4.3133e+00, -4.7349e+00,\n",
       "         4.7349e+00, -4.9652e+00,  4.9652e+00, -5.0000e+00,  5.0000e+00,\n",
       "        -4.8426e+00,  4.8426e+00, -4.5039e+00,  4.5039e+00, -4.0014e+00,\n",
       "         4.0014e+00, -3.3587e+00,  3.3587e+00, -2.6042e+00,  2.6042e+00,\n",
       "        -1.7699e+00,  1.7699e+00, -8.9004e-01,  8.9004e-01,  7.0000e-05,\n",
       "        -7.0000e-05,  8.6578e-01, -8.6578e-01,  1.6743e+00, -1.6743e+00,\n",
       "         2.3960e+00, -2.3960e+00,  3.0052e+00, -3.0052e+00,  3.4818e+00,\n",
       "        -3.4818e+00,  3.8110e+00, -3.8110e+00,  3.9845e+00, -3.9845e+00,\n",
       "         4.0000e+00, -4.0000e+00,  3.8618e+00, -3.8618e+00,  3.5800e+00,\n",
       "        -3.5800e+00,  3.1699e+00, -3.1699e+00,  2.6516e+00, -2.6516e+00,\n",
       "         2.0486e+00, -2.0486e+00,  1.3872e+00, -1.3872e+00,  6.9493e-01,\n",
       "        -6.9493e-01, -8.0000e-05])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data_feed[:,1] # y vector \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.atan2(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.sqrt(x*x + y*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.zeros(data_feed.shape,dtype=torch.float32)\n",
    "out[:,0] = a\n",
    "out[:,1] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  6.5000e+00],\n",
       "        [-3.1416e+00,  6.5000e+00],\n",
       "        [ 1.9635e-01,  6.4375e+00],\n",
       "        [-2.9452e+00,  6.4375e+00],\n",
       "        [ 3.9270e-01,  6.3750e+00],\n",
       "        [-2.7489e+00,  6.3750e+00],\n",
       "        [ 5.8905e-01,  6.3125e+00],\n",
       "        [-2.5525e+00,  6.3125e+00],\n",
       "        [ 7.8540e-01,  6.2500e+00],\n",
       "        [-2.3562e+00,  6.2500e+00],\n",
       "        [ 9.8175e-01,  6.1875e+00],\n",
       "        [-2.1598e+00,  6.1875e+00],\n",
       "        [ 1.1781e+00,  6.1250e+00],\n",
       "        [-1.9635e+00,  6.1250e+00],\n",
       "        [ 1.3744e+00,  6.0625e+00],\n",
       "        [-1.7671e+00,  6.0625e+00],\n",
       "        [ 1.5708e+00,  6.0000e+00],\n",
       "        [-1.5708e+00,  6.0000e+00],\n",
       "        [ 1.7671e+00,  5.9375e+00],\n",
       "        [-1.3744e+00,  5.9375e+00],\n",
       "        [ 1.9635e+00,  5.8750e+00],\n",
       "        [-1.1781e+00,  5.8750e+00],\n",
       "        [ 2.1599e+00,  5.8125e+00],\n",
       "        [-9.8174e-01,  5.8125e+00],\n",
       "        [ 2.3562e+00,  5.7500e+00],\n",
       "        [-7.8539e-01,  5.7500e+00],\n",
       "        [ 2.5525e+00,  5.6875e+00],\n",
       "        [-5.8904e-01,  5.6875e+00],\n",
       "        [ 2.7489e+00,  5.6250e+00],\n",
       "        [-3.9269e-01,  5.6250e+00],\n",
       "        [ 2.9453e+00,  5.5625e+00],\n",
       "        [-1.9634e-01,  5.5625e+00],\n",
       "        [-3.1416e+00,  5.5000e+00],\n",
       "        [ 7.2727e-06,  5.5000e+00],\n",
       "        [-2.9452e+00,  5.4375e+00],\n",
       "        [ 1.9636e-01,  5.4375e+00],\n",
       "        [-2.7489e+00,  5.3750e+00],\n",
       "        [ 3.9271e-01,  5.3750e+00],\n",
       "        [-2.5525e+00,  5.3125e+00],\n",
       "        [ 5.8906e-01,  5.3125e+00],\n",
       "        [-2.3562e+00,  5.2500e+00],\n",
       "        [ 7.8541e-01,  5.2500e+00],\n",
       "        [-2.1598e+00,  5.1875e+00],\n",
       "        [ 9.8176e-01,  5.1875e+00],\n",
       "        [-1.9635e+00,  5.1250e+00],\n",
       "        [ 1.1781e+00,  5.1250e+00],\n",
       "        [-1.7671e+00,  5.0625e+00],\n",
       "        [ 1.3745e+00,  5.0625e+00],\n",
       "        [-1.5708e+00,  5.0000e+00],\n",
       "        [ 1.5708e+00,  5.0000e+00],\n",
       "        [-1.3744e+00,  4.9375e+00],\n",
       "        [ 1.7672e+00,  4.9375e+00],\n",
       "        [-1.1781e+00,  4.8750e+00],\n",
       "        [ 1.9635e+00,  4.8750e+00],\n",
       "        [-9.8174e-01,  4.8125e+00],\n",
       "        [ 2.1599e+00,  4.8125e+00],\n",
       "        [-7.8538e-01,  4.7500e+00],\n",
       "        [ 2.3562e+00,  4.7500e+00],\n",
       "        [-5.8903e-01,  4.6875e+00],\n",
       "        [ 2.5526e+00,  4.6875e+00],\n",
       "        [-3.9268e-01,  4.6250e+00],\n",
       "        [ 2.7489e+00,  4.6250e+00],\n",
       "        [-1.9634e-01,  4.5625e+00],\n",
       "        [ 2.9453e+00,  4.5625e+00],\n",
       "        [ 1.5556e-05,  4.5000e+00],\n",
       "        [-3.1416e+00,  4.5000e+00],\n",
       "        [ 1.9636e-01,  4.4375e+00],\n",
       "        [-2.9452e+00,  4.4375e+00],\n",
       "        [ 3.9271e-01,  4.3750e+00],\n",
       "        [-2.7489e+00,  4.3750e+00],\n",
       "        [ 5.8906e-01,  4.3125e+00],\n",
       "        [-2.5525e+00,  4.3125e+00],\n",
       "        [ 7.8541e-01,  4.2500e+00],\n",
       "        [-2.3562e+00,  4.2500e+00],\n",
       "        [ 9.8177e-01,  4.1875e+00],\n",
       "        [-2.1598e+00,  4.1875e+00],\n",
       "        [ 1.1781e+00,  4.1250e+00],\n",
       "        [-1.9635e+00,  4.1250e+00],\n",
       "        [ 1.3745e+00,  4.0625e+00],\n",
       "        [-1.7671e+00,  4.0625e+00],\n",
       "        [ 1.5708e+00,  4.0000e+00],\n",
       "        [-1.5708e+00,  4.0000e+00],\n",
       "        [ 1.7672e+00,  3.9375e+00],\n",
       "        [-1.3744e+00,  3.9375e+00],\n",
       "        [ 1.9635e+00,  3.8750e+00],\n",
       "        [-1.1781e+00,  3.8750e+00],\n",
       "        [ 2.1599e+00,  3.8125e+00],\n",
       "        [-9.8173e-01,  3.8125e+00],\n",
       "        [ 2.3562e+00,  3.7500e+00],\n",
       "        [-7.8538e-01,  3.7500e+00],\n",
       "        [ 2.5526e+00,  3.6875e+00],\n",
       "        [-5.8903e-01,  3.6875e+00],\n",
       "        [ 2.7489e+00,  3.6250e+00],\n",
       "        [-3.9268e-01,  3.6250e+00],\n",
       "        [ 2.9453e+00,  3.5625e+00],\n",
       "        [-1.9633e-01,  3.5625e+00],\n",
       "        [-3.1416e+00,  3.5000e+00]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = torch.arange(start=-7,end=7.1,step=0.01,dtype=torch.float32)\n",
    "yrange = torch.arange(start=-6.6,end=6.7,step=0.01,dtype=torch.float32)\n",
    "xcoord = xrange.repeat(yrange.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0000, -6.9900, -6.9800,  ...,  7.0700,  7.0800,  7.0900])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.6000, -6.5900, -6.5800,  ...,  6.6700,  6.6800,  6.6900])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1330])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrange.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0000],\n",
       "        [-6.9900],\n",
       "        [-6.9800],\n",
       "        ...,\n",
       "        [ 7.0700],\n",
       "        [ 7.0800],\n",
       "        [ 7.0900]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcoord.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcoord = xrange.repeat(yrange.size()[0])\n",
    "ycoord = torch.repeat_interleave(yrange, xrange.size()[0], dim=0)\n",
    "grid = torch.cat((xcoord.unsqueeze(1),ycoord.unsqueeze(1)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0000, -6.9900, -6.9800,  ...,  7.0700,  7.0800,  7.0900])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.6000, -6.6000, -6.6000,  ...,  6.6900,  6.6900,  6.6900])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ycoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0000, -6.6000],\n",
       "        [-6.9900, -6.6000],\n",
       "        [-6.9800, -6.6000],\n",
       "        ...,\n",
       "        [ 7.0700,  6.6900],\n",
       "        [ 7.0800,  6.6900],\n",
       "        [ 7.0900,  6.6900]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hidden layer\n",
    "#first define the class\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RawNet(torch.nn.Module):\n",
    "    def __init__(self, num_hid):\n",
    "        super(RawNet, self).__init__()\n",
    "        self.feed = nn.Linear(2,num_hid)\n",
    "        self.hid = nn.Linear(num_hid,num_hid)\n",
    "        self.out = nn.Linear(num_hid,1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.active1 = torch.tanh(self.feed(x))\n",
    "        self.active2 = torch.tanh(self.hid(self.active1))\n",
    "        self.hidlayer = [self.active1,self.active2]\n",
    "        output = torch.sigmoid(self.out(self.active2))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, optimizer):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for _, (data,target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()    # zero the gradients\n",
    "        output = net(data)       # apply network\n",
    "        loss = F.binary_cross_entropy(output,target)\n",
    "        loss.backward()          # compute gradients\n",
    "        optimizer.step()         # update weights\n",
    "        pred = (output >= 0.5).float()\n",
    "        correct += (pred == target).float().sum()\n",
    "        total += target.size()[0]\n",
    "        accuracy = 100*correct/total\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('ep:%5d loss: %6.4f acc: %5.2f' %\n",
    "             (epoch,loss.item(),accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.5000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-6.5000e+00, -0.0000e+00,  1.0000e+00],\n",
       "        [ 6.3138e+00,  1.2559e+00,  0.0000e+00],\n",
       "        [-6.3138e+00, -1.2559e+00,  1.0000e+00],\n",
       "        [ 5.8897e+00,  2.4396e+00,  0.0000e+00],\n",
       "        [-5.8897e+00, -2.4396e+00,  1.0000e+00],\n",
       "        [ 5.2487e+00,  3.5070e+00,  0.0000e+00],\n",
       "        [-5.2487e+00, -3.5070e+00,  1.0000e+00],\n",
       "        [ 4.4194e+00,  4.4194e+00,  0.0000e+00],\n",
       "        [-4.4194e+00, -4.4194e+00,  1.0000e+00],\n",
       "        [ 3.4376e+00,  5.1447e+00,  0.0000e+00],\n",
       "        [-3.4376e+00, -5.1447e+00,  1.0000e+00],\n",
       "        [ 2.3439e+00,  5.6588e+00,  0.0000e+00],\n",
       "        [-2.3439e+00, -5.6588e+00,  1.0000e+00],\n",
       "        [ 1.1827e+00,  5.9460e+00,  0.0000e+00],\n",
       "        [-1.1827e+00, -5.9460e+00,  1.0000e+00],\n",
       "        [-2.0000e-05,  6.0000e+00,  0.0000e+00],\n",
       "        [ 2.0000e-05, -6.0000e+00,  1.0000e+00],\n",
       "        [-1.1584e+00,  5.8234e+00,  0.0000e+00],\n",
       "        [ 1.1584e+00, -5.8234e+00,  1.0000e+00],\n",
       "        [-2.2483e+00,  5.4278e+00,  0.0000e+00],\n",
       "        [ 2.2483e+00, -5.4278e+00,  1.0000e+00],\n",
       "        [-3.2293e+00,  4.8329e+00,  0.0000e+00],\n",
       "        [ 3.2293e+00, -4.8329e+00,  1.0000e+00],\n",
       "        [-4.0659e+00,  4.0658e+00,  0.0000e+00],\n",
       "        [ 4.0659e+00, -4.0658e+00,  1.0000e+00],\n",
       "        [-4.7290e+00,  3.1598e+00,  0.0000e+00],\n",
       "        [ 4.7290e+00, -3.1598e+00,  1.0000e+00],\n",
       "        [-5.1968e+00,  2.1526e+00,  0.0000e+00],\n",
       "        [ 5.1968e+00, -2.1526e+00,  1.0000e+00],\n",
       "        [-5.4556e+00,  1.0852e+00,  0.0000e+00],\n",
       "        [ 5.4556e+00, -1.0852e+00,  1.0000e+00],\n",
       "        [-5.5000e+00, -4.0000e-05,  0.0000e+00],\n",
       "        [ 5.5000e+00,  4.0000e-05,  1.0000e+00],\n",
       "        [-5.3330e+00, -1.0609e+00,  0.0000e+00],\n",
       "        [ 5.3330e+00,  1.0609e+00,  1.0000e+00],\n",
       "        [-4.9658e+00, -2.0570e+00,  0.0000e+00],\n",
       "        [ 4.9658e+00,  2.0570e+00,  1.0000e+00],\n",
       "        [-4.4172e+00, -2.9515e+00,  0.0000e+00],\n",
       "        [ 4.4172e+00,  2.9515e+00,  1.0000e+00],\n",
       "        [-3.7123e+00, -3.7123e+00,  0.0000e+00],\n",
       "        [ 3.7123e+00,  3.7123e+00,  1.0000e+00],\n",
       "        [-2.8820e+00, -4.3133e+00,  0.0000e+00],\n",
       "        [ 2.8820e+00,  4.3133e+00,  1.0000e+00],\n",
       "        [-1.9612e+00, -4.7349e+00,  0.0000e+00],\n",
       "        [ 1.9612e+00,  4.7349e+00,  1.0000e+00],\n",
       "        [-9.8759e-01, -4.9652e+00,  0.0000e+00],\n",
       "        [ 9.8759e-01,  4.9652e+00,  1.0000e+00],\n",
       "        [ 6.0000e-05, -5.0000e+00,  0.0000e+00],\n",
       "        [-6.0000e-05,  5.0000e+00,  1.0000e+00],\n",
       "        [ 9.6331e-01, -4.8426e+00,  0.0000e+00],\n",
       "        [-9.6331e-01,  4.8426e+00,  1.0000e+00],\n",
       "        [ 1.8656e+00, -4.5039e+00,  0.0000e+00],\n",
       "        [-1.8656e+00,  4.5039e+00,  1.0000e+00],\n",
       "        [ 2.6737e+00, -4.0014e+00,  0.0000e+00],\n",
       "        [-2.6737e+00,  4.0014e+00,  1.0000e+00],\n",
       "        [ 3.3588e+00, -3.3587e+00,  0.0000e+00],\n",
       "        [-3.3588e+00,  3.3587e+00,  1.0000e+00],\n",
       "        [ 3.8976e+00, -2.6042e+00,  0.0000e+00],\n",
       "        [-3.8976e+00,  2.6042e+00,  1.0000e+00],\n",
       "        [ 4.2730e+00, -1.7699e+00,  0.0000e+00],\n",
       "        [-4.2730e+00,  1.7699e+00,  1.0000e+00],\n",
       "        [ 4.4749e+00, -8.9004e-01,  0.0000e+00],\n",
       "        [-4.4749e+00,  8.9004e-01,  1.0000e+00],\n",
       "        [ 4.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-4.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 4.3522e+00,  8.6578e-01,  0.0000e+00],\n",
       "        [-4.3522e+00, -8.6578e-01,  1.0000e+00],\n",
       "        [ 4.0420e+00,  1.6743e+00,  0.0000e+00],\n",
       "        [-4.0420e+00, -1.6743e+00,  1.0000e+00],\n",
       "        [ 3.5857e+00,  2.3960e+00,  0.0000e+00],\n",
       "        [-3.5857e+00, -2.3960e+00,  1.0000e+00],\n",
       "        [ 3.0052e+00,  3.0052e+00,  0.0000e+00],\n",
       "        [-3.0052e+00, -3.0052e+00,  1.0000e+00],\n",
       "        [ 2.3264e+00,  3.4818e+00,  0.0000e+00],\n",
       "        [-2.3264e+00, -3.4818e+00,  1.0000e+00],\n",
       "        [ 1.5785e+00,  3.8110e+00,  0.0000e+00],\n",
       "        [-1.5785e+00, -3.8110e+00,  1.0000e+00],\n",
       "        [ 7.9248e-01,  3.9845e+00,  0.0000e+00],\n",
       "        [-7.9248e-01, -3.9845e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  4.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -4.0000e+00,  1.0000e+00],\n",
       "        [-7.6824e-01,  3.8618e+00,  0.0000e+00],\n",
       "        [ 7.6824e-01, -3.8618e+00,  1.0000e+00],\n",
       "        [-1.4830e+00,  3.5800e+00,  0.0000e+00],\n",
       "        [ 1.4830e+00, -3.5800e+00,  1.0000e+00],\n",
       "        [-2.1182e+00,  3.1699e+00,  0.0000e+00],\n",
       "        [ 2.1182e+00, -3.1699e+00,  1.0000e+00],\n",
       "        [-2.6517e+00,  2.6516e+00,  0.0000e+00],\n",
       "        [ 2.6517e+00, -2.6516e+00,  1.0000e+00],\n",
       "        [-3.0661e+00,  2.0486e+00,  0.0000e+00],\n",
       "        [ 3.0661e+00, -2.0486e+00,  1.0000e+00],\n",
       "        [-3.3491e+00,  1.3872e+00,  0.0000e+00],\n",
       "        [ 3.3491e+00, -1.3872e+00,  1.0000e+00],\n",
       "        [-3.4941e+00,  6.9493e-01,  0.0000e+00],\n",
       "        [ 3.4941e+00, -6.9493e-01,  1.0000e+00],\n",
       "        [-3.5000e+00, -8.0000e-05,  0.0000e+00],\n",
       "        [ 3.5000e+00,  8.0000e-05,  1.0000e+00],\n",
       "        [-3.3714e+00, -6.7070e-01,  0.0000e+00],\n",
       "        [ 3.3714e+00,  6.7070e-01,  1.0000e+00],\n",
       "        [-3.1181e+00, -1.2916e+00,  0.0000e+00],\n",
       "        [ 3.1181e+00,  1.2916e+00,  1.0000e+00],\n",
       "        [-2.7542e+00, -1.8404e+00,  0.0000e+00],\n",
       "        [ 2.7542e+00,  1.8404e+00,  1.0000e+00],\n",
       "        [-2.2980e+00, -2.2982e+00,  0.0000e+00],\n",
       "        [ 2.2980e+00,  2.2982e+00,  1.0000e+00],\n",
       "        [-1.7708e+00, -2.6504e+00,  0.0000e+00],\n",
       "        [ 1.7708e+00,  2.6504e+00,  1.0000e+00],\n",
       "        [-1.1958e+00, -2.8872e+00,  0.0000e+00],\n",
       "        [ 1.1958e+00,  2.8872e+00,  1.0000e+00],\n",
       "        [-5.9739e-01, -3.0037e+00,  0.0000e+00],\n",
       "        [ 5.9739e-01,  3.0037e+00,  1.0000e+00],\n",
       "        [ 8.0000e-05, -3.0000e+00,  0.0000e+00],\n",
       "        [-8.0000e-05,  3.0000e+00,  1.0000e+00],\n",
       "        [ 5.7315e-01, -2.8810e+00,  0.0000e+00],\n",
       "        [-5.7315e-01,  2.8810e+00,  1.0000e+00],\n",
       "        [ 1.1003e+00, -2.6561e+00,  0.0000e+00],\n",
       "        [-1.1003e+00,  2.6561e+00,  1.0000e+00],\n",
       "        [ 1.5626e+00, -2.3385e+00,  0.0000e+00],\n",
       "        [-1.5626e+00,  2.3385e+00,  1.0000e+00],\n",
       "        [ 1.9446e+00, -1.9445e+00,  0.0000e+00],\n",
       "        [-1.9446e+00,  1.9445e+00,  1.0000e+00],\n",
       "        [ 2.2346e+00, -1.4930e+00,  0.0000e+00],\n",
       "        [-2.2346e+00,  1.4930e+00,  1.0000e+00],\n",
       "        [ 2.4252e+00, -1.0045e+00,  0.0000e+00],\n",
       "        [-2.4252e+00,  1.0045e+00,  1.0000e+00],\n",
       "        [ 2.5133e+00, -4.9985e-01,  0.0000e+00],\n",
       "        [-2.5133e+00,  4.9985e-01,  1.0000e+00],\n",
       "        [ 2.5000e+00,  7.0000e-05,  0.0000e+00],\n",
       "        [-2.5000e+00, -7.0000e-05,  1.0000e+00],\n",
       "        [ 2.3907e+00,  4.7560e-01,  0.0000e+00],\n",
       "        [-2.3907e+00, -4.7560e-01,  1.0000e+00],\n",
       "        [ 2.1942e+00,  9.0894e-01,  0.0000e+00],\n",
       "        [-2.1942e+00, -9.0894e-01,  1.0000e+00],\n",
       "        [ 1.9227e+00,  1.2848e+00,  0.0000e+00],\n",
       "        [-1.9227e+00, -1.2848e+00,  1.0000e+00],\n",
       "        [ 1.5909e+00,  1.5910e+00,  0.0000e+00],\n",
       "        [-1.5909e+00, -1.5910e+00,  1.0000e+00],\n",
       "        [ 1.2153e+00,  1.8189e+00,  0.0000e+00],\n",
       "        [-1.2153e+00, -1.8189e+00,  1.0000e+00],\n",
       "        [ 8.1314e-01,  1.9633e+00,  0.0000e+00],\n",
       "        [-8.1314e-01, -1.9633e+00,  1.0000e+00],\n",
       "        [ 4.0231e-01,  2.0229e+00,  0.0000e+00],\n",
       "        [-4.0231e-01, -2.0229e+00,  1.0000e+00],\n",
       "        [-7.0000e-05,  2.0000e+00,  0.0000e+00],\n",
       "        [ 7.0000e-05, -2.0000e+00,  1.0000e+00],\n",
       "        [-3.7805e-01,  1.9003e+00,  0.0000e+00],\n",
       "        [ 3.7805e-01, -1.9003e+00,  1.0000e+00],\n",
       "        [-7.1759e-01,  1.7322e+00,  0.0000e+00],\n",
       "        [ 7.1759e-01, -1.7322e+00,  1.0000e+00],\n",
       "        [-1.0070e+00,  1.5070e+00,  0.0000e+00],\n",
       "        [ 1.0070e+00, -1.5070e+00,  1.0000e+00],\n",
       "        [-1.2375e+00,  1.2374e+00,  0.0000e+00],\n",
       "        [ 1.2375e+00, -1.2374e+00,  1.0000e+00],\n",
       "        [-1.4031e+00,  9.3748e-01,  0.0000e+00],\n",
       "        [ 1.4031e+00, -9.3748e-01,  1.0000e+00],\n",
       "        [-1.5013e+00,  6.2181e-01,  0.0000e+00],\n",
       "        [ 1.5013e+00, -6.2181e-01,  1.0000e+00],\n",
       "        [-1.5325e+00,  3.0477e-01,  0.0000e+00],\n",
       "        [ 1.5325e+00, -3.0477e-01,  1.0000e+00],\n",
       "        [-1.5000e+00, -6.0000e-05,  0.0000e+00],\n",
       "        [ 1.5000e+00,  6.0000e-05,  1.0000e+00],\n",
       "        [-1.4099e+00, -2.8049e-01,  0.0000e+00],\n",
       "        [ 1.4099e+00,  2.8049e-01,  1.0000e+00],\n",
       "        [-1.2703e+00, -5.2624e-01,  0.0000e+00],\n",
       "        [ 1.2703e+00,  5.2624e-01,  1.0000e+00],\n",
       "        [-1.0913e+00, -7.2923e-01,  0.0000e+00],\n",
       "        [ 1.0913e+00,  7.2923e-01,  1.0000e+00],\n",
       "        [-8.8385e-01, -8.8392e-01,  0.0000e+00],\n",
       "        [ 8.8385e-01,  8.8392e-01,  1.0000e+00],\n",
       "        [-6.5970e-01, -9.8740e-01,  0.0000e+00],\n",
       "        [ 6.5970e-01,  9.8740e-01,  1.0000e+00],\n",
       "        [-4.3048e-01, -1.0394e+00,  0.0000e+00],\n",
       "        [ 4.3048e-01,  1.0394e+00,  1.0000e+00],\n",
       "        [-2.0724e-01, -1.0421e+00,  0.0000e+00],\n",
       "        [ 2.0724e-01,  1.0421e+00,  1.0000e+00],\n",
       "        [ 4.0000e-05, -1.0000e+00,  0.0000e+00],\n",
       "        [-4.0000e-05,  1.0000e+00,  1.0000e+00],\n",
       "        [ 1.8293e-01, -9.1948e-01,  0.0000e+00],\n",
       "        [-1.8293e-01,  9.1948e-01,  1.0000e+00],\n",
       "        [ 3.3488e-01, -8.0838e-01,  0.0000e+00],\n",
       "        [-3.3488e-01,  8.0838e-01,  1.0000e+00],\n",
       "        [ 4.5143e-01, -6.7555e-01,  0.0000e+00],\n",
       "        [-4.5143e-01,  6.7555e-01,  1.0000e+00],\n",
       "        [ 5.3035e-01, -5.3031e-01,  0.0000e+00],\n",
       "        [-5.3035e-01,  5.3031e-01,  1.0000e+00],\n",
       "        [ 5.7165e-01, -3.8193e-01,  0.0000e+00],\n",
       "        [-5.7165e-01,  3.8193e-01,  1.0000e+00],\n",
       "        [ 5.7744e-01, -2.3915e-01,  0.0000e+00],\n",
       "        [-5.7744e-01,  2.3915e-01,  1.0000e+00],\n",
       "        [ 5.5170e-01, -1.0971e-01,  0.0000e+00],\n",
       "        [-5.5170e-01,  1.0971e-01,  1.0000e+00],\n",
       "        [ 5.0000e-01,  2.0000e-05,  0.0000e+00],\n",
       "        [-5.0000e-01, -2.0000e-05,  1.0000e+00]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_output(net):\n",
    "    xrange = torch.arange(start=-7,end=7.1,step=0.01,dtype=torch.float32)\n",
    "    yrange = torch.arange(start=-6.6,end=6.7,step=0.01,dtype=torch.float32)\n",
    "    xcoord = xrange.repeat(yrange.size()[0])\n",
    "    ycoord = torch.repeat_interleave(yrange, xrange.size()[0], dim=0)\n",
    "    grid = torch.cat((xcoord.unsqueeze(1),ycoord.unsqueeze(1)),1)\n",
    "    with torch.no_grad(): # suppress updating of gradients\n",
    "        net.eval()        # toggle batch norm, dropout\n",
    "        output = net(grid)\n",
    "        net.train() # toggle batch norm, dropout back again\n",
    "\n",
    "        pred = (output >= 0.5).float()\n",
    "\n",
    "        # plot function computed by model\n",
    "        plt.clf()\n",
    "        plt.pcolormesh(xrange,yrange,pred.cpu().view(yrange.size()[0],xrange.size()[0]), cmap='Wistia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hidden(net, layer, node):\n",
    "    print(\"node:\",node)\n",
    "    print(\"layer:\",layer)\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 1.0,\n",
       " 1.1,\n",
       " 1.2000000000000002,\n",
       " 1.3000000000000003,\n",
       " 1.4000000000000001,\n",
       " 1.5000000000000002,\n",
       " 1.6,\n",
       " 1.7000000000000002,\n",
       " 1.8000000000000003,\n",
       " 1.9000000000000001]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import arange\n",
    "p = [j for j in arange(.1,2,.1)]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:  100 loss: 0.7121 acc: 50.52\n",
      "ep:  200 loss: 0.7124 acc: 50.00\n",
      "ep:  300 loss: 0.7125 acc: 50.00\n",
      "ep:  400 loss: 0.7126 acc: 50.00\n",
      "ep:  500 loss: 0.7127 acc: 50.00\n",
      "ep:  600 loss: 0.7127 acc: 50.00\n",
      "ep:  700 loss: 0.7127 acc: 50.00\n",
      "ep:  800 loss: 0.7127 acc: 50.52\n",
      "ep:  900 loss: 0.7127 acc: 50.52\n",
      "ep: 1000 loss: 0.7127 acc: 50.52\n",
      "ep: 1100 loss: 0.7128 acc: 50.52\n",
      "ep: 1200 loss: 0.7128 acc: 50.52\n",
      "ep: 1300 loss: 0.7128 acc: 50.52\n",
      "ep: 1400 loss: 0.7128 acc: 50.52\n",
      "ep: 1500 loss: 0.7128 acc: 50.52\n",
      "ep: 1600 loss: 0.7128 acc: 50.52\n",
      "ep: 1700 loss: 0.7128 acc: 50.52\n",
      "ep: 1800 loss: 0.7128 acc: 50.52\n",
      "ep: 1900 loss: 0.7128 acc: 50.52\n",
      "ep: 2000 loss: 0.7128 acc: 50.52\n",
      "ep: 2100 loss: 0.7128 acc: 50.52\n",
      "ep: 2200 loss: 0.7128 acc: 50.52\n",
      "ep: 2300 loss: 0.7128 acc: 50.52\n",
      "ep: 2400 loss: 0.7128 acc: 50.52\n",
      "ep: 2500 loss: 0.7128 acc: 50.52\n",
      "ep: 2600 loss: 0.7128 acc: 50.52\n",
      "ep: 2700 loss: 0.7128 acc: 50.52\n",
      "ep: 2800 loss: 0.7128 acc: 50.52\n",
      "ep: 2900 loss: 0.7128 acc: 50.52\n",
      "ep: 3000 loss: 0.7128 acc: 50.52\n",
      "ep: 3100 loss: 0.7128 acc: 50.52\n",
      "ep: 3200 loss: 0.7128 acc: 50.52\n",
      "ep: 3300 loss: 0.7128 acc: 50.52\n",
      "ep: 3400 loss: 0.7128 acc: 50.52\n",
      "ep: 3500 loss: 0.7128 acc: 50.52\n",
      "ep: 3600 loss: 0.7128 acc: 50.52\n",
      "ep: 3700 loss: 0.7128 acc: 50.52\n",
      "ep: 3800 loss: 0.7128 acc: 50.52\n",
      "ep: 3900 loss: 0.7128 acc: 50.52\n",
      "ep: 4000 loss: 0.7128 acc: 50.52\n",
      "ep: 4100 loss: 0.7128 acc: 50.52\n",
      "ep: 4200 loss: 0.7128 acc: 50.52\n",
      "ep: 4300 loss: 0.7128 acc: 50.52\n",
      "ep: 4400 loss: 0.7128 acc: 50.52\n",
      "ep: 4500 loss: 0.7128 acc: 50.52\n",
      "ep: 4600 loss: 0.7128 acc: 50.52\n",
      "ep: 4700 loss: 0.7128 acc: 50.52\n",
      "ep: 4800 loss: 0.7128 acc: 50.52\n",
      "ep: 4900 loss: 0.7128 acc: 50.52\n",
      "ep: 5000 loss: 0.7128 acc: 50.52\n",
      "ep: 5100 loss: 0.7128 acc: 50.52\n",
      "ep: 5200 loss: 0.7128 acc: 50.52\n",
      "ep: 5300 loss: 0.7128 acc: 50.52\n",
      "ep: 5400 loss: 0.7128 acc: 50.52\n",
      "ep: 5500 loss: 0.7128 acc: 50.52\n",
      "ep: 5600 loss: 0.7128 acc: 50.52\n",
      "ep: 5700 loss: 0.7128 acc: 50.52\n",
      "ep: 5800 loss: 0.7128 acc: 50.52\n",
      "ep: 5900 loss: 0.7128 acc: 50.52\n",
      "ep: 6000 loss: 0.7128 acc: 50.52\n",
      "ep: 6100 loss: 0.7128 acc: 50.52\n",
      "ep: 6200 loss: 0.7128 acc: 50.52\n",
      "ep: 6300 loss: 0.7128 acc: 50.52\n",
      "ep: 6400 loss: 0.7128 acc: 50.52\n",
      "ep: 6500 loss: 0.7128 acc: 50.52\n",
      "ep: 6600 loss: 0.7128 acc: 50.52\n",
      "ep: 6700 loss: 0.7128 acc: 50.52\n",
      "ep: 6800 loss: 0.7128 acc: 50.52\n",
      "ep: 6900 loss: 0.7128 acc: 50.52\n",
      "ep: 7000 loss: 0.7128 acc: 50.52\n",
      "ep: 7100 loss: 0.7128 acc: 50.52\n",
      "ep: 7200 loss: 0.7128 acc: 50.52\n",
      "ep: 7300 loss: 0.7128 acc: 50.52\n",
      "ep: 7400 loss: 0.7128 acc: 50.52\n",
      "ep: 7500 loss: 0.7128 acc: 50.52\n",
      "ep: 7600 loss: 0.7128 acc: 50.52\n",
      "ep: 7700 loss: 0.7128 acc: 50.52\n",
      "ep: 7800 loss: 0.7128 acc: 50.52\n",
      "ep: 7900 loss: 0.7128 acc: 50.52\n",
      "ep: 8000 loss: 0.7128 acc: 50.52\n",
      "ep: 8100 loss: 0.7128 acc: 50.52\n",
      "ep: 8200 loss: 0.7128 acc: 50.52\n",
      "ep: 8300 loss: 0.7128 acc: 50.52\n",
      "ep: 8400 loss: 0.7128 acc: 50.52\n",
      "ep: 8500 loss: 0.7128 acc: 50.52\n",
      "ep: 8600 loss: 0.7128 acc: 50.52\n",
      "ep: 8700 loss: 0.7128 acc: 50.52\n",
      "ep: 8800 loss: 0.7128 acc: 50.52\n",
      "ep: 8900 loss: 0.7128 acc: 50.52\n",
      "ep: 9000 loss: 0.7128 acc: 50.52\n",
      "ep: 9100 loss: 0.7128 acc: 50.52\n",
      "ep: 9200 loss: 0.7128 acc: 50.52\n",
      "ep: 9300 loss: 0.7128 acc: 50.52\n",
      "ep: 9400 loss: 0.7128 acc: 50.52\n",
      "ep: 9500 loss: 0.7128 acc: 50.52\n",
      "ep: 9600 loss: 0.7128 acc: 50.52\n",
      "ep: 9700 loss: 0.7128 acc: 50.52\n",
      "ep: 9800 loss: 0.7128 acc: 50.52\n",
      "ep: 9900 loss: 0.7128 acc: 50.52\n",
      "ep:10000 loss: 0.7128 acc: 50.52\n",
      "ep:10100 loss: 0.7128 acc: 50.52\n",
      "ep:10200 loss: 0.7128 acc: 50.52\n",
      "ep:10300 loss: 0.7128 acc: 50.52\n",
      "ep:10400 loss: 0.7128 acc: 50.52\n",
      "ep:10500 loss: 0.7128 acc: 50.52\n",
      "ep:10600 loss: 0.7128 acc: 50.52\n",
      "ep:10700 loss: 0.7128 acc: 50.52\n",
      "ep:10800 loss: 0.7128 acc: 50.52\n",
      "ep:10900 loss: 0.7128 acc: 50.52\n",
      "ep:11000 loss: 0.7128 acc: 50.52\n",
      "ep:11100 loss: 0.7128 acc: 50.52\n",
      "ep:11200 loss: 0.7128 acc: 50.52\n",
      "ep:11300 loss: 0.7128 acc: 50.52\n",
      "ep:11400 loss: 0.7128 acc: 50.52\n",
      "ep:11500 loss: 0.7128 acc: 50.52\n",
      "ep:11600 loss: 0.7128 acc: 50.52\n",
      "ep:11700 loss: 0.7128 acc: 50.52\n",
      "ep:11800 loss: 0.7128 acc: 50.52\n",
      "ep:11900 loss: 0.7128 acc: 50.52\n",
      "ep:12000 loss: 0.7128 acc: 50.52\n",
      "ep:12100 loss: 0.7128 acc: 50.52\n",
      "ep:12200 loss: 0.7128 acc: 50.52\n",
      "ep:12300 loss: 0.7128 acc: 50.52\n",
      "ep:12400 loss: 0.7128 acc: 50.52\n",
      "ep:12500 loss: 0.7128 acc: 50.52\n",
      "ep:12600 loss: 0.7128 acc: 50.52\n",
      "ep:12700 loss: 0.7128 acc: 50.52\n",
      "ep:12800 loss: 0.7128 acc: 50.52\n",
      "ep:12900 loss: 0.7128 acc: 50.52\n",
      "ep:13000 loss: 0.7128 acc: 50.52\n",
      "ep:13100 loss: 0.7128 acc: 50.52\n",
      "ep:13200 loss: 0.7128 acc: 50.52\n",
      "ep:13300 loss: 0.7128 acc: 50.52\n",
      "ep:13400 loss: 0.7128 acc: 50.52\n",
      "ep:13500 loss: 0.7128 acc: 50.52\n",
      "ep:13600 loss: 0.7128 acc: 50.52\n",
      "ep:13700 loss: 0.7128 acc: 50.52\n",
      "ep:13800 loss: 0.7128 acc: 50.52\n",
      "ep:13900 loss: 0.7128 acc: 50.52\n",
      "ep:14000 loss: 0.7128 acc: 50.52\n",
      "ep:14100 loss: 0.7128 acc: 50.52\n",
      "ep:14200 loss: 0.7128 acc: 50.52\n",
      "ep:14300 loss: 0.7128 acc: 50.52\n",
      "ep:14400 loss: 0.7128 acc: 50.52\n",
      "ep:14500 loss: 0.7128 acc: 50.52\n",
      "ep:14600 loss: 0.7128 acc: 50.52\n",
      "ep:14700 loss: 0.7128 acc: 50.52\n",
      "ep:14800 loss: 0.7128 acc: 50.52\n",
      "ep:14900 loss: 0.7128 acc: 50.52\n",
      "ep:15000 loss: 0.7128 acc: 50.52\n",
      "ep:15100 loss: 0.7128 acc: 50.52\n",
      "ep:15200 loss: 0.7128 acc: 50.52\n",
      "ep:15300 loss: 0.7128 acc: 50.52\n",
      "ep:15400 loss: 0.7128 acc: 50.52\n",
      "ep:15500 loss: 0.7128 acc: 50.52\n",
      "ep:15600 loss: 0.7128 acc: 50.52\n",
      "ep:15700 loss: 0.7128 acc: 50.52\n",
      "ep:15800 loss: 0.7128 acc: 50.52\n",
      "ep:15900 loss: 0.7128 acc: 50.52\n",
      "ep:16000 loss: 0.7128 acc: 50.52\n",
      "ep:16100 loss: 0.7128 acc: 50.52\n",
      "ep:16200 loss: 0.7128 acc: 50.52\n",
      "ep:16300 loss: 0.7128 acc: 50.52\n",
      "ep:16400 loss: 0.7128 acc: 50.52\n",
      "ep:16500 loss: 0.7128 acc: 50.52\n",
      "ep:16600 loss: 0.7128 acc: 50.52\n",
      "ep:16700 loss: 0.7128 acc: 50.52\n",
      "ep:16800 loss: 0.7128 acc: 50.52\n",
      "ep:16900 loss: 0.7128 acc: 50.52\n",
      "ep:17000 loss: 0.7128 acc: 50.52\n",
      "ep:17100 loss: 0.7128 acc: 50.52\n",
      "ep:17200 loss: 0.7128 acc: 50.52\n",
      "ep:17300 loss: 0.7128 acc: 50.52\n",
      "ep:17400 loss: 0.7128 acc: 50.52\n",
      "ep:17500 loss: 0.7128 acc: 50.52\n",
      "ep:17600 loss: 0.7128 acc: 50.52\n",
      "ep:17700 loss: 0.7128 acc: 50.52\n",
      "ep:17800 loss: 0.7128 acc: 50.52\n",
      "ep:17900 loss: 0.7128 acc: 50.52\n",
      "ep:18000 loss: 0.7128 acc: 50.52\n",
      "ep:18100 loss: 0.7128 acc: 50.52\n",
      "ep:18200 loss: 0.7128 acc: 50.52\n",
      "ep:18300 loss: 0.7128 acc: 50.52\n",
      "ep:18400 loss: 0.7128 acc: 50.52\n",
      "ep:18500 loss: 0.7128 acc: 50.52\n",
      "ep:18600 loss: 0.7128 acc: 50.52\n",
      "ep:18700 loss: 0.7128 acc: 50.52\n",
      "ep:18800 loss: 0.7128 acc: 50.52\n",
      "ep:18900 loss: 0.7128 acc: 50.52\n",
      "ep:19000 loss: 0.7128 acc: 50.52\n",
      "ep:19100 loss: 0.7128 acc: 50.52\n",
      "ep:19200 loss: 0.7128 acc: 50.52\n",
      "ep:19300 loss: 0.7128 acc: 50.52\n",
      "ep:19400 loss: 0.7128 acc: 50.52\n",
      "ep:19500 loss: 0.7128 acc: 50.52\n",
      "ep:19600 loss: 0.7128 acc: 50.52\n",
      "ep:19700 loss: 0.7128 acc: 50.52\n",
      "ep:19800 loss: 0.7128 acc: 50.52\n",
      "ep:19900 loss: 0.7128 acc: 50.52\n",
      "init:0.1, acc:50.515464782714844 , epoch:19999\n",
      "ep:  100 loss: 0.6882 acc: 61.86\n",
      "ep:  200 loss: 0.6843 acc: 63.40\n",
      "ep:  300 loss: 0.6794 acc: 63.92\n",
      "ep:  400 loss: 0.6705 acc: 64.95\n",
      "ep:  500 loss: 0.6609 acc: 60.31\n",
      "ep:  600 loss: 0.6529 acc: 60.82\n",
      "ep:  700 loss: 0.6451 acc: 59.28\n",
      "ep:  800 loss: 0.6315 acc: 58.25\n",
      "ep:  900 loss: 0.5636 acc: 56.19\n",
      "ep: 1000 loss: 0.4628 acc: 65.98\n",
      "ep: 1100 loss: 0.4375 acc: 65.98\n",
      "ep: 1200 loss: 0.4188 acc: 73.20\n",
      "ep: 1300 loss: 0.4112 acc: 73.20\n",
      "ep: 1400 loss: 0.4118 acc: 74.23\n",
      "ep: 1500 loss: 0.4160 acc: 74.74\n",
      "ep: 1600 loss: 0.4189 acc: 76.29\n",
      "ep: 1700 loss: 0.4193 acc: 76.29\n",
      "ep: 1800 loss: 0.4180 acc: 78.35\n",
      "ep: 1900 loss: 0.4154 acc: 78.87\n",
      "ep: 2000 loss: 0.4113 acc: 78.35\n",
      "ep: 2100 loss: 0.4053 acc: 77.32\n",
      "ep: 2200 loss: 0.3988 acc: 77.32\n",
      "ep: 2300 loss: 0.3896 acc: 78.35\n",
      "ep: 2400 loss: 0.3758 acc: 80.41\n",
      "ep: 2500 loss: 0.3577 acc: 82.47\n",
      "ep: 2600 loss: 0.3386 acc: 84.02\n",
      "ep: 2700 loss: 0.3225 acc: 84.02\n",
      "ep: 2800 loss: 0.3058 acc: 83.51\n",
      "ep: 2900 loss: 0.2879 acc: 84.02\n",
      "ep: 3000 loss: 0.2713 acc: 86.08\n",
      "ep: 3100 loss: 0.2565 acc: 86.60\n",
      "ep: 3200 loss: 0.2436 acc: 87.11\n",
      "ep: 3300 loss: 0.2325 acc: 87.63\n",
      "ep: 3400 loss: 0.2291 acc: 87.11\n",
      "ep: 3500 loss: 0.2207 acc: 87.63\n",
      "ep: 3600 loss: 0.2136 acc: 88.14\n",
      "ep: 3700 loss: 0.2075 acc: 88.14\n",
      "ep: 3800 loss: 0.2025 acc: 88.66\n",
      "ep: 3900 loss: 0.1966 acc: 89.69\n",
      "ep: 4000 loss: 0.1910 acc: 89.69\n",
      "ep: 4100 loss: 0.1865 acc: 88.66\n",
      "ep: 4200 loss: 0.1815 acc: 88.66\n",
      "ep: 4300 loss: 0.1769 acc: 89.18\n",
      "ep: 4400 loss: 0.1726 acc: 89.69\n",
      "ep: 4500 loss: 0.1603 acc: 92.27\n",
      "ep: 4600 loss: 0.1500 acc: 94.85\n",
      "ep: 4700 loss: 0.1411 acc: 96.39\n",
      "ep: 4800 loss: 0.1309 acc: 96.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4900 loss: 0.1231 acc: 95.88\n",
      "ep: 5000 loss: 0.1071 acc: 97.42\n",
      "ep: 5100 loss: 0.0992 acc: 96.91\n",
      "ep: 5200 loss: 0.0912 acc: 96.91\n",
      "ep: 5300 loss: 0.0830 acc: 97.42\n",
      "ep: 5400 loss: 0.0749 acc: 98.45\n",
      "ep: 5500 loss: 0.0685 acc: 99.48\n",
      "ep: 5600 loss: 0.0632 acc: 99.48\n",
      "Achieves 100%\n",
      "init:0.2, acc:100.0 , epoch:5622\n",
      "ep:  100 loss: 0.6887 acc: 55.67\n",
      "ep:  200 loss: 0.6619 acc: 59.79\n",
      "ep:  300 loss: 0.6286 acc: 54.64\n",
      "ep:  400 loss: 0.5473 acc: 62.89\n",
      "ep:  500 loss: 0.4615 acc: 64.43\n",
      "ep:  600 loss: 0.3674 acc: 69.07\n",
      "ep:  700 loss: 0.2951 acc: 71.13\n",
      "ep:  800 loss: 0.2629 acc: 71.13\n",
      "ep:  900 loss: 0.2416 acc: 72.16\n",
      "ep: 1000 loss: 0.2251 acc: 73.71\n",
      "ep: 1100 loss: 0.2081 acc: 73.20\n",
      "ep: 1200 loss: 0.1905 acc: 73.20\n",
      "ep: 1300 loss: 0.1753 acc: 73.71\n",
      "ep: 1400 loss: 0.1609 acc: 76.29\n",
      "ep: 1500 loss: 0.1479 acc: 76.80\n",
      "ep: 1600 loss: 0.1339 acc: 81.44\n",
      "ep: 1700 loss: 0.1216 acc: 84.02\n",
      "ep: 1800 loss: 0.1112 acc: 84.02\n",
      "ep: 1900 loss: 0.1045 acc: 84.02\n",
      "ep: 2000 loss: 0.1010 acc: 84.02\n",
      "ep: 2100 loss: 0.0983 acc: 84.54\n",
      "ep: 2200 loss: 0.0953 acc: 85.05\n",
      "ep: 2300 loss: 0.0929 acc: 84.02\n",
      "ep: 2400 loss: 0.0905 acc: 82.99\n",
      "ep: 2500 loss: 0.0880 acc: 84.02\n",
      "ep: 2600 loss: 0.0845 acc: 86.08\n",
      "ep: 2700 loss: 0.0756 acc: 86.60\n",
      "ep: 2800 loss: 0.0671 acc: 87.11\n",
      "ep: 2900 loss: 0.0591 acc: 87.11\n",
      "ep: 3000 loss: 0.0541 acc: 86.60\n",
      "ep: 3100 loss: 0.0506 acc: 87.11\n",
      "ep: 3200 loss: 0.0483 acc: 88.14\n",
      "ep: 3300 loss: 0.0467 acc: 89.18\n",
      "ep: 3400 loss: 0.0452 acc: 89.69\n",
      "ep: 3500 loss: 0.0425 acc: 88.66\n",
      "ep: 3600 loss: 0.0408 acc: 89.69\n",
      "ep: 3700 loss: 0.0397 acc: 89.69\n",
      "ep: 3800 loss: 0.0388 acc: 89.69\n",
      "ep: 3900 loss: 0.0383 acc: 89.69\n",
      "ep: 4000 loss: 0.0383 acc: 89.69\n",
      "ep: 4100 loss: 0.0399 acc: 90.21\n",
      "ep: 4200 loss: 0.0405 acc: 90.21\n",
      "ep: 4300 loss: 0.0400 acc: 90.21\n",
      "ep: 4400 loss: 0.0393 acc: 90.72\n",
      "ep: 4500 loss: 0.0386 acc: 90.72\n",
      "ep: 4600 loss: 0.0380 acc: 90.72\n",
      "ep: 4700 loss: 0.0373 acc: 90.72\n",
      "ep: 4800 loss: 0.0366 acc: 91.75\n",
      "ep: 4900 loss: 0.0359 acc: 91.75\n",
      "ep: 5000 loss: 0.0351 acc: 94.33\n",
      "ep: 5100 loss: 0.0336 acc: 95.36\n",
      "ep: 5200 loss: 0.0330 acc: 95.36\n",
      "ep: 5300 loss: 0.0337 acc: 95.36\n",
      "ep: 5400 loss: 0.0339 acc: 95.36\n",
      "ep: 5500 loss: 0.0338 acc: 95.88\n",
      "ep: 5600 loss: 0.0336 acc: 96.39\n",
      "init:0.30000000000000004, acc:95.87628936767578 , epoch:5621\n",
      "ep:  100 loss: 0.6790 acc: 60.82\n",
      "ep:  200 loss: 0.6514 acc: 59.28\n",
      "ep:  300 loss: 0.5877 acc: 57.22\n",
      "ep:  400 loss: 0.5097 acc: 62.89\n",
      "ep:  500 loss: 0.4399 acc: 64.43\n",
      "ep:  600 loss: 0.3756 acc: 70.62\n",
      "ep:  700 loss: 0.3272 acc: 73.71\n",
      "ep:  800 loss: 0.2990 acc: 75.77\n",
      "ep:  900 loss: 0.2745 acc: 78.35\n",
      "ep: 1000 loss: 0.2531 acc: 80.41\n",
      "ep: 1100 loss: 0.2372 acc: 80.93\n",
      "ep: 1200 loss: 0.2227 acc: 82.99\n",
      "ep: 1300 loss: 0.2099 acc: 84.54\n",
      "ep: 1400 loss: 0.2013 acc: 85.05\n",
      "ep: 1500 loss: 0.1936 acc: 85.57\n",
      "ep: 1600 loss: 0.1872 acc: 86.08\n",
      "ep: 1700 loss: 0.1823 acc: 85.57\n",
      "ep: 1800 loss: 0.1766 acc: 85.57\n",
      "ep: 1900 loss: 0.1716 acc: 86.60\n",
      "ep: 2000 loss: 0.1678 acc: 86.60\n",
      "ep: 2100 loss: 0.1646 acc: 87.11\n",
      "ep: 2200 loss: 0.1620 acc: 87.63\n",
      "ep: 2300 loss: 0.1595 acc: 87.63\n",
      "ep: 2400 loss: 0.1572 acc: 87.63\n",
      "ep: 2500 loss: 0.1548 acc: 87.63\n",
      "ep: 2600 loss: 0.1529 acc: 87.63\n",
      "ep: 2700 loss: 0.1517 acc: 88.66\n",
      "ep: 2800 loss: 0.1480 acc: 89.69\n",
      "ep: 2900 loss: 0.1443 acc: 90.21\n",
      "ep: 3000 loss: 0.1444 acc: 90.21\n",
      "ep: 3100 loss: 0.1361 acc: 91.75\n",
      "ep: 3200 loss: 0.1318 acc: 92.27\n",
      "ep: 3300 loss: 0.1272 acc: 91.75\n",
      "ep: 3400 loss: 0.1001 acc: 93.81\n",
      "ep: 3500 loss: 0.0940 acc: 94.33\n",
      "ep: 3600 loss: 0.0825 acc: 96.91\n",
      "ep: 3700 loss: 0.0746 acc: 97.42\n",
      "ep: 3800 loss: 0.0695 acc: 97.94\n",
      "ep: 3900 loss: 0.0643 acc: 98.45\n",
      "ep: 4000 loss: 0.0586 acc: 98.45\n",
      "ep: 4100 loss: 0.0534 acc: 98.97\n",
      "ep: 4200 loss: 0.0487 acc: 98.97\n",
      "ep: 4300 loss: 0.0445 acc: 98.97\n",
      "ep: 4400 loss: 0.0407 acc: 98.97\n",
      "ep: 4500 loss: 0.0372 acc: 98.97\n",
      "ep: 4600 loss: 0.0343 acc: 98.97\n",
      "ep: 4700 loss: 0.0319 acc: 98.97\n",
      "ep: 4800 loss: 0.0299 acc: 98.97\n",
      "ep: 4900 loss: 0.0282 acc: 98.97\n",
      "ep: 5000 loss: 0.0267 acc: 98.97\n",
      "ep: 5100 loss: 0.0254 acc: 98.97\n",
      "ep: 5200 loss: 0.0244 acc: 98.97\n",
      "ep: 5300 loss: 0.0235 acc: 98.97\n",
      "ep: 5400 loss: 0.0228 acc: 98.97\n",
      "ep: 5500 loss: 0.0221 acc: 98.97\n",
      "ep: 5600 loss: 0.0217 acc: 99.48\n",
      "init:0.4, acc:99.48453521728516 , epoch:5620\n",
      "ep:  100 loss: 0.6512 acc: 61.86\n",
      "ep:  200 loss: 0.5876 acc: 61.86\n",
      "ep:  300 loss: 0.5186 acc: 58.76\n",
      "ep:  400 loss: 0.4443 acc: 63.40\n",
      "ep:  500 loss: 0.3852 acc: 64.43\n",
      "ep:  600 loss: 0.3473 acc: 65.98\n",
      "ep:  700 loss: 0.3138 acc: 68.04\n",
      "ep:  800 loss: 0.2868 acc: 72.16\n",
      "ep:  900 loss: 0.2678 acc: 75.26\n",
      "ep: 1000 loss: 0.2539 acc: 76.29\n",
      "ep: 1100 loss: 0.2414 acc: 76.80\n",
      "ep: 1200 loss: 0.2293 acc: 81.44\n",
      "ep: 1300 loss: 0.2136 acc: 82.99\n",
      "ep: 1400 loss: 0.1996 acc: 82.47\n",
      "ep: 1500 loss: 0.1905 acc: 83.51\n",
      "ep: 1600 loss: 0.1833 acc: 83.51\n",
      "ep: 1700 loss: 0.1777 acc: 84.54\n",
      "ep: 1800 loss: 0.1739 acc: 85.57\n",
      "ep: 1900 loss: 0.1704 acc: 86.08\n",
      "ep: 2000 loss: 0.1664 acc: 86.08\n",
      "ep: 2100 loss: 0.1623 acc: 86.60\n",
      "ep: 2200 loss: 0.1582 acc: 87.63\n",
      "ep: 2300 loss: 0.1542 acc: 87.63\n",
      "ep: 2400 loss: 0.1504 acc: 88.14\n",
      "ep: 2500 loss: 0.1468 acc: 88.14\n",
      "ep: 2600 loss: 0.1435 acc: 88.66\n",
      "ep: 2700 loss: 0.1405 acc: 88.66\n",
      "ep: 2800 loss: 0.1378 acc: 88.66\n",
      "ep: 2900 loss: 0.1354 acc: 88.66\n",
      "ep: 3000 loss: 0.1332 acc: 88.66\n",
      "ep: 3100 loss: 0.1312 acc: 89.18\n",
      "ep: 3200 loss: 0.1294 acc: 89.18\n",
      "ep: 3300 loss: 0.1276 acc: 89.18\n",
      "ep: 3400 loss: 0.1260 acc: 89.18\n",
      "ep: 3500 loss: 0.1244 acc: 89.18\n",
      "ep: 3600 loss: 0.1229 acc: 89.18\n",
      "ep: 3700 loss: 0.1215 acc: 89.18\n",
      "ep: 3800 loss: 0.1201 acc: 88.66\n",
      "ep: 3900 loss: 0.1189 acc: 88.66\n",
      "ep: 4000 loss: 0.1177 acc: 88.66\n",
      "ep: 4100 loss: 0.1166 acc: 88.66\n",
      "ep: 4200 loss: 0.1155 acc: 88.66\n",
      "ep: 4300 loss: 0.1145 acc: 89.18\n",
      "ep: 4400 loss: 0.1134 acc: 89.18\n",
      "ep: 4500 loss: 0.1123 acc: 89.18\n",
      "ep: 4600 loss: 0.1110 acc: 89.18\n",
      "ep: 4700 loss: 0.1096 acc: 89.18\n",
      "ep: 4800 loss: 0.1078 acc: 89.18\n",
      "ep: 4900 loss: 0.1057 acc: 89.18\n",
      "ep: 5000 loss: 0.1032 acc: 88.66\n",
      "ep: 5100 loss: 0.1008 acc: 89.18\n",
      "ep: 5200 loss: 0.0984 acc: 88.66\n",
      "ep: 5300 loss: 0.0961 acc: 88.14\n",
      "ep: 5400 loss: 0.0938 acc: 89.18\n",
      "ep: 5500 loss: 0.0916 acc: 89.18\n",
      "ep: 5600 loss: 0.0895 acc: 90.21\n",
      "init:0.5, acc:90.20618438720703 , epoch:5619\n",
      "ep:  100 loss: 0.6164 acc: 60.31\n",
      "ep:  200 loss: 0.5402 acc: 59.79\n",
      "ep:  300 loss: 0.4508 acc: 67.01\n",
      "ep:  400 loss: 0.3879 acc: 72.16\n",
      "ep:  500 loss: 0.3410 acc: 72.68\n",
      "ep:  600 loss: 0.3124 acc: 74.23\n",
      "ep:  700 loss: 0.2908 acc: 73.20\n",
      "ep:  800 loss: 0.2721 acc: 73.71\n",
      "ep:  900 loss: 0.2522 acc: 75.26\n",
      "ep: 1000 loss: 0.2284 acc: 77.84\n",
      "ep: 1100 loss: 0.2126 acc: 78.35\n",
      "ep: 1200 loss: 0.2014 acc: 77.84\n",
      "ep: 1300 loss: 0.1924 acc: 78.35\n",
      "ep: 1400 loss: 0.1850 acc: 78.35\n",
      "ep: 1500 loss: 0.1789 acc: 78.87\n",
      "ep: 1600 loss: 0.1735 acc: 79.38\n",
      "ep: 1700 loss: 0.1688 acc: 79.38\n",
      "ep: 1800 loss: 0.1646 acc: 79.38\n",
      "ep: 1900 loss: 0.1609 acc: 78.35\n",
      "ep: 2000 loss: 0.1576 acc: 79.38\n",
      "ep: 2100 loss: 0.1546 acc: 79.38\n",
      "ep: 2200 loss: 0.1518 acc: 79.90\n",
      "ep: 2300 loss: 0.1497 acc: 79.38\n",
      "ep: 2400 loss: 0.1483 acc: 80.41\n",
      "ep: 2500 loss: 0.1455 acc: 80.41\n",
      "ep: 2600 loss: 0.1396 acc: 80.93\n",
      "ep: 2700 loss: 0.1336 acc: 79.90\n",
      "ep: 2800 loss: 0.1297 acc: 79.90\n",
      "ep: 2900 loss: 0.1268 acc: 79.90\n",
      "ep: 3000 loss: 0.1243 acc: 79.38\n",
      "ep: 3100 loss: 0.1216 acc: 79.38\n",
      "ep: 3200 loss: 0.1188 acc: 79.38\n",
      "ep: 3300 loss: 0.1166 acc: 79.90\n",
      "ep: 3400 loss: 0.1148 acc: 79.90\n",
      "ep: 3500 loss: 0.1132 acc: 79.90\n",
      "ep: 3600 loss: 0.1118 acc: 79.90\n",
      "ep: 3700 loss: 0.1104 acc: 80.41\n",
      "ep: 3800 loss: 0.1090 acc: 80.93\n",
      "ep: 3900 loss: 0.1065 acc: 80.93\n",
      "ep: 4000 loss: 0.1038 acc: 81.96\n",
      "ep: 4100 loss: 0.1018 acc: 81.96\n",
      "ep: 4200 loss: 0.1003 acc: 81.96\n",
      "ep: 4300 loss: 0.0991 acc: 82.99\n",
      "ep: 4400 loss: 0.0976 acc: 82.99\n",
      "ep: 4500 loss: 0.0963 acc: 82.99\n",
      "ep: 4600 loss: 0.0952 acc: 83.51\n",
      "ep: 4700 loss: 0.0942 acc: 83.51\n",
      "ep: 4800 loss: 0.0933 acc: 84.02\n",
      "ep: 4900 loss: 0.0924 acc: 83.51\n",
      "ep: 5000 loss: 0.0916 acc: 83.51\n",
      "ep: 5100 loss: 0.0909 acc: 84.02\n",
      "ep: 5200 loss: 0.0901 acc: 84.02\n",
      "ep: 5300 loss: 0.0894 acc: 84.54\n",
      "ep: 5400 loss: 0.0887 acc: 85.05\n",
      "ep: 5500 loss: 0.0881 acc: 85.05\n",
      "ep: 5600 loss: 0.0874 acc: 85.05\n",
      "init:0.6, acc:85.05154418945312 , epoch:5618\n",
      "ep:  100 loss: 0.6110 acc: 58.25\n",
      "ep:  200 loss: 0.5241 acc: 63.92\n",
      "ep:  300 loss: 0.4439 acc: 64.43\n",
      "ep:  400 loss: 0.3847 acc: 65.98\n",
      "ep:  500 loss: 0.3400 acc: 67.01\n",
      "ep:  600 loss: 0.2888 acc: 68.04\n",
      "ep:  700 loss: 0.2515 acc: 71.13\n",
      "ep:  800 loss: 0.2256 acc: 72.16\n",
      "ep:  900 loss: 0.2061 acc: 73.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1000 loss: 0.1890 acc: 74.74\n",
      "ep: 1100 loss: 0.1743 acc: 75.77\n",
      "ep: 1200 loss: 0.1641 acc: 77.32\n",
      "ep: 1300 loss: 0.1565 acc: 80.41\n",
      "ep: 1400 loss: 0.1501 acc: 83.51\n",
      "ep: 1500 loss: 0.1413 acc: 85.05\n",
      "ep: 1600 loss: 0.1326 acc: 84.54\n",
      "ep: 1700 loss: 0.1248 acc: 85.57\n",
      "ep: 1800 loss: 0.1181 acc: 85.57\n",
      "ep: 1900 loss: 0.1123 acc: 85.57\n",
      "ep: 2000 loss: 0.1080 acc: 86.60\n",
      "ep: 2100 loss: 0.1033 acc: 87.11\n",
      "ep: 2200 loss: 0.0989 acc: 87.11\n",
      "ep: 2300 loss: 0.0949 acc: 87.11\n",
      "ep: 2400 loss: 0.0915 acc: 87.63\n",
      "ep: 2500 loss: 0.0885 acc: 88.14\n",
      "ep: 2600 loss: 0.0857 acc: 89.69\n",
      "ep: 2700 loss: 0.0831 acc: 89.69\n",
      "ep: 2800 loss: 0.0806 acc: 89.69\n",
      "ep: 2900 loss: 0.0783 acc: 90.21\n",
      "ep: 3000 loss: 0.0762 acc: 91.24\n",
      "ep: 3100 loss: 0.0744 acc: 91.24\n",
      "ep: 3200 loss: 0.0728 acc: 91.24\n",
      "ep: 3300 loss: 0.0713 acc: 91.24\n",
      "ep: 3400 loss: 0.0701 acc: 91.24\n",
      "ep: 3500 loss: 0.0690 acc: 91.24\n",
      "ep: 3600 loss: 0.0680 acc: 91.24\n",
      "ep: 3700 loss: 0.0672 acc: 91.24\n",
      "ep: 3800 loss: 0.0664 acc: 91.24\n",
      "ep: 3900 loss: 0.0657 acc: 91.75\n",
      "ep: 4000 loss: 0.0651 acc: 91.75\n",
      "ep: 4100 loss: 0.0645 acc: 91.75\n",
      "ep: 4200 loss: 0.0640 acc: 91.75\n",
      "ep: 4300 loss: 0.0635 acc: 91.75\n",
      "ep: 4400 loss: 0.0631 acc: 91.75\n",
      "ep: 4500 loss: 0.0626 acc: 91.75\n",
      "ep: 4600 loss: 0.0622 acc: 91.75\n",
      "ep: 4700 loss: 0.0619 acc: 92.27\n",
      "ep: 4800 loss: 0.0616 acc: 92.27\n",
      "ep: 4900 loss: 0.0613 acc: 92.27\n",
      "ep: 5000 loss: 0.0611 acc: 92.27\n",
      "ep: 5100 loss: 0.0609 acc: 92.27\n",
      "ep: 5200 loss: 0.0608 acc: 92.27\n",
      "ep: 5300 loss: 0.0606 acc: 92.27\n",
      "ep: 5400 loss: 0.0605 acc: 92.27\n",
      "ep: 5500 loss: 0.0604 acc: 92.27\n",
      "ep: 5600 loss: 0.0604 acc: 92.27\n",
      "init:0.7000000000000001, acc:92.2680435180664 , epoch:5617\n",
      "ep:  100 loss: 0.6092 acc: 58.76\n",
      "ep:  200 loss: 0.5268 acc: 64.95\n",
      "ep:  300 loss: 0.4223 acc: 65.98\n",
      "ep:  400 loss: 0.3417 acc: 72.16\n",
      "ep:  500 loss: 0.3022 acc: 69.59\n",
      "ep:  600 loss: 0.2764 acc: 70.10\n",
      "ep:  700 loss: 0.2578 acc: 71.65\n",
      "ep:  800 loss: 0.2424 acc: 72.16\n",
      "ep:  900 loss: 0.2276 acc: 73.71\n",
      "ep: 1000 loss: 0.2156 acc: 74.74\n",
      "ep: 1100 loss: 0.2052 acc: 74.74\n",
      "ep: 1200 loss: 0.1957 acc: 74.74\n",
      "ep: 1300 loss: 0.1862 acc: 76.80\n",
      "ep: 1400 loss: 0.1775 acc: 77.84\n",
      "ep: 1500 loss: 0.1697 acc: 78.35\n",
      "ep: 1600 loss: 0.1627 acc: 79.90\n",
      "ep: 1700 loss: 0.1564 acc: 79.90\n",
      "ep: 1800 loss: 0.1500 acc: 79.38\n",
      "ep: 1900 loss: 0.1423 acc: 80.93\n",
      "ep: 2000 loss: 0.1307 acc: 80.41\n",
      "ep: 2100 loss: 0.1230 acc: 80.93\n",
      "ep: 2200 loss: 0.1170 acc: 81.44\n",
      "ep: 2300 loss: 0.1113 acc: 81.44\n",
      "ep: 2400 loss: 0.1050 acc: 81.96\n",
      "ep: 2500 loss: 0.0998 acc: 81.44\n",
      "ep: 2600 loss: 0.0958 acc: 81.44\n",
      "ep: 2700 loss: 0.0923 acc: 81.44\n",
      "ep: 2800 loss: 0.0894 acc: 81.44\n",
      "ep: 2900 loss: 0.0869 acc: 81.44\n",
      "ep: 3000 loss: 0.0847 acc: 81.96\n",
      "ep: 3100 loss: 0.0828 acc: 81.96\n",
      "ep: 3200 loss: 0.0812 acc: 82.47\n",
      "ep: 3300 loss: 0.0795 acc: 82.99\n",
      "ep: 3400 loss: 0.0772 acc: 83.51\n",
      "ep: 3500 loss: 0.0734 acc: 84.02\n",
      "ep: 3600 loss: 0.0708 acc: 84.02\n",
      "ep: 3700 loss: 0.0688 acc: 84.02\n",
      "ep: 3800 loss: 0.0672 acc: 84.02\n",
      "ep: 3900 loss: 0.0659 acc: 84.02\n",
      "ep: 4000 loss: 0.0646 acc: 84.02\n",
      "ep: 4100 loss: 0.0636 acc: 84.02\n",
      "ep: 4200 loss: 0.0628 acc: 83.51\n",
      "ep: 4300 loss: 0.0622 acc: 82.47\n",
      "ep: 4400 loss: 0.0619 acc: 82.99\n",
      "ep: 4500 loss: 0.0635 acc: 82.99\n",
      "ep: 4600 loss: 0.0640 acc: 84.02\n",
      "ep: 4700 loss: 0.0636 acc: 83.51\n",
      "ep: 4800 loss: 0.0630 acc: 83.51\n",
      "ep: 4900 loss: 0.0625 acc: 83.51\n",
      "ep: 5000 loss: 0.0621 acc: 84.02\n",
      "ep: 5100 loss: 0.0617 acc: 84.02\n",
      "ep: 5200 loss: 0.0615 acc: 84.02\n",
      "ep: 5300 loss: 0.0601 acc: 84.02\n",
      "ep: 5400 loss: 0.0589 acc: 85.57\n",
      "ep: 5500 loss: 0.0578 acc: 85.57\n",
      "ep: 5600 loss: 0.0567 acc: 85.57\n",
      "init:0.8, acc:85.56700897216797 , epoch:5616\n",
      "ep:  100 loss: 0.5823 acc: 62.89\n",
      "ep:  200 loss: 0.5134 acc: 63.40\n",
      "ep:  300 loss: 0.4176 acc: 67.01\n",
      "ep:  400 loss: 0.3230 acc: 71.13\n",
      "ep:  500 loss: 0.2698 acc: 74.23\n",
      "ep:  600 loss: 0.2300 acc: 75.26\n",
      "ep:  700 loss: 0.1988 acc: 75.77\n",
      "ep:  800 loss: 0.1731 acc: 74.74\n",
      "ep:  900 loss: 0.1530 acc: 75.77\n",
      "ep: 1000 loss: 0.1385 acc: 76.29\n",
      "ep: 1100 loss: 0.1265 acc: 78.35\n",
      "ep: 1200 loss: 0.1022 acc: 78.35\n",
      "ep: 1300 loss: 0.0870 acc: 78.35\n",
      "ep: 1400 loss: 0.0766 acc: 78.87\n",
      "ep: 1500 loss: 0.0696 acc: 81.44\n",
      "ep: 1600 loss: 0.0636 acc: 82.47\n",
      "ep: 1700 loss: 0.0592 acc: 83.51\n",
      "ep: 1800 loss: 0.0540 acc: 84.54\n",
      "ep: 1900 loss: 0.0506 acc: 84.02\n",
      "ep: 2000 loss: 0.0479 acc: 83.51\n",
      "ep: 2100 loss: 0.0453 acc: 83.51\n",
      "ep: 2200 loss: 0.0416 acc: 83.51\n",
      "ep: 2300 loss: 0.0391 acc: 84.02\n",
      "ep: 2400 loss: 0.0366 acc: 84.02\n",
      "ep: 2500 loss: 0.0361 acc: 84.02\n",
      "ep: 2600 loss: 0.0357 acc: 85.57\n",
      "ep: 2700 loss: 0.0308 acc: 86.60\n",
      "ep: 2800 loss: 0.0283 acc: 87.11\n",
      "ep: 2900 loss: 0.0255 acc: 85.57\n",
      "ep: 3000 loss: 0.0233 acc: 87.11\n",
      "ep: 3100 loss: 0.0217 acc: 86.60\n",
      "ep: 3200 loss: 0.0206 acc: 86.60\n",
      "ep: 3300 loss: 0.0197 acc: 86.60\n",
      "ep: 3400 loss: 0.0196 acc: 86.08\n",
      "ep: 3500 loss: 0.0209 acc: 87.63\n",
      "ep: 3600 loss: 0.0199 acc: 87.11\n",
      "ep: 3700 loss: 0.0185 acc: 87.63\n",
      "ep: 3800 loss: 0.0181 acc: 87.63\n",
      "ep: 3900 loss: 0.0185 acc: 87.63\n",
      "ep: 4000 loss: 0.0180 acc: 87.63\n",
      "ep: 4100 loss: 0.0180 acc: 87.63\n",
      "ep: 4200 loss: 0.0179 acc: 88.66\n",
      "ep: 4300 loss: 0.0176 acc: 88.66\n",
      "ep: 4400 loss: 0.0173 acc: 89.69\n",
      "ep: 4500 loss: 0.0172 acc: 91.24\n",
      "ep: 4600 loss: 0.0173 acc: 91.24\n",
      "ep: 4700 loss: 0.0175 acc: 90.72\n",
      "ep: 4800 loss: 0.0178 acc: 90.72\n",
      "ep: 4900 loss: 0.0181 acc: 90.72\n",
      "ep: 5000 loss: 0.0191 acc: 91.75\n",
      "ep: 5100 loss: 0.0175 acc: 90.72\n",
      "ep: 5200 loss: 0.0162 acc: 90.72\n",
      "ep: 5300 loss: 0.0154 acc: 90.21\n",
      "ep: 5400 loss: 0.0148 acc: 90.21\n",
      "ep: 5500 loss: 0.0145 acc: 90.72\n",
      "ep: 5600 loss: 0.0143 acc: 90.72\n",
      "init:0.9, acc:90.72164916992188 , epoch:5615\n",
      "ep:  100 loss: 0.5750 acc: 62.89\n",
      "ep:  200 loss: 0.5220 acc: 64.95\n",
      "ep:  300 loss: 0.4732 acc: 64.43\n",
      "ep:  400 loss: 0.4014 acc: 67.53\n",
      "ep:  500 loss: 0.3439 acc: 69.59\n",
      "ep:  600 loss: 0.3018 acc: 70.62\n",
      "ep:  700 loss: 0.2782 acc: 72.16\n",
      "ep:  800 loss: 0.2620 acc: 71.65\n",
      "ep:  900 loss: 0.2476 acc: 75.77\n",
      "ep: 1000 loss: 0.2343 acc: 75.77\n",
      "ep: 1100 loss: 0.2218 acc: 76.80\n",
      "ep: 1200 loss: 0.2098 acc: 75.77\n",
      "ep: 1300 loss: 0.2015 acc: 75.26\n",
      "ep: 1400 loss: 0.1924 acc: 75.77\n",
      "ep: 1500 loss: 0.1848 acc: 76.29\n",
      "ep: 1600 loss: 0.1782 acc: 77.32\n",
      "ep: 1700 loss: 0.1720 acc: 76.80\n",
      "ep: 1800 loss: 0.1660 acc: 77.32\n",
      "ep: 1900 loss: 0.1614 acc: 76.80\n",
      "ep: 2000 loss: 0.1578 acc: 77.32\n",
      "ep: 2100 loss: 0.1548 acc: 77.84\n",
      "ep: 2200 loss: 0.1521 acc: 77.84\n",
      "ep: 2300 loss: 0.1497 acc: 78.35\n",
      "ep: 2400 loss: 0.1473 acc: 78.87\n",
      "ep: 2500 loss: 0.1448 acc: 78.87\n",
      "ep: 2600 loss: 0.1422 acc: 79.38\n",
      "ep: 2700 loss: 0.1395 acc: 79.38\n",
      "ep: 2800 loss: 0.1371 acc: 79.38\n",
      "ep: 2900 loss: 0.1348 acc: 79.90\n",
      "ep: 3000 loss: 0.1328 acc: 79.38\n",
      "ep: 3100 loss: 0.1309 acc: 79.38\n",
      "ep: 3200 loss: 0.1292 acc: 79.38\n",
      "ep: 3300 loss: 0.1277 acc: 79.38\n",
      "ep: 3400 loss: 0.1263 acc: 79.90\n",
      "ep: 3500 loss: 0.1250 acc: 79.90\n",
      "ep: 3600 loss: 0.1238 acc: 79.38\n",
      "ep: 3700 loss: 0.1226 acc: 80.93\n",
      "ep: 3800 loss: 0.1214 acc: 80.41\n",
      "ep: 3900 loss: 0.1202 acc: 80.93\n",
      "ep: 4000 loss: 0.1188 acc: 80.93\n",
      "ep: 4100 loss: 0.1175 acc: 80.93\n",
      "ep: 4200 loss: 0.1162 acc: 80.93\n",
      "ep: 4300 loss: 0.1150 acc: 80.93\n",
      "ep: 4400 loss: 0.1139 acc: 80.41\n",
      "ep: 4500 loss: 0.1128 acc: 80.41\n",
      "ep: 4600 loss: 0.1118 acc: 80.93\n",
      "ep: 4700 loss: 0.1108 acc: 80.93\n",
      "ep: 4800 loss: 0.1098 acc: 81.44\n",
      "ep: 4900 loss: 0.1089 acc: 81.44\n",
      "ep: 5000 loss: 0.1080 acc: 81.44\n",
      "ep: 5100 loss: 0.1072 acc: 81.44\n",
      "ep: 5200 loss: 0.1063 acc: 81.44\n",
      "ep: 5300 loss: 0.1055 acc: 81.44\n",
      "ep: 5400 loss: 0.1048 acc: 81.44\n",
      "ep: 5500 loss: 0.1040 acc: 81.44\n",
      "ep: 5600 loss: 0.1033 acc: 81.44\n",
      "init:1.0, acc:81.44329833984375 , epoch:5614\n",
      "ep:  100 loss: 0.5813 acc: 62.37\n",
      "ep:  200 loss: 0.5276 acc: 65.98\n",
      "ep:  300 loss: 0.4885 acc: 64.43\n",
      "ep:  400 loss: 0.4533 acc: 67.53\n",
      "ep:  500 loss: 0.4169 acc: 69.07\n",
      "ep:  600 loss: 0.3819 acc: 68.56\n",
      "ep:  700 loss: 0.3550 acc: 71.13\n",
      "ep:  800 loss: 0.3300 acc: 71.13\n",
      "ep:  900 loss: 0.3005 acc: 73.20\n",
      "ep: 1000 loss: 0.2839 acc: 73.20\n",
      "ep: 1100 loss: 0.2710 acc: 74.74\n",
      "ep: 1200 loss: 0.2596 acc: 76.80\n",
      "ep: 1300 loss: 0.2496 acc: 76.80\n",
      "ep: 1400 loss: 0.2406 acc: 76.80\n",
      "ep: 1500 loss: 0.2326 acc: 76.80\n",
      "ep: 1600 loss: 0.2258 acc: 76.80\n",
      "ep: 1700 loss: 0.2201 acc: 77.84\n",
      "ep: 1800 loss: 0.2150 acc: 78.35\n",
      "ep: 1900 loss: 0.2119 acc: 78.87\n",
      "ep: 2000 loss: 0.2062 acc: 79.38\n",
      "ep: 2100 loss: 0.2010 acc: 78.87\n",
      "ep: 2200 loss: 0.1963 acc: 78.87\n",
      "ep: 2300 loss: 0.1922 acc: 78.87\n",
      "ep: 2400 loss: 0.1885 acc: 78.87\n",
      "ep: 2500 loss: 0.1854 acc: 79.38\n",
      "ep: 2600 loss: 0.1816 acc: 79.38\n",
      "ep: 2700 loss: 0.1758 acc: 79.90\n",
      "ep: 2800 loss: 0.1699 acc: 80.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 2900 loss: 0.1648 acc: 80.41\n",
      "ep: 3000 loss: 0.1604 acc: 80.41\n",
      "ep: 3100 loss: 0.1556 acc: 79.38\n",
      "ep: 3200 loss: 0.1418 acc: 80.41\n",
      "ep: 3300 loss: 0.1345 acc: 81.44\n",
      "ep: 3400 loss: 0.1298 acc: 82.47\n",
      "ep: 3500 loss: 0.1264 acc: 82.47\n",
      "ep: 3600 loss: 0.1236 acc: 82.99\n",
      "ep: 3700 loss: 0.1208 acc: 82.47\n",
      "ep: 3800 loss: 0.1184 acc: 82.47\n",
      "ep: 3900 loss: 0.1160 acc: 82.47\n",
      "ep: 4000 loss: 0.1135 acc: 82.99\n",
      "ep: 4100 loss: 0.1111 acc: 82.99\n",
      "ep: 4200 loss: 0.1085 acc: 83.51\n",
      "ep: 4300 loss: 0.1024 acc: 82.47\n",
      "ep: 4400 loss: 0.0981 acc: 82.99\n",
      "ep: 4500 loss: 0.0952 acc: 82.99\n",
      "ep: 4600 loss: 0.0931 acc: 84.02\n",
      "ep: 4700 loss: 0.0915 acc: 84.02\n",
      "ep: 4800 loss: 0.0903 acc: 83.51\n",
      "ep: 4900 loss: 0.0893 acc: 84.02\n",
      "ep: 5000 loss: 0.0884 acc: 84.02\n",
      "ep: 5100 loss: 0.0876 acc: 84.02\n",
      "ep: 5200 loss: 0.0868 acc: 84.02\n",
      "ep: 5300 loss: 0.0861 acc: 84.02\n",
      "ep: 5400 loss: 0.0854 acc: 84.02\n",
      "ep: 5500 loss: 0.0846 acc: 84.54\n",
      "ep: 5600 loss: 0.0837 acc: 84.54\n",
      "init:1.1, acc:84.53607940673828 , epoch:5613\n",
      "ep:  100 loss: 0.5799 acc: 60.82\n",
      "ep:  200 loss: 0.5469 acc: 62.37\n",
      "ep:  300 loss: 0.5085 acc: 63.92\n",
      "ep:  400 loss: 0.4690 acc: 66.49\n",
      "ep:  500 loss: 0.4366 acc: 66.49\n",
      "ep:  600 loss: 0.4111 acc: 66.49\n",
      "ep:  700 loss: 0.3902 acc: 68.04\n",
      "ep:  800 loss: 0.3733 acc: 69.07\n",
      "ep:  900 loss: 0.3584 acc: 69.59\n",
      "ep: 1000 loss: 0.3430 acc: 70.62\n",
      "ep: 1100 loss: 0.3258 acc: 71.13\n",
      "ep: 1200 loss: 0.3096 acc: 71.13\n",
      "ep: 1300 loss: 0.2979 acc: 72.16\n",
      "ep: 1400 loss: 0.2892 acc: 72.68\n",
      "ep: 1500 loss: 0.2827 acc: 73.71\n",
      "ep: 1600 loss: 0.2775 acc: 73.71\n",
      "ep: 1700 loss: 0.2731 acc: 74.23\n",
      "ep: 1800 loss: 0.2693 acc: 74.74\n",
      "ep: 1900 loss: 0.2659 acc: 75.26\n",
      "ep: 2000 loss: 0.2626 acc: 74.23\n",
      "ep: 2100 loss: 0.2596 acc: 74.74\n",
      "ep: 2200 loss: 0.2567 acc: 73.71\n",
      "ep: 2300 loss: 0.2541 acc: 73.71\n",
      "ep: 2400 loss: 0.2515 acc: 73.71\n",
      "ep: 2500 loss: 0.2492 acc: 73.71\n",
      "ep: 2600 loss: 0.2469 acc: 73.71\n",
      "ep: 2700 loss: 0.2448 acc: 74.23\n",
      "ep: 2800 loss: 0.2427 acc: 74.23\n",
      "ep: 2900 loss: 0.2407 acc: 74.23\n",
      "ep: 3000 loss: 0.2385 acc: 74.23\n",
      "ep: 3100 loss: 0.2309 acc: 73.71\n",
      "ep: 3200 loss: 0.2257 acc: 75.26\n",
      "ep: 3300 loss: 0.2226 acc: 75.77\n",
      "ep: 3400 loss: 0.2200 acc: 75.77\n",
      "ep: 3500 loss: 0.2177 acc: 75.77\n",
      "ep: 3600 loss: 0.2156 acc: 75.26\n",
      "ep: 3700 loss: 0.2137 acc: 75.26\n",
      "ep: 3800 loss: 0.2119 acc: 75.77\n",
      "ep: 3900 loss: 0.2093 acc: 75.77\n",
      "ep: 4000 loss: 0.2053 acc: 75.77\n",
      "ep: 4100 loss: 0.2006 acc: 76.29\n",
      "ep: 4200 loss: 0.1982 acc: 76.29\n",
      "ep: 4300 loss: 0.1969 acc: 76.29\n",
      "ep: 4400 loss: 0.1955 acc: 77.32\n",
      "ep: 4500 loss: 0.1939 acc: 77.84\n",
      "ep: 4600 loss: 0.1924 acc: 77.84\n",
      "ep: 4700 loss: 0.1909 acc: 78.35\n",
      "ep: 4800 loss: 0.1893 acc: 78.87\n",
      "ep: 4900 loss: 0.1879 acc: 78.87\n",
      "ep: 5000 loss: 0.1865 acc: 78.35\n",
      "ep: 5100 loss: 0.1851 acc: 78.35\n",
      "ep: 5200 loss: 0.1839 acc: 77.84\n",
      "ep: 5300 loss: 0.1827 acc: 78.35\n",
      "ep: 5400 loss: 0.1817 acc: 78.35\n",
      "ep: 5500 loss: 0.1807 acc: 77.84\n",
      "ep: 5600 loss: 0.1799 acc: 78.35\n",
      "init:1.2000000000000002, acc:78.35051727294922 , epoch:5612\n",
      "ep:  100 loss: 0.5528 acc: 62.89\n",
      "ep:  200 loss: 0.5030 acc: 64.43\n",
      "ep:  300 loss: 0.4579 acc: 64.95\n",
      "ep:  400 loss: 0.4230 acc: 65.46\n",
      "ep:  500 loss: 0.3859 acc: 65.46\n",
      "ep:  600 loss: 0.3591 acc: 67.53\n",
      "ep:  700 loss: 0.3378 acc: 67.53\n",
      "ep:  800 loss: 0.3192 acc: 68.04\n",
      "ep:  900 loss: 0.3042 acc: 70.10\n",
      "ep: 1000 loss: 0.2887 acc: 71.13\n",
      "ep: 1100 loss: 0.2781 acc: 71.65\n",
      "ep: 1200 loss: 0.2679 acc: 71.65\n",
      "ep: 1300 loss: 0.2509 acc: 74.23\n",
      "ep: 1400 loss: 0.2310 acc: 77.84\n",
      "ep: 1500 loss: 0.2150 acc: 79.90\n",
      "ep: 1600 loss: 0.2037 acc: 82.47\n",
      "ep: 1700 loss: 0.1957 acc: 82.99\n",
      "ep: 1800 loss: 0.1865 acc: 82.99\n",
      "ep: 1900 loss: 0.1763 acc: 84.02\n",
      "ep: 2000 loss: 0.1703 acc: 84.54\n",
      "ep: 2100 loss: 0.1653 acc: 85.57\n",
      "ep: 2200 loss: 0.1608 acc: 85.57\n",
      "ep: 2300 loss: 0.1567 acc: 86.08\n",
      "ep: 2400 loss: 0.1529 acc: 85.57\n",
      "ep: 2500 loss: 0.1491 acc: 85.57\n",
      "ep: 2600 loss: 0.1448 acc: 85.57\n",
      "ep: 2700 loss: 0.1405 acc: 85.05\n",
      "ep: 2800 loss: 0.1370 acc: 85.05\n",
      "ep: 2900 loss: 0.1337 acc: 85.05\n",
      "ep: 3000 loss: 0.1300 acc: 84.54\n",
      "ep: 3100 loss: 0.1262 acc: 84.54\n",
      "ep: 3200 loss: 0.1216 acc: 84.54\n",
      "ep: 3300 loss: 0.1183 acc: 84.54\n",
      "ep: 3400 loss: 0.1154 acc: 85.05\n",
      "ep: 3500 loss: 0.1124 acc: 85.57\n",
      "ep: 3600 loss: 0.1096 acc: 85.57\n",
      "ep: 3700 loss: 0.1069 acc: 86.60\n",
      "ep: 3800 loss: 0.1042 acc: 87.63\n",
      "ep: 3900 loss: 0.1025 acc: 87.11\n",
      "ep: 4000 loss: 0.1006 acc: 87.11\n",
      "ep: 4100 loss: 0.0977 acc: 87.11\n",
      "ep: 4200 loss: 0.0954 acc: 87.11\n",
      "ep: 4300 loss: 0.0935 acc: 87.11\n",
      "ep: 4400 loss: 0.0918 acc: 87.11\n",
      "ep: 4500 loss: 0.0903 acc: 87.11\n",
      "ep: 4600 loss: 0.0888 acc: 87.11\n",
      "ep: 4700 loss: 0.0875 acc: 87.11\n",
      "ep: 4800 loss: 0.0863 acc: 87.11\n",
      "ep: 4900 loss: 0.0851 acc: 87.11\n",
      "ep: 5000 loss: 0.0839 acc: 87.63\n",
      "ep: 5100 loss: 0.0829 acc: 87.63\n",
      "ep: 5200 loss: 0.0818 acc: 87.63\n",
      "ep: 5300 loss: 0.0807 acc: 87.63\n",
      "ep: 5400 loss: 0.0797 acc: 88.14\n",
      "ep: 5500 loss: 0.0787 acc: 87.63\n",
      "ep: 5600 loss: 0.0778 acc: 88.14\n",
      "init:1.3000000000000003, acc:88.14433288574219 , epoch:5611\n",
      "ep:  100 loss: 0.5179 acc: 66.49\n",
      "ep:  200 loss: 0.4452 acc: 71.13\n",
      "ep:  300 loss: 0.3983 acc: 73.71\n",
      "ep:  400 loss: 0.3604 acc: 74.74\n",
      "ep:  500 loss: 0.3069 acc: 79.90\n",
      "ep:  600 loss: 0.2534 acc: 80.93\n",
      "ep:  700 loss: 0.2126 acc: 82.47\n",
      "ep:  800 loss: 0.1802 acc: 83.51\n",
      "ep:  900 loss: 0.1576 acc: 88.66\n",
      "ep: 1000 loss: 0.1396 acc: 89.18\n",
      "ep: 1100 loss: 0.1240 acc: 89.69\n",
      "ep: 1200 loss: 0.1111 acc: 90.72\n",
      "ep: 1300 loss: 0.0997 acc: 91.75\n",
      "ep: 1400 loss: 0.0857 acc: 92.78\n",
      "ep: 1500 loss: 0.0729 acc: 93.81\n",
      "ep: 1600 loss: 0.0660 acc: 93.81\n",
      "ep: 1700 loss: 0.0612 acc: 93.81\n",
      "ep: 1800 loss: 0.0579 acc: 94.33\n",
      "ep: 1900 loss: 0.0525 acc: 94.33\n",
      "ep: 2000 loss: 0.0486 acc: 94.85\n",
      "ep: 2100 loss: 0.0458 acc: 94.85\n",
      "ep: 2200 loss: 0.0435 acc: 94.85\n",
      "ep: 2300 loss: 0.0413 acc: 94.85\n",
      "ep: 2400 loss: 0.0394 acc: 94.85\n",
      "ep: 2500 loss: 0.0644 acc: 91.24\n",
      "ep: 2600 loss: 0.0371 acc: 95.36\n",
      "ep: 2700 loss: 0.0343 acc: 94.85\n",
      "ep: 2800 loss: 0.0305 acc: 94.85\n",
      "ep: 2900 loss: 0.0272 acc: 94.85\n",
      "ep: 3000 loss: 0.0249 acc: 94.85\n",
      "ep: 3100 loss: 0.0233 acc: 95.36\n",
      "ep: 3200 loss: 0.0220 acc: 95.88\n",
      "ep: 3300 loss: 0.0210 acc: 95.88\n",
      "ep: 3400 loss: 0.0203 acc: 95.88\n",
      "ep: 3500 loss: 0.0195 acc: 96.39\n",
      "ep: 3600 loss: 0.0189 acc: 96.39\n",
      "ep: 3700 loss: 0.0184 acc: 96.39\n",
      "ep: 3800 loss: 0.0180 acc: 96.39\n",
      "ep: 3900 loss: 0.0177 acc: 96.39\n",
      "ep: 4000 loss: 0.0174 acc: 96.91\n",
      "ep: 4100 loss: 0.0171 acc: 96.91\n",
      "ep: 4200 loss: 0.0166 acc: 96.91\n",
      "ep: 4300 loss: 0.0163 acc: 96.91\n",
      "ep: 4400 loss: 0.0161 acc: 96.91\n",
      "ep: 4500 loss: 0.0159 acc: 96.91\n",
      "ep: 4600 loss: 0.0158 acc: 96.91\n",
      "ep: 4700 loss: 0.0157 acc: 96.91\n",
      "ep: 4800 loss: 0.0156 acc: 96.91\n",
      "ep: 4900 loss: 0.0154 acc: 96.91\n",
      "ep: 5000 loss: 0.0153 acc: 96.91\n",
      "ep: 5100 loss: 0.0152 acc: 96.91\n",
      "ep: 5200 loss: 0.0199 acc: 96.91\n",
      "ep: 5300 loss: 0.0148 acc: 96.91\n",
      "ep: 5400 loss: 0.0147 acc: 96.91\n",
      "ep: 5500 loss: 0.0147 acc: 96.91\n",
      "ep: 5600 loss: 0.0147 acc: 96.91\n",
      "init:1.4000000000000001, acc:96.90721893310547 , epoch:5610\n",
      "ep:  100 loss: 0.5409 acc: 62.89\n",
      "ep:  200 loss: 0.4782 acc: 70.10\n",
      "ep:  300 loss: 0.4322 acc: 71.65\n",
      "ep:  400 loss: 0.3882 acc: 74.23\n",
      "ep:  500 loss: 0.3543 acc: 77.32\n",
      "ep:  600 loss: 0.3289 acc: 78.35\n",
      "ep:  700 loss: 0.3002 acc: 79.90\n",
      "ep:  800 loss: 0.2474 acc: 85.05\n",
      "ep:  900 loss: 0.2054 acc: 85.57\n",
      "ep: 1000 loss: 0.1855 acc: 86.60\n",
      "ep: 1100 loss: 0.1694 acc: 87.63\n",
      "ep: 1200 loss: 0.1566 acc: 88.14\n",
      "ep: 1300 loss: 0.1480 acc: 88.66\n",
      "ep: 1400 loss: 0.1418 acc: 89.18\n",
      "ep: 1500 loss: 0.1372 acc: 90.21\n",
      "ep: 1600 loss: 0.1329 acc: 90.72\n",
      "ep: 1700 loss: 0.1297 acc: 91.24\n",
      "ep: 1800 loss: 0.1262 acc: 91.24\n",
      "ep: 1900 loss: 0.1224 acc: 91.24\n",
      "ep: 2000 loss: 0.1192 acc: 91.75\n",
      "ep: 2100 loss: 0.1165 acc: 91.24\n",
      "ep: 2200 loss: 0.1142 acc: 91.24\n",
      "ep: 2300 loss: 0.1123 acc: 91.24\n",
      "ep: 2400 loss: 0.1107 acc: 91.24\n",
      "ep: 2500 loss: 0.1093 acc: 91.24\n",
      "ep: 2600 loss: 0.1080 acc: 91.75\n",
      "ep: 2700 loss: 0.1075 acc: 91.75\n",
      "ep: 2800 loss: 0.1055 acc: 92.27\n",
      "ep: 2900 loss: 0.1036 acc: 92.27\n",
      "ep: 3000 loss: 0.1019 acc: 92.27\n",
      "ep: 3100 loss: 0.1004 acc: 92.27\n",
      "ep: 3200 loss: 0.0987 acc: 92.27\n",
      "ep: 3300 loss: 0.0967 acc: 92.27\n",
      "ep: 3400 loss: 0.0953 acc: 92.27\n",
      "ep: 3500 loss: 0.0944 acc: 92.27\n",
      "ep: 3600 loss: 0.0937 acc: 92.27\n",
      "ep: 3700 loss: 0.0931 acc: 91.75\n",
      "ep: 3800 loss: 0.0925 acc: 92.27\n",
      "ep: 3900 loss: 0.0920 acc: 92.27\n",
      "ep: 4000 loss: 0.0916 acc: 92.27\n",
      "ep: 4100 loss: 0.0912 acc: 92.27\n",
      "ep: 4200 loss: 0.0909 acc: 92.27\n",
      "ep: 4300 loss: 0.0907 acc: 92.27\n",
      "ep: 4400 loss: 0.0904 acc: 92.27\n",
      "ep: 4500 loss: 0.0902 acc: 92.27\n",
      "ep: 4600 loss: 0.0900 acc: 92.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4700 loss: 0.0898 acc: 92.27\n",
      "ep: 4800 loss: 0.0897 acc: 92.27\n",
      "ep: 4900 loss: 0.0895 acc: 92.27\n",
      "ep: 5000 loss: 0.0894 acc: 92.27\n",
      "ep: 5100 loss: 0.0893 acc: 92.27\n",
      "ep: 5200 loss: 0.0891 acc: 92.27\n",
      "ep: 5300 loss: 0.0890 acc: 92.27\n",
      "ep: 5400 loss: 0.0889 acc: 92.27\n",
      "ep: 5500 loss: 0.0888 acc: 92.27\n",
      "ep: 5600 loss: 0.0887 acc: 92.27\n",
      "init:1.5000000000000002, acc:92.2680435180664 , epoch:5609\n",
      "ep:  100 loss: 0.5615 acc: 64.95\n",
      "ep:  200 loss: 0.4859 acc: 68.56\n",
      "ep:  300 loss: 0.4421 acc: 67.53\n",
      "ep:  400 loss: 0.4038 acc: 69.59\n",
      "ep:  500 loss: 0.3709 acc: 70.10\n",
      "ep:  600 loss: 0.3469 acc: 70.62\n",
      "ep:  700 loss: 0.3280 acc: 71.13\n",
      "ep:  800 loss: 0.3145 acc: 72.68\n",
      "ep:  900 loss: 0.3013 acc: 73.20\n",
      "ep: 1000 loss: 0.2918 acc: 73.20\n",
      "ep: 1100 loss: 0.2840 acc: 73.71\n",
      "ep: 1200 loss: 0.2776 acc: 73.20\n",
      "ep: 1300 loss: 0.2724 acc: 74.23\n",
      "ep: 1400 loss: 0.2677 acc: 73.71\n",
      "ep: 1500 loss: 0.2622 acc: 74.23\n",
      "ep: 1600 loss: 0.2495 acc: 74.74\n",
      "ep: 1700 loss: 0.2382 acc: 76.29\n",
      "ep: 1800 loss: 0.2267 acc: 76.80\n",
      "ep: 1900 loss: 0.2180 acc: 77.32\n",
      "ep: 2000 loss: 0.2100 acc: 76.80\n",
      "ep: 2100 loss: 0.2030 acc: 76.29\n",
      "ep: 2200 loss: 0.1979 acc: 76.80\n",
      "ep: 2300 loss: 0.1936 acc: 77.32\n",
      "ep: 2400 loss: 0.1908 acc: 77.84\n",
      "ep: 2500 loss: 0.1907 acc: 79.90\n",
      "ep: 2600 loss: 0.1915 acc: 80.41\n",
      "ep: 2700 loss: 0.1884 acc: 79.38\n",
      "ep: 2800 loss: 0.1857 acc: 79.90\n",
      "ep: 2900 loss: 0.1835 acc: 80.93\n",
      "ep: 3000 loss: 0.1819 acc: 81.44\n",
      "ep: 3100 loss: 0.1806 acc: 80.93\n",
      "ep: 3200 loss: 0.1792 acc: 80.41\n",
      "ep: 3300 loss: 0.1775 acc: 80.41\n",
      "ep: 3400 loss: 0.1760 acc: 80.41\n",
      "ep: 3500 loss: 0.1747 acc: 80.41\n",
      "ep: 3600 loss: 0.1734 acc: 80.41\n",
      "ep: 3700 loss: 0.1723 acc: 80.41\n",
      "ep: 3800 loss: 0.1713 acc: 80.41\n",
      "ep: 3900 loss: 0.1705 acc: 80.41\n",
      "ep: 4000 loss: 0.1695 acc: 80.41\n",
      "ep: 4100 loss: 0.1685 acc: 80.93\n",
      "ep: 4200 loss: 0.1673 acc: 81.96\n",
      "ep: 4300 loss: 0.1657 acc: 81.96\n",
      "ep: 4400 loss: 0.1635 acc: 81.96\n",
      "ep: 4500 loss: 0.1618 acc: 82.47\n",
      "ep: 4600 loss: 0.1603 acc: 81.96\n",
      "ep: 4700 loss: 0.1589 acc: 81.44\n",
      "ep: 4800 loss: 0.1574 acc: 81.96\n",
      "ep: 4900 loss: 0.1555 acc: 81.96\n",
      "ep: 5000 loss: 0.1520 acc: 81.96\n",
      "ep: 5100 loss: 0.1473 acc: 82.47\n",
      "ep: 5200 loss: 0.1437 acc: 82.47\n",
      "ep: 5300 loss: 0.1406 acc: 82.47\n",
      "ep: 5400 loss: 0.1377 acc: 82.47\n",
      "ep: 5500 loss: 0.1359 acc: 82.47\n",
      "ep: 5600 loss: 0.1348 acc: 82.47\n",
      "init:1.6, acc:82.47422790527344 , epoch:5608\n",
      "ep:  100 loss: 0.6289 acc: 61.34\n",
      "ep:  200 loss: 0.5315 acc: 64.43\n",
      "ep:  300 loss: 0.4816 acc: 68.04\n",
      "ep:  400 loss: 0.3901 acc: 69.07\n",
      "ep:  500 loss: 0.3498 acc: 73.20\n",
      "ep:  600 loss: 0.3184 acc: 78.35\n",
      "ep:  700 loss: 0.2956 acc: 79.90\n",
      "ep:  800 loss: 0.2707 acc: 80.93\n",
      "ep:  900 loss: 0.2485 acc: 81.44\n",
      "ep: 1000 loss: 0.2288 acc: 82.47\n",
      "ep: 1100 loss: 0.2151 acc: 84.02\n",
      "ep: 1200 loss: 0.1958 acc: 85.57\n",
      "ep: 1300 loss: 0.1758 acc: 85.57\n",
      "ep: 1400 loss: 0.1570 acc: 86.08\n",
      "ep: 1500 loss: 0.1482 acc: 86.08\n",
      "ep: 1600 loss: 0.1428 acc: 86.08\n",
      "ep: 1700 loss: 0.1375 acc: 86.60\n",
      "ep: 1800 loss: 0.1324 acc: 86.60\n",
      "ep: 1900 loss: 0.1281 acc: 86.60\n",
      "ep: 2000 loss: 0.1245 acc: 86.08\n",
      "ep: 2100 loss: 0.1215 acc: 86.08\n",
      "ep: 2200 loss: 0.1191 acc: 86.08\n",
      "ep: 2300 loss: 0.1170 acc: 86.08\n",
      "ep: 2400 loss: 0.1145 acc: 86.08\n",
      "ep: 2500 loss: 0.1120 acc: 86.08\n",
      "ep: 2600 loss: 0.1096 acc: 86.08\n",
      "ep: 2700 loss: 0.1073 acc: 86.60\n",
      "ep: 2800 loss: 0.1051 acc: 87.11\n",
      "ep: 2900 loss: 0.1029 acc: 87.63\n",
      "ep: 3000 loss: 0.1004 acc: 87.63\n",
      "ep: 3100 loss: 0.0982 acc: 88.66\n",
      "ep: 3200 loss: 0.0963 acc: 88.66\n",
      "ep: 3300 loss: 0.0948 acc: 88.66\n",
      "ep: 3400 loss: 0.0934 acc: 88.66\n",
      "ep: 3500 loss: 0.0922 acc: 89.18\n",
      "ep: 3600 loss: 0.0911 acc: 89.18\n",
      "ep: 3700 loss: 0.0900 acc: 88.66\n",
      "ep: 3800 loss: 0.0890 acc: 88.66\n",
      "ep: 3900 loss: 0.0881 acc: 88.66\n",
      "ep: 4000 loss: 0.0872 acc: 89.18\n",
      "ep: 4100 loss: 0.0864 acc: 89.18\n",
      "ep: 4200 loss: 0.0856 acc: 89.18\n",
      "ep: 4300 loss: 0.0848 acc: 89.69\n",
      "ep: 4400 loss: 0.0840 acc: 89.69\n",
      "ep: 4500 loss: 0.0833 acc: 89.69\n",
      "ep: 4600 loss: 0.0826 acc: 90.72\n",
      "ep: 4700 loss: 0.0819 acc: 90.72\n",
      "ep: 4800 loss: 0.0812 acc: 90.72\n",
      "ep: 4900 loss: 0.0806 acc: 90.72\n",
      "ep: 5000 loss: 0.0799 acc: 90.72\n",
      "ep: 5100 loss: 0.0793 acc: 90.72\n",
      "ep: 5200 loss: 0.0788 acc: 90.72\n",
      "ep: 5300 loss: 0.0783 acc: 90.72\n",
      "ep: 5400 loss: 0.0778 acc: 91.24\n",
      "ep: 5500 loss: 0.0773 acc: 90.72\n",
      "ep: 5600 loss: 0.0769 acc: 90.72\n",
      "init:1.7000000000000002, acc:91.23711395263672 , epoch:5607\n",
      "ep:  100 loss: 0.5784 acc: 60.82\n",
      "ep:  200 loss: 0.4636 acc: 64.95\n",
      "ep:  300 loss: 0.4089 acc: 67.53\n",
      "ep:  400 loss: 0.3807 acc: 68.56\n",
      "ep:  500 loss: 0.3293 acc: 69.59\n",
      "ep:  600 loss: 0.3126 acc: 71.13\n",
      "ep:  700 loss: 0.2982 acc: 73.20\n",
      "ep:  800 loss: 0.2819 acc: 73.20\n",
      "ep:  900 loss: 0.2719 acc: 72.16\n",
      "ep: 1000 loss: 0.2629 acc: 72.16\n",
      "ep: 1100 loss: 0.2546 acc: 73.20\n",
      "ep: 1200 loss: 0.2468 acc: 72.68\n",
      "ep: 1300 loss: 0.2396 acc: 73.20\n",
      "ep: 1400 loss: 0.2337 acc: 73.20\n",
      "ep: 1500 loss: 0.2289 acc: 73.20\n",
      "ep: 1600 loss: 0.2234 acc: 73.20\n",
      "ep: 1700 loss: 0.2086 acc: 74.74\n",
      "ep: 1800 loss: 0.1980 acc: 75.77\n",
      "ep: 1900 loss: 0.1912 acc: 78.35\n",
      "ep: 2000 loss: 0.1884 acc: 77.32\n",
      "ep: 2100 loss: 0.1865 acc: 77.84\n",
      "ep: 2200 loss: 0.1849 acc: 77.84\n",
      "ep: 2300 loss: 0.1830 acc: 77.32\n",
      "ep: 2400 loss: 0.1916 acc: 79.90\n",
      "ep: 2500 loss: 0.1841 acc: 80.41\n",
      "ep: 2600 loss: 0.1745 acc: 80.41\n",
      "ep: 2700 loss: 0.1669 acc: 80.93\n",
      "ep: 2800 loss: 0.1617 acc: 80.93\n",
      "ep: 2900 loss: 0.1576 acc: 83.51\n",
      "ep: 3000 loss: 0.1543 acc: 83.51\n",
      "ep: 3100 loss: 0.1515 acc: 83.51\n",
      "ep: 3200 loss: 0.1492 acc: 82.99\n",
      "ep: 3300 loss: 0.1471 acc: 82.99\n",
      "ep: 3400 loss: 0.1453 acc: 83.51\n",
      "ep: 3500 loss: 0.1437 acc: 83.51\n",
      "ep: 3600 loss: 0.1419 acc: 84.02\n",
      "ep: 3700 loss: 0.1403 acc: 84.02\n",
      "ep: 3800 loss: 0.1388 acc: 83.51\n",
      "ep: 3900 loss: 0.1374 acc: 84.02\n",
      "ep: 4000 loss: 0.1361 acc: 83.51\n",
      "ep: 4100 loss: 0.1350 acc: 82.99\n",
      "ep: 4200 loss: 0.1339 acc: 82.47\n",
      "ep: 4300 loss: 0.1329 acc: 82.47\n",
      "ep: 4400 loss: 0.1320 acc: 82.47\n",
      "ep: 4500 loss: 0.1308 acc: 82.47\n",
      "ep: 4600 loss: 0.1300 acc: 82.47\n",
      "ep: 4700 loss: 0.1293 acc: 83.51\n",
      "ep: 4800 loss: 0.1288 acc: 82.99\n",
      "ep: 4900 loss: 0.1283 acc: 82.99\n",
      "ep: 5000 loss: 0.1277 acc: 82.99\n",
      "ep: 5100 loss: 0.1269 acc: 84.02\n",
      "ep: 5200 loss: 0.1229 acc: 84.54\n",
      "ep: 5300 loss: 0.1189 acc: 85.57\n",
      "ep: 5400 loss: 0.1169 acc: 85.57\n",
      "ep: 5500 loss: 0.1158 acc: 85.05\n",
      "ep: 5600 loss: 0.1150 acc: 85.05\n",
      "init:1.8000000000000003, acc:84.53607940673828 , epoch:5606\n",
      "ep:  100 loss: 0.5872 acc: 61.34\n",
      "ep:  200 loss: 0.5562 acc: 63.40\n",
      "ep:  300 loss: 0.5410 acc: 62.37\n",
      "ep:  400 loss: 0.5248 acc: 63.92\n",
      "ep:  500 loss: 0.4819 acc: 63.92\n",
      "ep:  600 loss: 0.4350 acc: 65.46\n",
      "ep:  700 loss: 0.3770 acc: 65.98\n",
      "ep:  800 loss: 0.3539 acc: 66.49\n",
      "ep:  900 loss: 0.3279 acc: 66.49\n",
      "ep: 1000 loss: 0.3045 acc: 68.56\n",
      "ep: 1100 loss: 0.2885 acc: 67.53\n",
      "ep: 1200 loss: 0.2710 acc: 69.07\n",
      "ep: 1300 loss: 0.2565 acc: 69.59\n",
      "ep: 1400 loss: 0.2453 acc: 70.10\n",
      "ep: 1500 loss: 0.2370 acc: 70.10\n",
      "ep: 1600 loss: 0.2279 acc: 71.13\n",
      "ep: 1700 loss: 0.2201 acc: 72.16\n",
      "ep: 1800 loss: 0.2154 acc: 73.20\n",
      "ep: 1900 loss: 0.2113 acc: 73.20\n",
      "ep: 2000 loss: 0.2062 acc: 73.20\n",
      "ep: 2100 loss: 0.1992 acc: 73.71\n",
      "ep: 2200 loss: 0.1948 acc: 76.29\n",
      "ep: 2300 loss: 0.1916 acc: 76.29\n",
      "ep: 2400 loss: 0.1891 acc: 76.29\n",
      "ep: 2500 loss: 0.1871 acc: 76.80\n",
      "ep: 2600 loss: 0.1852 acc: 77.32\n",
      "ep: 2700 loss: 0.1828 acc: 77.32\n",
      "ep: 2800 loss: 0.1799 acc: 76.29\n",
      "ep: 2900 loss: 0.1785 acc: 77.84\n",
      "ep: 3000 loss: 0.1771 acc: 77.84\n",
      "ep: 3100 loss: 0.1756 acc: 77.32\n",
      "ep: 3200 loss: 0.1741 acc: 77.84\n",
      "ep: 3300 loss: 0.1726 acc: 77.84\n",
      "ep: 3400 loss: 0.1662 acc: 78.87\n",
      "ep: 3500 loss: 0.1614 acc: 78.35\n",
      "ep: 3600 loss: 0.1587 acc: 78.35\n",
      "ep: 3700 loss: 0.1567 acc: 78.87\n",
      "ep: 3800 loss: 0.1544 acc: 78.35\n",
      "ep: 3900 loss: 0.1505 acc: 78.87\n",
      "ep: 4000 loss: 0.1477 acc: 78.87\n",
      "ep: 4100 loss: 0.1462 acc: 78.87\n",
      "ep: 4200 loss: 0.1448 acc: 78.35\n",
      "ep: 4300 loss: 0.1434 acc: 78.35\n",
      "ep: 4400 loss: 0.1420 acc: 79.38\n",
      "ep: 4500 loss: 0.1405 acc: 79.38\n",
      "ep: 4600 loss: 0.1389 acc: 79.38\n",
      "ep: 4700 loss: 0.1371 acc: 79.90\n",
      "ep: 4800 loss: 0.1352 acc: 79.90\n",
      "ep: 4900 loss: 0.1334 acc: 79.90\n",
      "ep: 5000 loss: 0.1317 acc: 80.41\n",
      "ep: 5100 loss: 0.1303 acc: 80.41\n",
      "ep: 5200 loss: 0.1291 acc: 80.41\n",
      "ep: 5300 loss: 0.1282 acc: 80.93\n",
      "ep: 5400 loss: 0.1274 acc: 80.93\n",
      "ep: 5500 loss: 0.1266 acc: 80.93\n",
      "ep: 5600 loss: 0.1257 acc: 81.96\n",
      "init:1.9000000000000001, acc:81.9587631225586 , epoch:5605\n"
     ]
    }
   ],
   "source": [
    "num_input = data.shape[1] - 1\n",
    "\n",
    "full_input  = data[:,0:num_input]\n",
    "full_target = data[:,num_input:num_input+1]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(full_input,full_target)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset,batch_size=97)\n",
    "\n",
    "epoch = 20000\n",
    "y_epoch_converge = []\n",
    "x_init_vals = []\n",
    "z_accuracy = []\n",
    "for init in p:\n",
    "    net = RawNet(10)\n",
    "\n",
    "    if list(net.parameters()):\n",
    "        # initialize weight values\n",
    "        for m in list(net.parameters()):\n",
    "            m.data.normal_(0,init)\n",
    "\n",
    "        # use Adam optimizer\n",
    "        optimizer = torch.optim.Adam(net.parameters(),eps=0.000001,lr=.01,\n",
    "                                     betas=(0.9,0.999),weight_decay=0.0001)\n",
    "        # training loop\n",
    "        for epoch in range(1, epoch):\n",
    "            accuracy = train(net, train_loader, optimizer)\n",
    "            if accuracy == 100:\n",
    "                print(\"Achieves 100%\")\n",
    "                break\n",
    "\n",
    "    y_epoch_converge.append(epoch)\n",
    "    x_init_vals.append(init)\n",
    "    z_accuracy.append(accuracy)\n",
    "    print(f\"init:{init}, acc:{accuracy} , epoch:{epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 1.0,\n",
       " 1.1,\n",
       " 1.2000000000000002,\n",
       " 1.3000000000000003,\n",
       " 1.4000000000000001,\n",
       " 1.5000000000000002,\n",
       " 1.6,\n",
       " 1.7000000000000002,\n",
       " 1.8000000000000003,\n",
       " 1.9000000000000001]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_init_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19999,\n",
       " 5622,\n",
       " 5621,\n",
       " 5620,\n",
       " 5619,\n",
       " 5618,\n",
       " 5617,\n",
       " 5616,\n",
       " 5615,\n",
       " 5614,\n",
       " 5613,\n",
       " 5612,\n",
       " 5611,\n",
       " 5610,\n",
       " 5609,\n",
       " 5608,\n",
       " 5607,\n",
       " 5606,\n",
       " 5605]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_epoch_converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 1.0,\n",
       " 1.1,\n",
       " 1.2000000000000002,\n",
       " 1.3000000000000003,\n",
       " 1.4000000000000001,\n",
       " 1.5000000000000002,\n",
       " 1.6,\n",
       " 1.7000000000000002,\n",
       " 1.8000000000000003,\n",
       " 1.9000000000000001]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_init_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'epoch')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_init_vals,y_epoch_converge)\n",
    "plt.xlabel('Initial weights')\n",
    "plt.ylabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.active1[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97])\n",
      "torch.Size([97])\n",
      "1875300\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(net.active1[:,0].shape)\n",
    "    net.eval() \n",
    "    print(net.active1[:,0].shape)\n",
    "    output = net(grid)\n",
    "    print(net.active1[:,0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADRJJREFUeJzt3V+MpXV9x/H3p7vSpsqGxh1Cwi5dNlVbqjSQkdSQ2hbQoBK86QUmGtSLTU0lmGiQP+lVb5raqCSaNhPAGzchDWI1Bv9A1Ca9YOvwTwqLhlCVVQnDRbOmJpIN317ModkuszPnnOeZec7zm/crIdnzZ3/nCxne++x3z+xJVSFJasdvDT2AJKlfhl2SGmPYJakxhl2SGmPYJakxhl2SGmPYJakxhl2SGmPYJakxe4d40f37z61Dh5aGeGlto0ce+a+hR5Ba91JVbRnPQcJ+6NASx1b/boiX1jbamw8PPYLUuFM/neZZrmIkqTGGXZIaY9jVC9cw0uIw7JLUGMMuSY0x7OrMNYy0WHoJe5LzktyX5Jkkx5O8o49zJUmz6+t97HcC36qqv0pyDvC7PZ0rSZpR57An2Qe8E/gwQFW9DLzc9VyNg2sYafH0sYo5DKwBX0ryWJK7kry+h3MlSXPoI+x7gcuBf6qqy4D/AW4980lJjiRZTbK6tnayh5fV0LxalxZTH2E/AZyoqmOT2/exHvr/p6pWqmq5qpaXlvb18LKSpI10DntVvQA8n+Qtk7uuBp7ueq4kaT59vSvmJuDo5B0xzwEf6elcSdKMegl7VT0OLPdxlsbB/bq0uPzOU0lqjGGXpMYYds3MNYy02Ay7JDXGsEtSYwy7ZuIaRlp8hl2SGmPYJakxhl1Tcw0jjYNhl6TGGHZJaoxh11Rcw0jjYdglqTGGXZIaY9i1Jdcw0rgYdklqjGGXpMYYdm3KNYw0PoZdkhpj2CWpMYZdZ+UaRhonwy5JjTHsktQYw64NuYaRxqu3sCfZk+SxJN/o60xJ0uz6vGK/GTje43kaiFfr0rj1EvYkB4D3AXf1cZ4kaX59XbF/HrgFeOVsT0hyJMlqktW1tZM9vawk6Uydw57kOuDFqnpks+dV1UpVLVfV8tLSvq4vK0k6iz6u2K8Erk/yE+Be4KokX+7hXA3A/bo0fp3DXlW3VdWBqjoE3AB8t6o+2HkySdJcfB+7JDVmb5+HVdX3ge/3eaZ2jmsYqQ1esUtSYwy7JDXGsAtwDSO1xLBLUmMMuyQ1xrDLNYzUGMMuSY0x7LucV+tSewy7JDXGsEtSYwy7JDXGsO9i7telNhl2SWqMYZekxhj2Xco1jNQuwy5JjTHsktQYw74LuYaR2mbYJakxhl2SGmPYdxnXMFL7DLskNcawS1JjOoc9ycEk30tyPMlTSW7uYzD1zzWMtDvs7eGMU8Anq+rRJOcCjyR5sKqe7uFsSdKMOl+xV9Uvq+rRyY9/BRwHLux6riRpPr3u2JMcAi4DjvV5rrpzDSPtHr2FPckbgK8An6iqkxs8fiTJapLVtbXXPCxJ6kkvYU/yOtajfrSq7t/oOVW1UlXLVbW8tLSvj5eVJG2gj3fFBLgbOF5Vn+0+kvrmGkbaXfq4Yr8S+BBwVZLHJ/+8t4dzJUlz6Px2x6r6dyA9zCJJ6oHfedo41zDS7mPYJakxhl2SGmPYG+YaRtqdDLskNcawS1JjDHujXMNIu5dhl6TGGHZJaoxhb5BrGGl3M+yS1BjDLkmNMeyNcQ0jybBLUmMMuyQ1xrA3xDWMJDDsktQcwy5JjTHsjXANI+lVhl2SGmPYJakxhr0BrmEknc6wS1JjDLskNaaXsCe5NsmPkjyb5NY+ztR0XMNIOlPnsCfZA3wReA9wCfCBJJd0PVeSNJ8+rtivAJ6tqueq6mXgXuD9PZwrSZpDH2G/EHj+tNsnJvdpm7mGkbSRPsKeDe6r1zwpOZJkNcnq2trJHl5WkrSRPsJ+Ajh42u0DwC/OfFJVrVTVclUtLy3t6+FlJUkb6SPsPwDelOTiJOcANwBf7+FcbcI1jKSz2dv1gKo6leTjwLeBPcA9VfVU58kkSXPpHHaAqnoAeKCPsyRJ3fidpyPkGkbSZgy7JDXGsEtSYwz7yLiGkbQVwy5JjTHsktQYwz4irmEkTcOwS1JjDLskNcawj4RrGEnTMuyS1BjDLkmNMewj4BpG0iwMuyQ1xrBLUmMM+4JzDSNpVoZdkhpj2CWpMYZ9gbmGkTQPwy5JjTHsktQYw76gXMNImpdhl6TGGHZJakynsCf5TJJnkvwwyVeTnNfXYLuZaxhJXXS9Yn8QeGtVXQr8GLit+0iSpC46hb2qvlNVpyY3HwYOdB9JktRFnzv2jwLf7PG8Xck1jKSu9m71hCQPARds8NAdVfW1yXPuAE4BRzc55whwBOCii94417CSpK1tGfaqumazx5PcCFwHXF1Vtck5K8AKwPLy4bM+T5LUzZZh30ySa4FPA39eVb/uZ6TdyzWMpD503bF/ATgXeDDJ40n+uYeZJEkddLpir6o/6GsQSVI//M7TBeEaRlJfDLskNcawS1JjDPsCcA0jqU+GXZIaY9glqTGGfWCuYST1zbBLUmMMuyQ1xrAPyDWMpO1g2CWpMYZdkhpj2AfiGkbSdjHsktQYwy5JjTHsA3ANI2k7GXZJaoxhl6TGGPYd5hpG0nYz7JLUGMMuSY0x7DvINYyknWDYJakxhl2SGtNL2JN8Kkkl2d/HeS1yDSNpp3QOe5KDwLuAn3UfR5LUVR9X7J8DbgGqh7MkSR11CnuS64GfV9UTUzz3SJLVJKtraye7vOzouIaRtJP2bvWEJA8BF2zw0B3A7cC7p3mhqloBVgCWlw97dS9J22TLsFfVNRvdn+RtwMXAE0kADgCPJrmiql7odUpJ0tS2DPvZVNWTwPmv3k7yE2C5ql7qYa5muIaRtNN8H7skNWbuK/YzVdWhvs6SJM3PK/Zt5BpG0hAMuyQ1xrBLUmMM+zZxDSNpKIZdkhpj2CWpMYZ9G7iGkTQkwy5JjTHsktQYw94z1zCShmbYJakxhl2SGmPYe+QaRtIiMOyS1BjDLkmNMew9cQ0jaVEYdklqjGGXpMYY9h64hpG0SAy7JDXGsEtSYwx7R65hJC0awy5Jjekc9iQ3JflRkqeS/EMfQ0mS5re3y09O8pfA+4FLq+o3Sc7vZyxJ0ry6XrF/DPj7qvoNQFW92H2k8XC/LmkRdQ37m4E/S3Isyb8leXsfQ0mS5rflKibJQ8AFGzx0x+Tn/x7wp8DbgX9JcriqaoNzjgBHAC666I1dZl4IXq1LWlRbhr2qrjnbY0k+Btw/Cfl/JHkF2A+sbXDOCrACsLx8+DXhlyT1o+sq5l+BqwCSvBk4B3ip61CSpPl1elcMcA9wT5L/BF4GbtxoDdMa1zCSFlmnsFfVy8AHe5pFktQDv/NUkhpj2GfkGkbSojPsktQYwy5JjTHsM3ANI2kMMsS7E5OsAT/dxpfYz3jfTz/m2WHc8495dhj3/GOeHXZu/t+vqqWtnjRI2LdbktWqWh56jnmMeXYY9/xjnh3GPf+YZ4fFm99VjCQ1xrBLUmNaDfvK0AN0MObZYdzzj3l2GPf8Y54dFmz+JnfskrSbtXrFLkm7VrNhb+FDtpN8Kkkl2T/0LNNK8pkkzyT5YZKvJjlv6JmmkeTaydfLs0luHXqeaSU5mOR7SY5PvtZvHnqmeSTZk+SxJN8YepZZJDkvyX2Tr/njSd4x9EzQaNjP+JDtPwb+ceCRZpbkIPAu4GdDzzKjB4G3VtWlwI+B2waeZ0tJ9gBfBN4DXAJ8IMklw041tVPAJ6vqj1j/JLO/GdHsp7sZOD70EHO4E/hWVf0h8CcsyL9Dk2GnjQ/Z/hxwCzCqPwSpqu9U1anJzYeBA0POM6UrgGer6rnJX0V9L+sXBguvqn5ZVY9Ofvwr1sNy4bBTzSbJAeB9wF1DzzKLJPuAdwJ3w/pfY15V/z3sVOtaDfuoP2Q7yfXAz6vqiaFn6eijwDeHHmIKFwLPn3b7BCOLI0CSQ8BlwLFhJ5nZ51m/iHll6EFmdJj1jwH90mSNdFeS1w89FHT/BKXB9PUh20PZYv7bgXfv7ETT22z2qvra5Dl3sL4mOLqTs80pG9y3MF8r00jyBuArwCeq6uTQ80wryXXAi1X1SJK/GHqeGe0FLgduqqpjSe4EbgX+dtixRhz2vj5keyhnmz/J24CLgSeSwPoq49EkV1TVCzs44llt9t8eIMmNwHXA1Yv0i+kmTgAHT7t9APjFQLPMLMnrWI/60aq6f+h5ZnQlcH2S9wK/A+xL8uWqGsMns50ATlTVq79Duo/1sA+u1VXMaD9ku6qerKrzq+pQVR1i/Yvn8kWJ+laSXAt8Gri+qn499DxT+gHwpiQXJzkHuAH4+sAzTSXrv/rfDRyvqs8OPc+squq2qjow+Vq/AfjuSKLO5P/J55O8ZXLX1cDTA470f0Z7xb6FXfkh2wviC8BvAw9OfsfxcFX99bAjba6qTiX5OPBtYA9wT1U9NfBY07oS+BDwZJLHJ/fdXlUPDDjTbnITcHRyQfAc8JGB5wH8zlNJak6rqxhJ2rUMuyQ1xrBLUmMMuyQ1xrBLUmMMuyQ1xrBLUmMMuyQ15n8ByWLpwXkjexIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # suppress updating of gradients\n",
    "    # plot function computed by model\n",
    "    net.eval()\n",
    "    net(grid)\n",
    "    hid = net.hidlayer[0]\n",
    "    pred = (hid[:,0].view(hid.shape[0],1) >= 0.5).float()\n",
    "    plt.clf()\n",
    "    plt.pcolormesh(xrange,yrange,pred.cpu().view(yrange.size()[0],xrange.size()[0]), cmap='inferno')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1875300, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6938,  0.6930,  0.6921,  ..., -0.6984, -0.6992, -0.6999])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.active1[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1875300, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " with torch.no_grad(): # suppress updating of gradients\n",
    "    net.eval()        # toggle batch norm, dropout\n",
    "    output = net(grid)\n",
    "    net.train() # toggle batch norm, dropout back again\n",
    "\n",
    "    pred = (output >= 0.5).float()\n",
    "    \n",
    "pred.shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.0000, -6.9900, -6.9800,  ...,  7.0700,  7.0800,  7.0900])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x1bfe46c6048>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.pcolormesh(xrange,yrange,pred.cpu().view(yrange.size()[0],xrange.size()[0]), cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1330, 1410]' is invalid for input of size 937650000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-47be79e5d057>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myrange\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxrange\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1330, 1410]' is invalid for input of size 937650000"
     ]
    }
   ],
   "source": [
    "net.active1.cpu().view(yrange.size()[0],xrange.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.active1[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.active1[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (net.active1[:,0] >= 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolarNet(torch.nn.Module):\n",
    "    def __init__(self, num_hid):\n",
    "        super(PolarNet, self).__init__()\n",
    "        self.feed = nn.Linear(2,num_hid) \n",
    "        self.hid = nn.Linear(num_hid,1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input[:,0]\n",
    "        y = input[:,1]\n",
    "        out = torch.zeros(input.shape,dtype=torch.float32)\n",
    "        out[:,0] = torch.sqrt(x*x + y*y) #tranform data\n",
    "        out[:,1] = torch.atan2(y,x)\n",
    "        #here we start the forward pass\n",
    "        self.active1 = torch.tanh(self.feed(out))\n",
    "        self.active2 = self.hid(self.active1)\n",
    "        self.hidlayer = [self.active1]\n",
    "        return F.sigmoid(self.active2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:  100 loss: 0.6966 acc: 51.55\n",
      "ep:  200 loss: 0.7009 acc: 52.58\n",
      "ep:  300 loss: 0.7008 acc: 52.58\n",
      "ep:  400 loss: 0.7001 acc: 53.61\n",
      "ep:  500 loss: 0.6994 acc: 53.09\n",
      "ep:  600 loss: 0.6987 acc: 52.06\n",
      "ep:  700 loss: 0.6981 acc: 52.06\n",
      "ep:  800 loss: 0.6976 acc: 52.06\n",
      "ep:  900 loss: 0.6972 acc: 51.55\n",
      "ep: 1000 loss: 0.6968 acc: 52.58\n",
      "ep: 1100 loss: 0.6964 acc: 52.58\n",
      "ep: 1200 loss: 0.6961 acc: 52.58\n",
      "ep: 1300 loss: 0.6958 acc: 53.09\n",
      "ep: 1400 loss: 0.6956 acc: 53.09\n",
      "ep: 1500 loss: 0.6954 acc: 52.06\n",
      "ep: 1600 loss: 0.6952 acc: 52.06\n",
      "ep: 1700 loss: 0.6951 acc: 52.06\n",
      "ep: 1800 loss: 0.6950 acc: 51.55\n",
      "ep: 1900 loss: 0.6949 acc: 52.06\n",
      "ep: 2000 loss: 0.6949 acc: 51.03\n",
      "ep: 2100 loss: 0.6948 acc: 51.03\n",
      "ep: 2200 loss: 0.6948 acc: 51.55\n",
      "ep: 2300 loss: 0.6948 acc: 51.03\n",
      "ep: 2400 loss: 0.6948 acc: 51.03\n",
      "ep: 2500 loss: 0.6948 acc: 51.03\n",
      "ep: 2600 loss: 0.6948 acc: 51.03\n",
      "ep: 2700 loss: 0.6948 acc: 51.03\n",
      "ep: 2800 loss: 0.6948 acc: 51.03\n",
      "ep: 2900 loss: 0.6948 acc: 51.03\n",
      "ep: 3000 loss: 0.6948 acc: 51.03\n",
      "ep: 3100 loss: 0.6948 acc: 51.03\n",
      "ep: 3200 loss: 0.6948 acc: 51.03\n",
      "ep: 3300 loss: 0.6948 acc: 51.03\n",
      "ep: 3400 loss: 0.6948 acc: 51.55\n",
      "ep: 3500 loss: 0.6948 acc: 51.55\n",
      "ep: 3600 loss: 0.6948 acc: 51.55\n",
      "ep: 3700 loss: 0.6948 acc: 51.55\n",
      "ep: 3800 loss: 0.6948 acc: 51.55\n",
      "ep: 3900 loss: 0.6948 acc: 51.55\n",
      "ep: 4000 loss: 0.6948 acc: 51.55\n",
      "ep: 4100 loss: 0.6948 acc: 51.55\n",
      "ep: 4200 loss: 0.6948 acc: 51.03\n",
      "ep: 4300 loss: 0.6948 acc: 51.55\n",
      "ep: 4400 loss: 0.6948 acc: 51.55\n",
      "ep: 4500 loss: 0.6948 acc: 51.03\n",
      "ep: 4600 loss: 0.6948 acc: 51.55\n",
      "ep: 4700 loss: 0.6948 acc: 51.55\n",
      "ep: 4800 loss: 0.6948 acc: 51.03\n",
      "ep: 4900 loss: 0.6948 acc: 51.55\n",
      "ep: 5000 loss: 0.6948 acc: 51.55\n",
      "ep: 5100 loss: 0.6948 acc: 51.55\n",
      "ep: 5200 loss: 0.6948 acc: 51.55\n",
      "ep: 5300 loss: 0.6948 acc: 51.03\n",
      "ep: 5400 loss: 0.6948 acc: 51.55\n",
      "ep: 5500 loss: 0.6948 acc: 51.03\n",
      "ep: 5600 loss: 0.6948 acc: 51.03\n",
      "ep: 5700 loss: 0.6948 acc: 51.55\n",
      "ep: 5800 loss: 0.6948 acc: 51.03\n",
      "ep: 5900 loss: 0.6948 acc: 51.03\n",
      "ep: 6000 loss: 0.6948 acc: 51.55\n",
      "ep: 6100 loss: 0.6948 acc: 51.03\n",
      "ep: 6200 loss: 0.6948 acc: 51.03\n",
      "ep: 6300 loss: 0.6948 acc: 51.03\n",
      "ep: 6400 loss: 0.6948 acc: 51.03\n",
      "ep: 6500 loss: 0.6948 acc: 51.03\n",
      "ep: 6600 loss: 0.6948 acc: 51.03\n",
      "ep: 6700 loss: 0.6948 acc: 51.03\n",
      "ep: 6800 loss: 0.6948 acc: 51.03\n",
      "ep: 6900 loss: 0.6948 acc: 51.55\n",
      "ep: 7000 loss: 0.6948 acc: 51.03\n",
      "ep: 7100 loss: 0.6948 acc: 51.03\n",
      "ep: 7200 loss: 0.6948 acc: 51.03\n",
      "ep: 7300 loss: 0.6948 acc: 51.03\n",
      "ep: 7400 loss: 0.6948 acc: 51.03\n",
      "ep: 7500 loss: 0.6948 acc: 51.03\n",
      "ep: 7600 loss: 0.6948 acc: 51.03\n",
      "ep: 7700 loss: 0.6948 acc: 51.03\n",
      "ep: 7800 loss: 0.6948 acc: 51.03\n",
      "ep: 7900 loss: 0.6948 acc: 51.03\n",
      "ep: 8000 loss: 0.6948 acc: 51.03\n",
      "ep: 8100 loss: 0.6948 acc: 51.55\n",
      "ep: 8200 loss: 0.6948 acc: 51.03\n",
      "ep: 8300 loss: 0.6948 acc: 51.03\n",
      "ep: 8400 loss: 0.6948 acc: 51.55\n",
      "ep: 8500 loss: 0.6948 acc: 51.03\n",
      "ep: 8600 loss: 0.6948 acc: 51.03\n",
      "ep: 8700 loss: 0.6948 acc: 51.03\n",
      "ep: 8800 loss: 0.6948 acc: 51.03\n",
      "ep: 8900 loss: 0.6948 acc: 51.03\n",
      "ep: 9000 loss: 0.6948 acc: 51.03\n",
      "ep: 9100 loss: 0.6948 acc: 51.03\n",
      "ep: 9200 loss: 0.6948 acc: 51.03\n",
      "ep: 9300 loss: 0.6948 acc: 51.03\n",
      "ep: 9400 loss: 0.6948 acc: 51.03\n",
      "ep: 9500 loss: 0.6948 acc: 51.03\n",
      "ep: 9600 loss: 0.6948 acc: 51.03\n",
      "ep: 9700 loss: 0.6948 acc: 51.03\n",
      "ep: 9800 loss: 0.6948 acc: 51.03\n",
      "ep: 9900 loss: 0.6948 acc: 51.03\n",
      "ep:10000 loss: 0.6948 acc: 51.03\n",
      "ep:10100 loss: 0.6948 acc: 51.03\n",
      "ep:10200 loss: 0.6948 acc: 51.03\n",
      "ep:10300 loss: 0.6948 acc: 51.03\n",
      "ep:10400 loss: 0.6948 acc: 51.03\n",
      "ep:10500 loss: 0.6948 acc: 51.03\n",
      "ep:10600 loss: 0.6948 acc: 51.03\n",
      "ep:10700 loss: 0.6948 acc: 51.03\n",
      "ep:10800 loss: 0.6948 acc: 51.55\n",
      "ep:10900 loss: 0.6948 acc: 51.03\n",
      "ep:11000 loss: 0.6948 acc: 51.03\n",
      "ep:11100 loss: 0.6948 acc: 51.03\n",
      "ep:11200 loss: 0.6948 acc: 51.03\n",
      "ep:11300 loss: 0.6948 acc: 51.03\n",
      "ep:11400 loss: 0.6948 acc: 51.03\n",
      "ep:11500 loss: 0.6948 acc: 51.03\n",
      "ep:11600 loss: 0.6948 acc: 51.03\n",
      "ep:11700 loss: 0.6948 acc: 51.03\n",
      "ep:11800 loss: 0.6948 acc: 51.03\n",
      "ep:11900 loss: 0.6948 acc: 51.03\n",
      "ep:12000 loss: 0.6948 acc: 51.03\n",
      "ep:12100 loss: 0.6948 acc: 51.55\n",
      "ep:12200 loss: 0.6948 acc: 51.03\n",
      "ep:12300 loss: 0.6948 acc: 51.03\n",
      "ep:12400 loss: 0.6948 acc: 51.03\n",
      "ep:12500 loss: 0.6948 acc: 51.03\n",
      "ep:12600 loss: 0.6948 acc: 51.03\n",
      "ep:12700 loss: 0.6948 acc: 51.55\n",
      "ep:12800 loss: 0.6948 acc: 51.03\n",
      "ep:12900 loss: 0.6948 acc: 51.03\n",
      "ep:13000 loss: 0.6948 acc: 51.03\n",
      "ep:13100 loss: 0.6948 acc: 51.03\n",
      "ep:13200 loss: 0.6948 acc: 51.03\n",
      "ep:13300 loss: 0.6948 acc: 51.03\n",
      "ep:13400 loss: 0.6948 acc: 51.03\n",
      "ep:13500 loss: 0.6948 acc: 51.03\n",
      "ep:13600 loss: 0.6948 acc: 51.55\n",
      "ep:13700 loss: 0.6948 acc: 51.03\n",
      "ep:13800 loss: 0.6948 acc: 51.03\n",
      "ep:13900 loss: 0.6948 acc: 51.03\n",
      "ep:14000 loss: 0.6948 acc: 51.03\n",
      "ep:14100 loss: 0.6948 acc: 51.03\n",
      "ep:14200 loss: 0.6948 acc: 51.55\n",
      "ep:14300 loss: 0.6948 acc: 51.03\n",
      "ep:14400 loss: 0.6948 acc: 51.03\n",
      "ep:14500 loss: 0.6948 acc: 51.03\n",
      "ep:14600 loss: 0.6948 acc: 51.03\n",
      "ep:14700 loss: 0.6948 acc: 51.03\n",
      "ep:14800 loss: 0.6948 acc: 51.03\n",
      "ep:14900 loss: 0.6948 acc: 51.03\n",
      "ep:15000 loss: 0.6948 acc: 51.03\n",
      "ep:15100 loss: 0.6948 acc: 51.55\n",
      "ep:15200 loss: 0.6948 acc: 51.03\n",
      "ep:15300 loss: 0.6948 acc: 51.03\n",
      "ep:15400 loss: 0.6948 acc: 51.55\n",
      "ep:15500 loss: 0.6948 acc: 51.03\n",
      "ep:15600 loss: 0.6948 acc: 51.03\n",
      "ep:15700 loss: 0.6948 acc: 51.03\n",
      "ep:15800 loss: 0.6948 acc: 51.03\n",
      "ep:15900 loss: 0.6948 acc: 51.03\n",
      "ep:16000 loss: 0.6948 acc: 51.03\n",
      "ep:16100 loss: 0.6948 acc: 51.03\n",
      "ep:16200 loss: 0.6948 acc: 51.03\n",
      "ep:16300 loss: 0.6948 acc: 51.55\n",
      "ep:16400 loss: 0.6948 acc: 51.03\n",
      "ep:16500 loss: 0.6948 acc: 51.03\n",
      "ep:16600 loss: 0.6948 acc: 51.03\n",
      "ep:16700 loss: 0.6948 acc: 51.03\n",
      "ep:16800 loss: 0.6948 acc: 51.55\n",
      "ep:16900 loss: 0.6948 acc: 51.03\n",
      "ep:17000 loss: 0.6948 acc: 51.03\n",
      "ep:17100 loss: 0.6948 acc: 51.55\n",
      "ep:17200 loss: 0.6948 acc: 51.03\n",
      "ep:17300 loss: 0.6948 acc: 51.03\n",
      "ep:17400 loss: 0.6948 acc: 51.03\n",
      "ep:17500 loss: 0.6948 acc: 51.03\n",
      "ep:17600 loss: 0.6948 acc: 51.03\n",
      "ep:17700 loss: 0.6948 acc: 51.03\n",
      "ep:17800 loss: 0.6948 acc: 51.03\n",
      "ep:17900 loss: 0.6948 acc: 51.03\n",
      "ep:18000 loss: 0.6948 acc: 51.03\n",
      "ep:18100 loss: 0.6948 acc: 51.03\n",
      "ep:18200 loss: 0.6948 acc: 51.03\n",
      "ep:18300 loss: 0.6948 acc: 51.03\n",
      "ep:18400 loss: 0.6948 acc: 51.03\n",
      "ep:18500 loss: 0.6948 acc: 51.03\n",
      "ep:18600 loss: 0.6948 acc: 51.03\n",
      "ep:18700 loss: 0.6948 acc: 51.03\n",
      "ep:18800 loss: 0.6948 acc: 51.03\n",
      "ep:18900 loss: 0.6948 acc: 51.03\n",
      "ep:19000 loss: 0.6948 acc: 51.03\n",
      "ep:19100 loss: 0.6948 acc: 51.03\n",
      "ep:19200 loss: 0.6948 acc: 51.03\n",
      "ep:19300 loss: 0.6948 acc: 51.03\n",
      "ep:19400 loss: 0.6948 acc: 51.03\n",
      "ep:19500 loss: 0.6948 acc: 51.03\n",
      "ep:19600 loss: 0.6948 acc: 51.03\n",
      "ep:19700 loss: 0.6948 acc: 51.55\n",
      "ep:19800 loss: 0.6948 acc: 51.03\n",
      "ep:19900 loss: 0.6948 acc: 51.03\n",
      "hidden:1, acc:51.546390533447266 , epoch:19999\n",
      "ep:  100 loss: 0.6834 acc: 56.19\n",
      "ep:  200 loss: 0.6839 acc: 57.22\n",
      "ep:  300 loss: 0.6815 acc: 56.19\n",
      "ep:  400 loss: 0.6730 acc: 55.67\n",
      "ep:  500 loss: 0.6653 acc: 55.67\n",
      "ep:  600 loss: 0.6606 acc: 56.19\n",
      "ep:  700 loss: 0.6575 acc: 56.19\n",
      "ep:  800 loss: 0.6553 acc: 55.67\n",
      "ep:  900 loss: 0.6536 acc: 56.19\n",
      "ep: 1000 loss: 0.6523 acc: 56.19\n",
      "ep: 1100 loss: 0.6512 acc: 56.19\n",
      "ep: 1200 loss: 0.6502 acc: 56.19\n",
      "ep: 1300 loss: 0.6494 acc: 56.70\n",
      "ep: 1400 loss: 0.6487 acc: 56.19\n",
      "ep: 1500 loss: 0.6480 acc: 56.70\n",
      "ep: 1600 loss: 0.6474 acc: 56.70\n",
      "ep: 1700 loss: 0.6469 acc: 56.70\n",
      "ep: 1800 loss: 0.6464 acc: 56.70\n",
      "ep: 1900 loss: 0.6460 acc: 56.70\n",
      "ep: 2000 loss: 0.6457 acc: 56.70\n",
      "ep: 2100 loss: 0.6454 acc: 56.19\n",
      "ep: 2200 loss: 0.6451 acc: 56.19\n",
      "ep: 2300 loss: 0.6449 acc: 55.67\n",
      "ep: 2400 loss: 0.6447 acc: 55.67\n",
      "ep: 2500 loss: 0.6446 acc: 55.15\n",
      "ep: 2600 loss: 0.6445 acc: 55.67\n",
      "ep: 2700 loss: 0.6444 acc: 55.67\n",
      "ep: 2800 loss: 0.6444 acc: 55.15\n",
      "ep: 2900 loss: 0.6443 acc: 55.15\n",
      "ep: 3000 loss: 0.6443 acc: 55.67\n",
      "ep: 3100 loss: 0.6443 acc: 55.67\n",
      "ep: 3200 loss: 0.6442 acc: 55.67\n",
      "ep: 3300 loss: 0.6442 acc: 55.67\n",
      "ep: 3400 loss: 0.6442 acc: 55.67\n",
      "ep: 3500 loss: 0.6442 acc: 55.67\n",
      "ep: 3600 loss: 0.6442 acc: 55.67\n",
      "ep: 3700 loss: 0.6441 acc: 55.67\n",
      "ep: 3800 loss: 0.6441 acc: 55.67\n",
      "ep: 3900 loss: 0.6441 acc: 55.67\n",
      "ep: 4000 loss: 0.6441 acc: 55.67\n",
      "ep: 4100 loss: 0.6441 acc: 55.67\n",
      "ep: 4200 loss: 0.6441 acc: 55.67\n",
      "ep: 4300 loss: 0.6441 acc: 55.67\n",
      "ep: 4400 loss: 0.6441 acc: 55.67\n",
      "ep: 4500 loss: 0.6441 acc: 55.67\n",
      "ep: 4600 loss: 0.6440 acc: 55.67\n",
      "ep: 4700 loss: 0.6440 acc: 55.67\n",
      "ep: 4800 loss: 0.6440 acc: 55.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4900 loss: 0.6440 acc: 55.67\n",
      "ep: 5000 loss: 0.6440 acc: 55.67\n",
      "ep: 5100 loss: 0.6440 acc: 55.67\n",
      "ep: 5200 loss: 0.6440 acc: 55.67\n",
      "ep: 5300 loss: 0.6440 acc: 55.67\n",
      "ep: 5400 loss: 0.6440 acc: 55.67\n",
      "ep: 5500 loss: 0.6440 acc: 55.67\n",
      "ep: 5600 loss: 0.6440 acc: 55.67\n",
      "ep: 5700 loss: 0.6440 acc: 55.67\n",
      "ep: 5800 loss: 0.6440 acc: 55.67\n",
      "ep: 5900 loss: 0.6440 acc: 55.67\n",
      "ep: 6000 loss: 0.6440 acc: 55.67\n",
      "ep: 6100 loss: 0.6440 acc: 55.67\n",
      "ep: 6200 loss: 0.6440 acc: 55.67\n",
      "ep: 6300 loss: 0.6440 acc: 55.67\n",
      "ep: 6400 loss: 0.6440 acc: 55.67\n",
      "ep: 6500 loss: 0.6440 acc: 55.67\n",
      "ep: 6600 loss: 0.6440 acc: 55.67\n",
      "ep: 6700 loss: 0.6440 acc: 55.67\n",
      "ep: 6800 loss: 0.6440 acc: 55.67\n",
      "ep: 6900 loss: 0.6440 acc: 55.67\n",
      "ep: 7000 loss: 0.6440 acc: 55.67\n",
      "ep: 7100 loss: 0.6440 acc: 55.67\n",
      "ep: 7200 loss: 0.6440 acc: 55.67\n",
      "ep: 7300 loss: 0.6440 acc: 55.67\n",
      "ep: 7400 loss: 0.6440 acc: 55.67\n",
      "ep: 7500 loss: 0.6440 acc: 55.67\n",
      "ep: 7600 loss: 0.6440 acc: 55.67\n",
      "ep: 7700 loss: 0.6440 acc: 55.67\n",
      "ep: 7800 loss: 0.6440 acc: 55.67\n",
      "ep: 7900 loss: 0.6440 acc: 55.67\n",
      "ep: 8000 loss: 0.6440 acc: 55.67\n",
      "ep: 8100 loss: 0.6440 acc: 55.67\n",
      "ep: 8200 loss: 0.6440 acc: 55.67\n",
      "ep: 8300 loss: 0.6440 acc: 55.67\n",
      "ep: 8400 loss: 0.6440 acc: 55.67\n",
      "ep: 8500 loss: 0.6440 acc: 55.67\n",
      "ep: 8600 loss: 0.6440 acc: 55.67\n",
      "ep: 8700 loss: 0.6440 acc: 55.67\n",
      "ep: 8800 loss: 0.6440 acc: 55.67\n",
      "ep: 8900 loss: 0.6440 acc: 55.67\n",
      "ep: 9000 loss: 0.6440 acc: 55.67\n",
      "ep: 9100 loss: 0.6440 acc: 55.67\n",
      "ep: 9200 loss: 0.6440 acc: 55.67\n",
      "ep: 9300 loss: 0.6440 acc: 55.67\n",
      "ep: 9400 loss: 0.6440 acc: 55.67\n",
      "ep: 9500 loss: 0.6440 acc: 55.67\n",
      "ep: 9600 loss: 0.6440 acc: 55.67\n",
      "ep: 9700 loss: 0.6440 acc: 55.67\n",
      "ep: 9800 loss: 0.6440 acc: 55.67\n",
      "ep: 9900 loss: 0.6440 acc: 55.67\n",
      "ep:10000 loss: 0.6440 acc: 55.67\n",
      "ep:10100 loss: 0.6440 acc: 55.67\n",
      "ep:10200 loss: 0.6440 acc: 55.67\n",
      "ep:10300 loss: 0.6440 acc: 55.67\n",
      "ep:10400 loss: 0.6440 acc: 55.67\n",
      "ep:10500 loss: 0.6440 acc: 55.67\n",
      "ep:10600 loss: 0.6440 acc: 55.67\n",
      "ep:10700 loss: 0.6440 acc: 55.67\n",
      "ep:10800 loss: 0.6440 acc: 55.67\n",
      "ep:10900 loss: 0.6440 acc: 55.67\n",
      "ep:11000 loss: 0.6440 acc: 55.67\n",
      "ep:11100 loss: 0.6440 acc: 55.67\n",
      "ep:11200 loss: 0.6440 acc: 55.67\n",
      "ep:11300 loss: 0.6440 acc: 55.67\n",
      "ep:11400 loss: 0.6440 acc: 55.67\n",
      "ep:11500 loss: 0.6440 acc: 55.67\n",
      "ep:11600 loss: 0.6440 acc: 55.67\n",
      "ep:11700 loss: 0.6440 acc: 55.67\n",
      "ep:11800 loss: 0.6440 acc: 55.67\n",
      "ep:11900 loss: 0.6440 acc: 55.67\n",
      "ep:12000 loss: 0.6440 acc: 55.67\n",
      "ep:12100 loss: 0.6440 acc: 55.67\n",
      "ep:12200 loss: 0.6440 acc: 55.67\n",
      "ep:12300 loss: 0.6440 acc: 55.67\n",
      "ep:12400 loss: 0.6440 acc: 55.67\n",
      "ep:12500 loss: 0.6440 acc: 55.67\n",
      "ep:12600 loss: 0.6440 acc: 55.67\n",
      "ep:12700 loss: 0.6440 acc: 55.67\n",
      "ep:12800 loss: 0.6440 acc: 55.67\n",
      "ep:12900 loss: 0.6440 acc: 55.67\n",
      "ep:13000 loss: 0.6440 acc: 55.67\n",
      "ep:13100 loss: 0.6440 acc: 55.67\n",
      "ep:13200 loss: 0.6440 acc: 55.67\n",
      "ep:13300 loss: 0.6440 acc: 55.67\n",
      "ep:13400 loss: 0.6440 acc: 55.67\n",
      "ep:13500 loss: 0.6440 acc: 55.67\n",
      "ep:13600 loss: 0.6440 acc: 55.67\n",
      "ep:13700 loss: 0.6440 acc: 55.67\n",
      "ep:13800 loss: 0.6440 acc: 55.67\n",
      "ep:13900 loss: 0.6440 acc: 55.67\n",
      "ep:14000 loss: 0.6440 acc: 55.67\n",
      "ep:14100 loss: 0.6440 acc: 55.67\n",
      "ep:14200 loss: 0.6440 acc: 55.67\n",
      "ep:14300 loss: 0.6440 acc: 55.67\n",
      "ep:14400 loss: 0.6440 acc: 55.67\n",
      "ep:14500 loss: 0.6440 acc: 55.67\n",
      "ep:14600 loss: 0.6440 acc: 55.67\n",
      "ep:14700 loss: 0.6440 acc: 55.67\n",
      "ep:14800 loss: 0.6440 acc: 55.67\n",
      "ep:14900 loss: 0.6440 acc: 55.67\n",
      "ep:15000 loss: 0.6440 acc: 55.67\n",
      "ep:15100 loss: 0.6440 acc: 55.67\n",
      "ep:15200 loss: 0.6440 acc: 55.67\n",
      "ep:15300 loss: 0.6440 acc: 55.67\n",
      "ep:15400 loss: 0.6440 acc: 55.67\n",
      "ep:15500 loss: 0.6440 acc: 55.67\n",
      "ep:15600 loss: 0.6440 acc: 55.67\n",
      "ep:15700 loss: 0.6440 acc: 55.67\n",
      "ep:15800 loss: 0.6440 acc: 55.67\n",
      "ep:15900 loss: 0.6440 acc: 55.67\n",
      "ep:16000 loss: 0.6440 acc: 55.67\n",
      "ep:16100 loss: 0.6440 acc: 55.67\n",
      "ep:16200 loss: 0.6440 acc: 55.67\n",
      "ep:16300 loss: 0.6440 acc: 55.67\n",
      "ep:16400 loss: 0.6440 acc: 55.67\n",
      "ep:16500 loss: 0.6440 acc: 55.67\n",
      "ep:16600 loss: 0.6440 acc: 55.67\n",
      "ep:16700 loss: 0.6440 acc: 55.67\n",
      "ep:16800 loss: 0.6440 acc: 55.67\n",
      "ep:16900 loss: 0.6440 acc: 55.67\n",
      "ep:17000 loss: 0.6440 acc: 55.67\n",
      "ep:17100 loss: 0.6440 acc: 55.67\n",
      "ep:17200 loss: 0.6440 acc: 55.67\n",
      "ep:17300 loss: 0.6440 acc: 55.67\n",
      "ep:17400 loss: 0.6440 acc: 55.67\n",
      "ep:17500 loss: 0.6440 acc: 55.67\n",
      "ep:17600 loss: 0.6440 acc: 55.67\n",
      "ep:17700 loss: 0.6440 acc: 55.67\n",
      "ep:17800 loss: 0.6440 acc: 55.67\n",
      "ep:17900 loss: 0.6440 acc: 55.67\n",
      "ep:18000 loss: 0.6440 acc: 55.67\n",
      "ep:18100 loss: 0.6440 acc: 55.67\n",
      "ep:18200 loss: 0.6440 acc: 55.67\n",
      "ep:18300 loss: 0.6440 acc: 55.67\n",
      "ep:18400 loss: 0.6440 acc: 55.67\n",
      "ep:18500 loss: 0.6440 acc: 55.67\n",
      "ep:18600 loss: 0.6440 acc: 55.67\n",
      "ep:18700 loss: 0.6440 acc: 55.67\n",
      "ep:18800 loss: 0.6440 acc: 55.67\n",
      "ep:18900 loss: 0.6440 acc: 55.67\n",
      "ep:19000 loss: 0.6440 acc: 55.67\n",
      "ep:19100 loss: 0.6440 acc: 55.67\n",
      "ep:19200 loss: 0.6440 acc: 55.67\n",
      "ep:19300 loss: 0.6440 acc: 55.67\n",
      "ep:19400 loss: 0.6440 acc: 55.67\n",
      "ep:19500 loss: 0.6440 acc: 55.67\n",
      "ep:19600 loss: 0.6440 acc: 55.67\n",
      "ep:19700 loss: 0.6440 acc: 55.67\n",
      "ep:19800 loss: 0.6440 acc: 55.67\n",
      "ep:19900 loss: 0.6440 acc: 55.67\n",
      "hidden:2, acc:55.67010498046875 , epoch:19998\n",
      "ep:  100 loss: 0.6242 acc: 60.82\n",
      "ep:  200 loss: 0.5405 acc: 58.25\n",
      "ep:  300 loss: 0.4320 acc: 65.98\n",
      "ep:  400 loss: 0.3797 acc: 68.56\n",
      "ep:  500 loss: 0.3725 acc: 68.56\n",
      "ep:  600 loss: 0.3699 acc: 69.07\n",
      "ep:  700 loss: 0.3686 acc: 69.59\n",
      "ep:  800 loss: 0.3677 acc: 69.59\n",
      "ep:  900 loss: 0.3670 acc: 69.59\n",
      "ep: 1000 loss: 0.3664 acc: 69.59\n",
      "ep: 1100 loss: 0.3659 acc: 70.10\n",
      "ep: 1200 loss: 0.3654 acc: 69.59\n",
      "ep: 1300 loss: 0.3649 acc: 69.59\n",
      "ep: 1400 loss: 0.3645 acc: 69.59\n",
      "ep: 1500 loss: 0.3640 acc: 69.59\n",
      "ep: 1600 loss: 0.3636 acc: 69.59\n",
      "ep: 1700 loss: 0.3632 acc: 69.59\n",
      "ep: 1800 loss: 0.3628 acc: 69.59\n",
      "ep: 1900 loss: 0.3624 acc: 69.59\n",
      "ep: 2000 loss: 0.3621 acc: 69.59\n",
      "ep: 2100 loss: 0.3618 acc: 69.59\n",
      "ep: 2200 loss: 0.3615 acc: 69.59\n",
      "ep: 2300 loss: 0.3612 acc: 69.59\n",
      "ep: 2400 loss: 0.3609 acc: 69.07\n",
      "ep: 2500 loss: 0.3607 acc: 69.07\n",
      "ep: 2600 loss: 0.3604 acc: 69.59\n",
      "ep: 2700 loss: 0.3602 acc: 69.59\n",
      "ep: 2800 loss: 0.3600 acc: 69.59\n",
      "ep: 2900 loss: 0.3598 acc: 69.59\n",
      "ep: 3000 loss: 0.3597 acc: 68.56\n",
      "ep: 3100 loss: 0.3595 acc: 68.56\n",
      "ep: 3200 loss: 0.3594 acc: 68.56\n",
      "ep: 3300 loss: 0.3592 acc: 68.56\n",
      "ep: 3400 loss: 0.3591 acc: 69.07\n",
      "ep: 3500 loss: 0.3590 acc: 68.56\n",
      "ep: 3600 loss: 0.3589 acc: 68.56\n",
      "ep: 3700 loss: 0.3588 acc: 68.56\n",
      "ep: 3800 loss: 0.3588 acc: 68.56\n",
      "ep: 3900 loss: 0.3587 acc: 68.56\n",
      "ep: 4000 loss: 0.3587 acc: 69.07\n",
      "ep: 4100 loss: 0.3587 acc: 69.07\n",
      "ep: 4200 loss: 0.3586 acc: 69.07\n",
      "ep: 4300 loss: 0.3586 acc: 68.56\n",
      "ep: 4400 loss: 0.3586 acc: 68.56\n",
      "ep: 4500 loss: 0.3586 acc: 68.56\n",
      "ep: 4600 loss: 0.3586 acc: 69.07\n",
      "ep: 4700 loss: 0.3587 acc: 69.07\n",
      "ep: 4800 loss: 0.3587 acc: 69.07\n",
      "ep: 4900 loss: 0.3587 acc: 69.07\n",
      "ep: 5000 loss: 0.3587 acc: 69.07\n",
      "ep: 5100 loss: 0.3587 acc: 69.07\n",
      "ep: 5200 loss: 0.3587 acc: 69.07\n",
      "ep: 5300 loss: 0.3587 acc: 69.07\n",
      "ep: 5400 loss: 0.3588 acc: 69.07\n",
      "ep: 5500 loss: 0.3588 acc: 69.07\n",
      "ep: 5600 loss: 0.3588 acc: 69.07\n",
      "ep: 5700 loss: 0.3588 acc: 69.07\n",
      "ep: 5800 loss: 0.3588 acc: 69.07\n",
      "ep: 5900 loss: 0.3588 acc: 69.07\n",
      "ep: 6000 loss: 0.3588 acc: 69.07\n",
      "ep: 6100 loss: 0.3588 acc: 69.07\n",
      "ep: 6200 loss: 0.3588 acc: 69.07\n",
      "ep: 6300 loss: 0.3588 acc: 69.07\n",
      "ep: 6400 loss: 0.3587 acc: 69.07\n",
      "ep: 6500 loss: 0.3587 acc: 69.07\n",
      "ep: 6600 loss: 0.3587 acc: 69.07\n",
      "ep: 6700 loss: 0.3587 acc: 69.07\n",
      "ep: 6800 loss: 0.3587 acc: 69.07\n",
      "ep: 6900 loss: 0.3587 acc: 69.07\n",
      "ep: 7000 loss: 0.3587 acc: 69.07\n",
      "ep: 7100 loss: 0.3587 acc: 69.07\n",
      "ep: 7200 loss: 0.3587 acc: 69.07\n",
      "ep: 7300 loss: 0.3587 acc: 69.07\n",
      "ep: 7400 loss: 0.3587 acc: 69.07\n",
      "ep: 7500 loss: 0.3587 acc: 69.07\n",
      "ep: 7600 loss: 0.3587 acc: 69.07\n",
      "ep: 7700 loss: 0.3587 acc: 69.07\n",
      "ep: 7800 loss: 0.3587 acc: 69.07\n",
      "ep: 7900 loss: 0.3587 acc: 69.07\n",
      "ep: 8000 loss: 0.3587 acc: 69.07\n",
      "ep: 8100 loss: 0.3587 acc: 69.07\n",
      "ep: 8200 loss: 0.3587 acc: 69.07\n",
      "ep: 8300 loss: 0.3587 acc: 69.07\n",
      "ep: 8400 loss: 0.3587 acc: 69.07\n",
      "ep: 8500 loss: 0.3587 acc: 69.07\n",
      "ep: 8600 loss: 0.3587 acc: 69.07\n",
      "ep: 8700 loss: 0.3587 acc: 69.59\n",
      "ep: 8800 loss: 0.3587 acc: 69.07\n",
      "ep: 8900 loss: 0.3587 acc: 69.07\n",
      "ep: 9000 loss: 0.3587 acc: 69.07\n",
      "ep: 9100 loss: 0.3587 acc: 69.07\n",
      "ep: 9200 loss: 0.3587 acc: 69.07\n",
      "ep: 9300 loss: 0.3587 acc: 69.07\n",
      "ep: 9400 loss: 0.3587 acc: 69.07\n",
      "ep: 9500 loss: 0.3587 acc: 69.07\n",
      "ep: 9600 loss: 0.3587 acc: 69.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 9700 loss: 0.3587 acc: 69.07\n",
      "ep: 9800 loss: 0.3587 acc: 69.07\n",
      "ep: 9900 loss: 0.3587 acc: 69.07\n",
      "ep:10000 loss: 0.3587 acc: 69.07\n",
      "ep:10100 loss: 0.3587 acc: 69.07\n",
      "ep:10200 loss: 0.3587 acc: 69.07\n",
      "ep:10300 loss: 0.3587 acc: 69.07\n",
      "ep:10400 loss: 0.3587 acc: 69.07\n",
      "ep:10500 loss: 0.3587 acc: 69.07\n",
      "ep:10600 loss: 0.3587 acc: 69.07\n",
      "ep:10700 loss: 0.3587 acc: 69.07\n",
      "ep:10800 loss: 0.3587 acc: 69.07\n",
      "ep:10900 loss: 0.3587 acc: 69.07\n",
      "ep:11000 loss: 0.3587 acc: 69.07\n",
      "ep:11100 loss: 0.3587 acc: 69.07\n",
      "ep:11200 loss: 0.3587 acc: 69.07\n",
      "ep:11300 loss: 0.3587 acc: 69.59\n",
      "ep:11400 loss: 0.3587 acc: 69.07\n",
      "ep:11500 loss: 0.3587 acc: 69.07\n",
      "ep:11600 loss: 0.3587 acc: 69.07\n",
      "ep:11700 loss: 0.3587 acc: 69.07\n",
      "ep:11800 loss: 0.3587 acc: 69.07\n",
      "ep:11900 loss: 0.3587 acc: 69.07\n",
      "ep:12000 loss: 0.3587 acc: 69.07\n",
      "ep:12100 loss: 0.3587 acc: 69.07\n",
      "ep:12200 loss: 0.3587 acc: 69.07\n",
      "ep:12300 loss: 0.3587 acc: 69.07\n",
      "ep:12400 loss: 0.3587 acc: 69.07\n",
      "ep:12500 loss: 0.3587 acc: 69.07\n",
      "ep:12600 loss: 0.3587 acc: 69.07\n",
      "ep:12700 loss: 0.3587 acc: 69.07\n",
      "ep:12800 loss: 0.3587 acc: 69.07\n",
      "ep:12900 loss: 0.3587 acc: 69.07\n",
      "ep:13000 loss: 0.3587 acc: 69.07\n",
      "ep:13100 loss: 0.3587 acc: 69.59\n",
      "ep:13200 loss: 0.3587 acc: 69.07\n",
      "ep:13300 loss: 0.3587 acc: 69.07\n",
      "ep:13400 loss: 0.3587 acc: 69.07\n",
      "ep:13500 loss: 0.3587 acc: 69.07\n",
      "ep:13600 loss: 0.3587 acc: 69.07\n",
      "ep:13700 loss: 0.3587 acc: 69.07\n",
      "ep:13800 loss: 0.3587 acc: 69.07\n",
      "ep:13900 loss: 0.3587 acc: 69.07\n",
      "ep:14000 loss: 0.3587 acc: 69.07\n",
      "ep:14100 loss: 0.3587 acc: 69.07\n",
      "ep:14200 loss: 0.3587 acc: 69.07\n",
      "ep:14300 loss: 0.3587 acc: 69.07\n",
      "ep:14400 loss: 0.3587 acc: 69.07\n",
      "ep:14500 loss: 0.3587 acc: 69.07\n",
      "ep:14600 loss: 0.3587 acc: 69.59\n",
      "ep:14700 loss: 0.3587 acc: 69.07\n",
      "ep:14800 loss: 0.3587 acc: 69.07\n",
      "ep:14900 loss: 0.3587 acc: 69.07\n",
      "ep:15000 loss: 0.3587 acc: 69.07\n",
      "ep:15100 loss: 0.3587 acc: 69.07\n",
      "ep:15200 loss: 0.3587 acc: 69.07\n",
      "ep:15300 loss: 0.3587 acc: 69.07\n",
      "ep:15400 loss: 0.3587 acc: 69.07\n",
      "ep:15500 loss: 0.3587 acc: 69.07\n",
      "ep:15600 loss: 0.3587 acc: 69.07\n",
      "ep:15700 loss: 0.3587 acc: 69.07\n",
      "ep:15800 loss: 0.3587 acc: 69.07\n",
      "ep:15900 loss: 0.3587 acc: 69.07\n",
      "ep:16000 loss: 0.3587 acc: 69.07\n",
      "ep:16100 loss: 0.3587 acc: 69.07\n",
      "ep:16200 loss: 0.3587 acc: 69.07\n",
      "ep:16300 loss: 0.3587 acc: 69.07\n",
      "ep:16400 loss: 0.3587 acc: 69.07\n",
      "ep:16500 loss: 0.3587 acc: 69.07\n",
      "ep:16600 loss: 0.3587 acc: 69.59\n",
      "ep:16700 loss: 0.3587 acc: 69.07\n",
      "ep:16800 loss: 0.3587 acc: 69.07\n",
      "ep:16900 loss: 0.3587 acc: 69.07\n",
      "ep:17000 loss: 0.3587 acc: 69.07\n",
      "ep:17100 loss: 0.3587 acc: 69.07\n",
      "ep:17200 loss: 0.3587 acc: 69.07\n",
      "ep:17300 loss: 0.3587 acc: 69.59\n",
      "ep:17400 loss: 0.3587 acc: 69.07\n",
      "ep:17500 loss: 0.3587 acc: 69.07\n",
      "ep:17600 loss: 0.3587 acc: 69.07\n",
      "ep:17700 loss: 0.3587 acc: 69.07\n",
      "ep:17800 loss: 0.3587 acc: 69.07\n",
      "ep:17900 loss: 0.3587 acc: 69.07\n",
      "ep:18000 loss: 0.3587 acc: 69.59\n",
      "ep:18100 loss: 0.3587 acc: 69.07\n",
      "ep:18200 loss: 0.3587 acc: 69.07\n",
      "ep:18300 loss: 0.3587 acc: 69.07\n",
      "ep:18400 loss: 0.3587 acc: 69.07\n",
      "ep:18500 loss: 0.3587 acc: 69.07\n",
      "ep:18600 loss: 0.3587 acc: 69.59\n",
      "ep:18700 loss: 0.3587 acc: 69.07\n",
      "ep:18800 loss: 0.3587 acc: 69.07\n",
      "ep:18900 loss: 0.3587 acc: 69.07\n",
      "ep:19000 loss: 0.3587 acc: 69.07\n",
      "ep:19100 loss: 0.3587 acc: 69.07\n",
      "ep:19200 loss: 0.3587 acc: 69.07\n",
      "ep:19300 loss: 0.3587 acc: 69.07\n",
      "ep:19400 loss: 0.3587 acc: 69.07\n",
      "ep:19500 loss: 0.3587 acc: 69.07\n",
      "ep:19600 loss: 0.3587 acc: 69.07\n",
      "ep:19700 loss: 0.3587 acc: 69.07\n",
      "ep:19800 loss: 0.3587 acc: 69.07\n",
      "ep:19900 loss: 0.3587 acc: 69.07\n",
      "hidden:3, acc:69.0721664428711 , epoch:19997\n",
      "ep:  100 loss: 0.6716 acc: 55.67\n",
      "ep:  200 loss: 0.5928 acc: 59.28\n",
      "ep:  300 loss: 0.5115 acc: 56.19\n",
      "ep:  400 loss: 0.3952 acc: 69.07\n",
      "ep:  500 loss: 0.3788 acc: 69.07\n",
      "ep:  600 loss: 0.3730 acc: 69.59\n",
      "ep:  700 loss: 0.3699 acc: 69.59\n",
      "ep:  800 loss: 0.3677 acc: 70.10\n",
      "ep:  900 loss: 0.3639 acc: 69.59\n",
      "ep: 1000 loss: 0.3333 acc: 67.01\n",
      "ep: 1100 loss: 0.3124 acc: 67.01\n",
      "ep: 1200 loss: 0.3020 acc: 67.01\n",
      "ep: 1300 loss: 0.2978 acc: 67.01\n",
      "ep: 1400 loss: 0.2958 acc: 66.49\n",
      "ep: 1500 loss: 0.2946 acc: 66.49\n",
      "ep: 1600 loss: 0.2937 acc: 66.49\n",
      "ep: 1700 loss: 0.2931 acc: 66.49\n",
      "ep: 1800 loss: 0.2924 acc: 66.49\n",
      "ep: 1900 loss: 0.2918 acc: 65.98\n",
      "ep: 2000 loss: 0.2911 acc: 65.98\n",
      "ep: 2100 loss: 0.2905 acc: 65.98\n",
      "ep: 2200 loss: 0.2897 acc: 66.49\n",
      "ep: 2300 loss: 0.2890 acc: 65.98\n",
      "ep: 2400 loss: 0.2882 acc: 65.98\n",
      "ep: 2500 loss: 0.2873 acc: 65.98\n",
      "ep: 2600 loss: 0.2863 acc: 65.46\n",
      "ep: 2700 loss: 0.2852 acc: 64.95\n",
      "ep: 2800 loss: 0.2838 acc: 65.46\n",
      "ep: 2900 loss: 0.2816 acc: 65.46\n",
      "ep: 3000 loss: 0.2766 acc: 69.59\n",
      "ep: 3100 loss: 0.2487 acc: 71.65\n",
      "ep: 3200 loss: 0.2197 acc: 73.20\n",
      "ep: 3300 loss: 0.1932 acc: 74.74\n",
      "ep: 3400 loss: 0.1708 acc: 75.77\n",
      "ep: 3500 loss: 0.1529 acc: 74.74\n",
      "ep: 3600 loss: 0.1389 acc: 72.68\n",
      "ep: 3700 loss: 0.1281 acc: 71.13\n",
      "ep: 3800 loss: 0.1196 acc: 68.56\n",
      "ep: 3900 loss: 0.1128 acc: 66.49\n",
      "ep: 4000 loss: 0.1072 acc: 66.49\n",
      "ep: 4100 loss: 0.1022 acc: 66.49\n",
      "ep: 4200 loss: 0.0977 acc: 66.49\n",
      "ep: 4300 loss: 0.0937 acc: 66.49\n",
      "ep: 4400 loss: 0.0902 acc: 66.49\n",
      "ep: 4500 loss: 0.0872 acc: 66.49\n",
      "ep: 4600 loss: 0.0845 acc: 66.49\n",
      "ep: 4700 loss: 0.0822 acc: 66.49\n",
      "ep: 4800 loss: 0.0801 acc: 66.49\n",
      "ep: 4900 loss: 0.0783 acc: 66.49\n",
      "ep: 5000 loss: 0.0765 acc: 66.49\n",
      "ep: 5100 loss: 0.0748 acc: 66.49\n",
      "ep: 5200 loss: 0.0731 acc: 66.49\n",
      "ep: 5300 loss: 0.0714 acc: 66.49\n",
      "ep: 5400 loss: 0.0697 acc: 66.49\n",
      "ep: 5500 loss: 0.0679 acc: 73.20\n",
      "ep: 5600 loss: 0.0661 acc: 82.99\n",
      "ep: 5700 loss: 0.0642 acc: 82.99\n",
      "ep: 5800 loss: 0.0624 acc: 82.99\n",
      "ep: 5900 loss: 0.0606 acc: 82.99\n",
      "ep: 6000 loss: 0.0589 acc: 82.99\n",
      "ep: 6100 loss: 0.0574 acc: 82.99\n",
      "ep: 6200 loss: 0.0559 acc: 82.99\n",
      "ep: 6300 loss: 0.0547 acc: 82.99\n",
      "ep: 6400 loss: 0.0536 acc: 82.99\n",
      "ep: 6500 loss: 0.0526 acc: 82.99\n",
      "ep: 6600 loss: 0.0517 acc: 82.99\n",
      "ep: 6700 loss: 0.0510 acc: 82.99\n",
      "ep: 6800 loss: 0.0503 acc: 82.99\n",
      "ep: 6900 loss: 0.0498 acc: 82.99\n",
      "ep: 7000 loss: 0.0493 acc: 82.99\n",
      "ep: 7100 loss: 0.0488 acc: 82.99\n",
      "ep: 7200 loss: 0.0484 acc: 82.99\n",
      "ep: 7300 loss: 0.0481 acc: 82.99\n",
      "ep: 7400 loss: 0.0478 acc: 82.99\n",
      "ep: 7500 loss: 0.0475 acc: 82.99\n",
      "ep: 7600 loss: 0.0473 acc: 82.99\n",
      "ep: 7700 loss: 0.0471 acc: 82.99\n",
      "ep: 7800 loss: 0.0469 acc: 82.99\n",
      "ep: 7900 loss: 0.0468 acc: 82.99\n",
      "ep: 8000 loss: 0.0467 acc: 82.99\n",
      "ep: 8100 loss: 0.0466 acc: 82.99\n",
      "ep: 8200 loss: 0.0465 acc: 82.99\n",
      "ep: 8300 loss: 0.0464 acc: 82.99\n",
      "ep: 8400 loss: 0.0464 acc: 82.99\n",
      "ep: 8500 loss: 0.0463 acc: 82.99\n",
      "ep: 8600 loss: 0.0463 acc: 82.99\n",
      "ep: 8700 loss: 0.0463 acc: 82.99\n",
      "ep: 8800 loss: 0.0463 acc: 82.99\n",
      "ep: 8900 loss: 0.0463 acc: 82.99\n",
      "ep: 9000 loss: 0.0463 acc: 82.99\n",
      "ep: 9100 loss: 0.0463 acc: 82.99\n",
      "ep: 9200 loss: 0.0463 acc: 82.99\n",
      "ep: 9300 loss: 0.0465 acc: 82.99\n",
      "ep: 9400 loss: 0.0488 acc: 82.99\n",
      "ep: 9500 loss: 0.0675 acc: 91.24\n",
      "ep: 9600 loss: 0.0574 acc: 91.24\n",
      "ep: 9700 loss: 0.0541 acc: 91.24\n",
      "ep: 9800 loss: 0.0526 acc: 91.24\n",
      "ep: 9900 loss: 0.0517 acc: 91.24\n",
      "ep:10000 loss: 0.0512 acc: 91.24\n",
      "ep:10100 loss: 0.0509 acc: 91.24\n",
      "ep:10200 loss: 0.0507 acc: 91.24\n",
      "ep:10300 loss: 0.0505 acc: 91.24\n",
      "ep:10400 loss: 0.0504 acc: 91.24\n",
      "ep:10500 loss: 0.0504 acc: 91.24\n",
      "ep:10600 loss: 0.0503 acc: 91.24\n",
      "ep:10700 loss: 0.0503 acc: 91.24\n",
      "ep:10800 loss: 0.0502 acc: 91.24\n",
      "ep:10900 loss: 0.0502 acc: 91.24\n",
      "ep:11000 loss: 0.0502 acc: 91.24\n",
      "ep:11100 loss: 0.0502 acc: 91.24\n",
      "ep:11200 loss: 0.0502 acc: 91.24\n",
      "ep:11300 loss: 0.0502 acc: 91.24\n",
      "ep:11400 loss: 0.0502 acc: 91.24\n",
      "ep:11500 loss: 0.0502 acc: 91.24\n",
      "ep:11600 loss: 0.0502 acc: 91.24\n",
      "ep:11700 loss: 0.0502 acc: 91.24\n",
      "ep:11800 loss: 0.0502 acc: 91.24\n",
      "ep:11900 loss: 0.0502 acc: 91.24\n",
      "ep:12000 loss: 0.0502 acc: 91.24\n",
      "ep:12100 loss: 0.0502 acc: 91.24\n",
      "ep:12200 loss: 0.0502 acc: 91.24\n",
      "ep:12300 loss: 0.0502 acc: 91.24\n",
      "ep:12400 loss: 0.0502 acc: 91.24\n",
      "ep:12500 loss: 0.0502 acc: 91.24\n",
      "ep:12600 loss: 0.0502 acc: 91.24\n",
      "ep:12700 loss: 0.0502 acc: 91.24\n",
      "ep:12800 loss: 0.0502 acc: 91.24\n",
      "ep:12900 loss: 0.0502 acc: 91.24\n",
      "ep:13000 loss: 0.0502 acc: 91.24\n",
      "ep:13100 loss: 0.0502 acc: 91.24\n",
      "ep:13200 loss: 0.0502 acc: 91.24\n",
      "ep:13300 loss: 0.0502 acc: 91.24\n",
      "ep:13400 loss: 0.0502 acc: 91.24\n",
      "ep:13500 loss: 0.0502 acc: 91.24\n",
      "ep:13600 loss: 0.0502 acc: 91.24\n",
      "ep:13700 loss: 0.0502 acc: 91.24\n",
      "ep:13800 loss: 0.0502 acc: 91.24\n",
      "ep:13900 loss: 0.0502 acc: 91.24\n",
      "ep:14000 loss: 0.0502 acc: 91.24\n",
      "ep:14100 loss: 0.0502 acc: 91.24\n",
      "ep:14200 loss: 0.0502 acc: 91.24\n",
      "ep:14300 loss: 0.0502 acc: 91.24\n",
      "ep:14400 loss: 0.0502 acc: 91.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:14500 loss: 0.0502 acc: 91.24\n",
      "ep:14600 loss: 0.0502 acc: 91.24\n",
      "ep:14700 loss: 0.0502 acc: 91.24\n",
      "ep:14800 loss: 0.0502 acc: 91.24\n",
      "ep:14900 loss: 0.0502 acc: 91.24\n",
      "ep:15000 loss: 0.0502 acc: 91.24\n",
      "ep:15100 loss: 0.0502 acc: 91.24\n",
      "ep:15200 loss: 0.0502 acc: 91.24\n",
      "ep:15300 loss: 0.0502 acc: 91.24\n",
      "ep:15400 loss: 0.0502 acc: 91.24\n",
      "ep:15500 loss: 0.0502 acc: 91.24\n",
      "ep:15600 loss: 0.0502 acc: 91.24\n",
      "ep:15700 loss: 0.0502 acc: 91.24\n",
      "ep:15800 loss: 0.0502 acc: 91.24\n",
      "ep:15900 loss: 0.0502 acc: 91.24\n",
      "ep:16000 loss: 0.0502 acc: 91.24\n",
      "ep:16100 loss: 0.0502 acc: 91.24\n",
      "ep:16200 loss: 0.0502 acc: 91.24\n",
      "ep:16300 loss: 0.0502 acc: 91.24\n",
      "ep:16400 loss: 0.0502 acc: 91.24\n",
      "ep:16500 loss: 0.0502 acc: 91.24\n",
      "ep:16600 loss: 0.0502 acc: 91.24\n",
      "ep:16700 loss: 0.0502 acc: 91.24\n",
      "ep:16800 loss: 0.0502 acc: 91.24\n",
      "ep:16900 loss: 0.0502 acc: 91.24\n",
      "ep:17000 loss: 0.0502 acc: 91.24\n",
      "ep:17100 loss: 0.0502 acc: 91.24\n",
      "ep:17200 loss: 0.0502 acc: 91.24\n",
      "ep:17300 loss: 0.0502 acc: 91.24\n",
      "ep:17400 loss: 0.0502 acc: 91.24\n",
      "ep:17500 loss: 0.0502 acc: 91.24\n",
      "ep:17600 loss: 0.0502 acc: 91.24\n",
      "ep:17700 loss: 0.0502 acc: 91.24\n",
      "ep:17800 loss: 0.0502 acc: 91.24\n",
      "ep:17900 loss: 0.0502 acc: 91.24\n",
      "ep:18000 loss: 0.0502 acc: 91.24\n",
      "ep:18100 loss: 0.0502 acc: 91.24\n",
      "ep:18200 loss: 0.0502 acc: 91.24\n",
      "ep:18300 loss: 0.0502 acc: 91.24\n",
      "ep:18400 loss: 0.0502 acc: 91.24\n",
      "ep:18500 loss: 0.0502 acc: 91.24\n",
      "ep:18600 loss: 0.0502 acc: 91.24\n",
      "ep:18700 loss: 0.0502 acc: 91.24\n",
      "ep:18800 loss: 0.0502 acc: 91.24\n",
      "ep:18900 loss: 0.0502 acc: 91.24\n",
      "ep:19000 loss: 0.0502 acc: 91.24\n",
      "ep:19100 loss: 0.0502 acc: 91.24\n",
      "ep:19200 loss: 0.0502 acc: 91.24\n",
      "ep:19300 loss: 0.0502 acc: 91.24\n",
      "ep:19400 loss: 0.0502 acc: 91.24\n",
      "ep:19500 loss: 0.0502 acc: 91.24\n",
      "ep:19600 loss: 0.0502 acc: 91.24\n",
      "ep:19700 loss: 0.0502 acc: 91.24\n",
      "ep:19800 loss: 0.0502 acc: 91.24\n",
      "ep:19900 loss: 0.0502 acc: 91.24\n",
      "hidden:4, acc:91.23711395263672 , epoch:19996\n",
      "ep:  100 loss: 0.6100 acc: 58.76\n",
      "ep:  200 loss: 0.4521 acc: 64.43\n",
      "ep:  300 loss: 0.3757 acc: 68.56\n",
      "ep:  400 loss: 0.3229 acc: 67.53\n",
      "ep:  500 loss: 0.2423 acc: 58.25\n",
      "ep:  600 loss: 0.1969 acc: 58.76\n",
      "ep:  700 loss: 0.1710 acc: 59.28\n",
      "ep:  800 loss: 0.1551 acc: 75.26\n",
      "ep:  900 loss: 0.1444 acc: 75.26\n",
      "ep: 1000 loss: 0.1369 acc: 75.26\n",
      "ep: 1100 loss: 0.1313 acc: 75.26\n",
      "ep: 1200 loss: 0.1270 acc: 75.26\n",
      "ep: 1300 loss: 0.1236 acc: 75.26\n",
      "ep: 1400 loss: 0.1208 acc: 75.26\n",
      "ep: 1500 loss: 0.1185 acc: 75.26\n",
      "ep: 1600 loss: 0.1166 acc: 75.26\n",
      "ep: 1700 loss: 0.1150 acc: 75.26\n",
      "ep: 1800 loss: 0.1136 acc: 75.26\n",
      "ep: 1900 loss: 0.1124 acc: 75.26\n",
      "ep: 2000 loss: 0.1114 acc: 75.26\n",
      "ep: 2100 loss: 0.1104 acc: 75.26\n",
      "ep: 2200 loss: 0.1096 acc: 75.26\n",
      "ep: 2300 loss: 0.1089 acc: 75.26\n",
      "ep: 2400 loss: 0.1082 acc: 75.26\n",
      "ep: 2500 loss: 0.1077 acc: 75.26\n",
      "ep: 2600 loss: 0.1071 acc: 75.26\n",
      "ep: 2700 loss: 0.1067 acc: 75.26\n",
      "ep: 2800 loss: 0.1062 acc: 75.26\n",
      "ep: 2900 loss: 0.1058 acc: 75.26\n",
      "ep: 3000 loss: 0.1055 acc: 75.26\n",
      "ep: 3100 loss: 0.1052 acc: 75.26\n",
      "ep: 3200 loss: 0.1049 acc: 75.26\n",
      "ep: 3300 loss: 0.1046 acc: 75.26\n",
      "ep: 3400 loss: 0.1044 acc: 75.26\n",
      "ep: 3500 loss: 0.1042 acc: 75.26\n",
      "ep: 3600 loss: 0.1040 acc: 75.26\n",
      "ep: 3700 loss: 0.1038 acc: 75.26\n",
      "ep: 3800 loss: 0.1036 acc: 75.26\n",
      "ep: 3900 loss: 0.1035 acc: 75.26\n",
      "ep: 4000 loss: 0.1033 acc: 75.26\n",
      "ep: 4100 loss: 0.1032 acc: 75.26\n",
      "ep: 4200 loss: 0.1031 acc: 75.26\n",
      "ep: 4300 loss: 0.1030 acc: 75.26\n",
      "ep: 4400 loss: 0.1029 acc: 75.26\n",
      "ep: 4500 loss: 0.1028 acc: 75.26\n",
      "ep: 4600 loss: 0.1027 acc: 75.26\n",
      "ep: 4700 loss: 0.1026 acc: 75.26\n",
      "ep: 4800 loss: 0.1026 acc: 75.26\n",
      "ep: 4900 loss: 0.1025 acc: 75.26\n",
      "ep: 5000 loss: 0.1024 acc: 75.26\n",
      "ep: 5100 loss: 0.1024 acc: 75.26\n",
      "ep: 5200 loss: 0.1023 acc: 75.26\n",
      "ep: 5300 loss: 0.1023 acc: 75.26\n",
      "ep: 5400 loss: 0.1022 acc: 75.26\n",
      "ep: 5500 loss: 0.1022 acc: 75.26\n",
      "ep: 5600 loss: 0.1021 acc: 75.26\n",
      "ep: 5700 loss: 0.1021 acc: 75.26\n",
      "ep: 5800 loss: 0.1020 acc: 75.26\n",
      "ep: 5900 loss: 0.1020 acc: 75.26\n",
      "ep: 6000 loss: 0.1020 acc: 75.26\n",
      "ep: 6100 loss: 0.1019 acc: 75.26\n",
      "ep: 6200 loss: 0.1019 acc: 75.26\n",
      "ep: 6300 loss: 0.1019 acc: 75.26\n",
      "ep: 6400 loss: 0.1019 acc: 75.26\n",
      "ep: 6500 loss: 0.1019 acc: 75.26\n",
      "ep: 6600 loss: 0.1019 acc: 75.26\n",
      "ep: 6700 loss: 0.1020 acc: 75.26\n",
      "ep: 6800 loss: 0.1022 acc: 75.26\n",
      "ep: 6900 loss: 0.1029 acc: 75.26\n",
      "ep: 7000 loss: 0.1064 acc: 75.26\n",
      "ep: 7100 loss: 0.0976 acc: 67.01\n",
      "ep: 7200 loss: 0.0897 acc: 67.01\n",
      "ep: 7300 loss: 0.0833 acc: 67.01\n",
      "ep: 7400 loss: 0.0778 acc: 67.01\n",
      "ep: 7500 loss: 0.0729 acc: 67.01\n",
      "ep: 7600 loss: 0.0685 acc: 67.01\n",
      "ep: 7700 loss: 0.0645 acc: 67.01\n",
      "ep: 7800 loss: 0.0609 acc: 83.51\n",
      "ep: 7900 loss: 0.0577 acc: 83.51\n",
      "ep: 8000 loss: 0.0548 acc: 83.51\n",
      "ep: 8100 loss: 0.0522 acc: 83.51\n",
      "ep: 8200 loss: 0.0498 acc: 83.51\n",
      "ep: 8300 loss: 0.0476 acc: 83.51\n",
      "ep: 8400 loss: 0.0457 acc: 83.51\n",
      "ep: 8500 loss: 0.0439 acc: 83.51\n",
      "ep: 8600 loss: 0.0423 acc: 83.51\n",
      "ep: 8700 loss: 0.0409 acc: 83.51\n",
      "ep: 8800 loss: 0.0395 acc: 83.51\n",
      "ep: 8900 loss: 0.0383 acc: 83.51\n",
      "ep: 9000 loss: 0.0372 acc: 83.51\n",
      "ep: 9100 loss: 0.0362 acc: 83.51\n",
      "ep: 9200 loss: 0.0352 acc: 83.51\n",
      "ep: 9300 loss: 0.0344 acc: 83.51\n",
      "ep: 9400 loss: 0.0336 acc: 83.51\n",
      "ep: 9500 loss: 0.0328 acc: 83.51\n",
      "ep: 9600 loss: 0.0321 acc: 83.51\n",
      "ep: 9700 loss: 0.0315 acc: 83.51\n",
      "ep: 9800 loss: 0.0309 acc: 83.51\n",
      "ep: 9900 loss: 0.0303 acc: 83.51\n",
      "ep:10000 loss: 0.0298 acc: 83.51\n",
      "ep:10100 loss: 0.0294 acc: 83.51\n",
      "ep:10200 loss: 0.0289 acc: 83.51\n",
      "ep:10300 loss: 0.0285 acc: 83.51\n",
      "ep:10400 loss: 0.0281 acc: 83.51\n",
      "ep:10500 loss: 0.0278 acc: 83.51\n",
      "ep:10600 loss: 0.0275 acc: 83.51\n",
      "ep:10700 loss: 0.0272 acc: 83.51\n",
      "ep:10800 loss: 0.0269 acc: 83.51\n",
      "ep:10900 loss: 0.0266 acc: 83.51\n",
      "ep:11000 loss: 0.0264 acc: 83.51\n",
      "ep:11100 loss: 0.0261 acc: 83.51\n",
      "ep:11200 loss: 0.0259 acc: 83.51\n",
      "ep:11300 loss: 0.0257 acc: 83.51\n",
      "ep:11400 loss: 0.0255 acc: 83.51\n",
      "ep:11500 loss: 0.0254 acc: 83.51\n",
      "ep:11600 loss: 0.0252 acc: 83.51\n",
      "ep:11700 loss: 0.0250 acc: 83.51\n",
      "ep:11800 loss: 0.0249 acc: 83.51\n",
      "ep:11900 loss: 0.0248 acc: 83.51\n",
      "ep:12000 loss: 0.0247 acc: 83.51\n",
      "ep:12100 loss: 0.0245 acc: 83.51\n",
      "ep:12200 loss: 0.0244 acc: 83.51\n",
      "ep:12300 loss: 0.0243 acc: 83.51\n",
      "ep:12400 loss: 0.0242 acc: 83.51\n",
      "ep:12500 loss: 0.0241 acc: 83.51\n",
      "ep:12600 loss: 0.0241 acc: 83.51\n",
      "ep:12700 loss: 0.0240 acc: 83.51\n",
      "ep:12800 loss: 0.0239 acc: 83.51\n",
      "ep:12900 loss: 0.0238 acc: 83.51\n",
      "ep:13000 loss: 0.0238 acc: 83.51\n",
      "ep:13100 loss: 0.0237 acc: 83.51\n",
      "ep:13200 loss: 0.0237 acc: 83.51\n",
      "ep:13300 loss: 0.0236 acc: 83.51\n",
      "ep:13400 loss: 0.0236 acc: 83.51\n",
      "ep:13500 loss: 0.0235 acc: 83.51\n",
      "ep:13600 loss: 0.0235 acc: 83.51\n",
      "ep:13700 loss: 0.0234 acc: 83.51\n",
      "ep:13800 loss: 0.0234 acc: 83.51\n",
      "ep:13900 loss: 0.0234 acc: 83.51\n",
      "ep:14000 loss: 0.0233 acc: 83.51\n",
      "ep:14100 loss: 0.0233 acc: 83.51\n",
      "ep:14200 loss: 0.0233 acc: 83.51\n",
      "ep:14300 loss: 0.0233 acc: 83.51\n",
      "ep:14400 loss: 0.0232 acc: 83.51\n",
      "ep:14500 loss: 0.0232 acc: 83.51\n",
      "ep:14600 loss: 0.0232 acc: 83.51\n",
      "ep:14700 loss: 0.0232 acc: 83.51\n",
      "ep:14800 loss: 0.0232 acc: 83.51\n",
      "ep:14900 loss: 0.0231 acc: 83.51\n",
      "ep:15000 loss: 0.0231 acc: 83.51\n",
      "ep:15100 loss: 0.0231 acc: 83.51\n",
      "ep:15200 loss: 0.0231 acc: 83.51\n",
      "ep:15300 loss: 0.0231 acc: 83.51\n",
      "ep:15400 loss: 0.0231 acc: 83.51\n",
      "ep:15500 loss: 0.0231 acc: 83.51\n",
      "ep:15600 loss: 0.0230 acc: 83.51\n",
      "ep:15700 loss: 0.0230 acc: 83.51\n",
      "ep:15800 loss: 0.0230 acc: 83.51\n",
      "ep:15900 loss: 0.0230 acc: 83.51\n",
      "ep:16000 loss: 0.0230 acc: 83.51\n",
      "ep:16100 loss: 0.0230 acc: 83.51\n",
      "ep:16200 loss: 0.0230 acc: 83.51\n",
      "ep:16300 loss: 0.0230 acc: 83.51\n",
      "ep:16400 loss: 0.0230 acc: 83.51\n",
      "ep:16500 loss: 0.0230 acc: 83.51\n",
      "ep:16600 loss: 0.0230 acc: 83.51\n",
      "ep:16700 loss: 0.0230 acc: 83.51\n",
      "ep:16800 loss: 0.0229 acc: 83.51\n",
      "ep:16900 loss: 0.0229 acc: 83.51\n",
      "ep:17000 loss: 0.0229 acc: 83.51\n",
      "ep:17100 loss: 0.0229 acc: 83.51\n",
      "ep:17200 loss: 0.0229 acc: 83.51\n",
      "ep:17300 loss: 0.0229 acc: 83.51\n",
      "ep:17400 loss: 0.0229 acc: 83.51\n",
      "ep:17500 loss: 0.0229 acc: 83.51\n",
      "ep:17600 loss: 0.0229 acc: 83.51\n",
      "ep:17700 loss: 0.0229 acc: 83.51\n",
      "ep:17800 loss: 0.0229 acc: 83.51\n",
      "ep:17900 loss: 0.0229 acc: 83.51\n",
      "ep:18000 loss: 0.0229 acc: 83.51\n",
      "ep:18100 loss: 0.0229 acc: 83.51\n",
      "ep:18200 loss: 0.0229 acc: 83.51\n",
      "ep:18300 loss: 0.0229 acc: 83.51\n",
      "ep:18400 loss: 0.0229 acc: 83.51\n",
      "ep:18500 loss: 0.0229 acc: 83.51\n",
      "ep:18600 loss: 0.0229 acc: 83.51\n",
      "ep:18700 loss: 0.0229 acc: 83.51\n",
      "ep:18800 loss: 0.0229 acc: 83.51\n",
      "ep:18900 loss: 0.0229 acc: 83.51\n",
      "ep:19000 loss: 0.0229 acc: 83.51\n",
      "ep:19100 loss: 0.0229 acc: 83.51\n",
      "ep:19200 loss: 0.0229 acc: 83.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:19300 loss: 0.0229 acc: 83.51\n",
      "ep:19400 loss: 0.0229 acc: 83.51\n",
      "ep:19500 loss: 0.0229 acc: 83.51\n",
      "ep:19600 loss: 0.0229 acc: 83.51\n",
      "ep:19700 loss: 0.0229 acc: 83.51\n",
      "ep:19800 loss: 0.0229 acc: 83.51\n",
      "ep:19900 loss: 0.0229 acc: 83.51\n",
      "hidden:5, acc:83.50515747070312 , epoch:19995\n",
      "ep:  100 loss: 0.6257 acc: 59.79\n",
      "ep:  200 loss: 0.4333 acc: 66.49\n",
      "ep:  300 loss: 0.3253 acc: 65.98\n",
      "ep:  400 loss: 0.2647 acc: 70.62\n",
      "ep:  500 loss: 0.2189 acc: 69.07\n",
      "ep:  600 loss: 0.1752 acc: 74.23\n",
      "ep:  700 loss: 0.1410 acc: 74.74\n",
      "ep:  800 loss: 0.1167 acc: 74.74\n",
      "ep:  900 loss: 0.0997 acc: 74.74\n",
      "ep: 1000 loss: 0.0873 acc: 74.23\n",
      "ep: 1100 loss: 0.0772 acc: 74.23\n",
      "ep: 1200 loss: 0.0687 acc: 74.23\n",
      "ep: 1300 loss: 0.0613 acc: 77.84\n",
      "ep: 1400 loss: 0.0549 acc: 79.90\n",
      "ep: 1500 loss: 0.0492 acc: 80.41\n",
      "ep: 1600 loss: 0.0444 acc: 80.41\n",
      "ep: 1700 loss: 0.0402 acc: 81.44\n",
      "ep: 1800 loss: 0.0363 acc: 82.47\n",
      "ep: 1900 loss: 0.0323 acc: 83.51\n",
      "ep: 2000 loss: 0.0294 acc: 75.26\n",
      "ep: 2100 loss: 0.0259 acc: 75.77\n",
      "ep: 2200 loss: 0.0216 acc: 91.75\n",
      "ep: 2300 loss: 0.0179 acc: 91.75\n",
      "ep: 2400 loss: 0.0151 acc: 91.75\n",
      "ep: 2500 loss: 0.0131 acc: 91.75\n",
      "ep: 2600 loss: 0.0115 acc: 91.75\n",
      "ep: 2700 loss: 0.0103 acc: 91.75\n",
      "ep: 2800 loss: 0.0093 acc: 91.75\n",
      "ep: 2900 loss: 0.0096 acc: 91.75\n",
      "converged:6\n",
      "hidden:6, acc:100.0 , epoch:2916\n",
      "Done with task!, found optimal : 6\n"
     ]
    }
   ],
   "source": [
    "num_input = data.shape[1] - 1\n",
    "\n",
    "full_input  = data[:,0:num_input]\n",
    "full_target = data[:,num_input:num_input+1]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(full_input,full_target)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset,batch_size=97)\n",
    "\n",
    "epoch = 20000\n",
    "converge = False\n",
    "hid_layers = 1\n",
    "while not converge:\n",
    "    net = PolarNet(hid_layers)\n",
    "    if list(net.parameters()):\n",
    "        # initialize weight values\n",
    "        for m in list(net.parameters()):\n",
    "            m.data.normal_(0,.1)\n",
    "        # use Adam optimizer\n",
    "        optimizer = torch.optim.Adam(net.parameters(),eps=0.000001,lr=.01,\n",
    "                                     betas=(0.9,0.999),weight_decay=0.0001)\n",
    "        # training loop\n",
    "        for epoch in range(1, epoch):\n",
    "            accuracy = train(net, train_loader, optimizer)\n",
    "            if accuracy == 100:\n",
    "                print(f\"converged:{hid_layers}\")\n",
    "                converge = True\n",
    "                break\n",
    "    print(f\"hidden:{hid_layers}, acc:{accuracy} , epoch:{epoch}\")\n",
    "    hid_layers += 1\n",
    "print(f\"Done with task!, found optimal : {hid_layers - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "star16 = torch.Tensor(\n",
    "    [[1,1,0,0,0,0,0,0],\n",
    "     [0,1,1,0,0,0,0,0],\n",
    "     [0,0,1,1,0,0,0,0],\n",
    "     [0,0,0,1,1,0,0,0],\n",
    "     [0,0,0,0,1,1,0,0],\n",
    "     [0,0,0,0,0,1,1,0],\n",
    "     [0,0,0,0,0,0,1,1],\n",
    "     [1,0,0,0,0,0,0,1],\n",
    "     [1,1,0,0,0,0,0,1],\n",
    "     [1,1,1,0,0,0,0,0],\n",
    "     [0,1,1,1,0,0,0,0],\n",
    "     [0,0,1,1,1,0,0,0],\n",
    "     [0,0,0,1,1,1,0,0],\n",
    "     [0,0,0,0,1,1,1,0],\n",
    "     [0,0,0,0,0,1,1,1],\n",
    "     [1,0,0,0,0,0,1,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = star16\n",
    "num_in  = target.size()[0]\n",
    "num_out = target.size()[1]\n",
    "\n",
    "# input is one-hot with same number of rows as target\n",
    "input = torch.eye(num_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star16.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart18 =  torch.Tensor([\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,1,0,0,0,0,0,1,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "    [1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = heart18\n",
    "num_in  = target.size()[0]\n",
    "num_out = target.size()[1]\n",
    "\n",
    "# input is one-hot with same number of rows as target\n",
    "input = torch.eye(num_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring different things on part\n",
    "# define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "# fetch and load training data\n",
    "trainset = datasets.KMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # fetch and load test data\n",
    "testset = datasets.KMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NetLin(nn.Module):\n",
    "    # linear function followed by log_softmax\n",
    "    def __init__(self):\n",
    "        super(NetLin, self).__init__()\n",
    "        input_size = 784 # 28x28 = 784 x 1\n",
    "        self.linear = nn.Linear(input_size,10)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1) #flatten image into 784x1\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "        \n",
    "\n",
    "\n",
    "class NetFull(nn.Module):\n",
    "    # two fully connected tanh layers followed by log softmax\n",
    "    #10 -- Acc = 66%\n",
    "    #40 -- Acc = 80%\n",
    "    #60 -- Acc = 82%\n",
    "    #70 -- Acc = 83%\n",
    "    #80 -- Acc = 84%\n",
    "    #90 -- Acc = 84%\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Netfull\"\n",
    "    def __init__(self):\n",
    "        super(NetFull, self).__init__()\n",
    "        self.input_layer = 784\n",
    "        self.hid_nodes = 2000\n",
    "        self.linear1 = nn.Linear(self.input_layer,self.hid_nodes) #10\n",
    "        self.linear2 = nn.Linear(self.hid_nodes,10) #Try multiples of 10 for the hidden layer\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)#flatten image into 784x1\n",
    "        x = torch.tanh(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return torch.log_softmax(x,dim=1)\n",
    "\n",
    "class NetConv(nn.Module):\n",
    "    # two convolutional layers and one fully connected layer,\n",
    "    # all using relu, followed by log_softmax\n",
    "    def __str__(self):\n",
    "        return \"NetConv\"\n",
    "    def __init__(self):\n",
    "        super(NetConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320) #flatten the image \n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x,dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from kuzu import NetLin, NetFull, NetConv\n",
    "    \n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    print(\"model:\",model)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    conf_matrix = np.zeros((10,10)) # initialize confusion matrix\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # determine index with maximal log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            # update confusion matrix\n",
    "            conf_matrix = conf_matrix + metrics.confusion_matrix(\n",
    "                          target.cpu(),pred.cpu(),labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "        # print confusion matrix\n",
    "        np.set_printoptions(precision=4, suppress=True)\n",
    "        print(type(conf_matrix))\n",
    "        print(conf_matrix)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import arange\n",
    "inits = [j for j in arange(.1,1,.1)]\n",
    "inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# fetch and load training data\n",
    "trainset = datasets.KMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "\n",
    "# fetch and load test data\n",
    "testset = datasets.KMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# choose network architecture\n",
    "# if args.net == 'lin':\n",
    "#     net = NetLin().to(device)\n",
    "# elif args.net == 'full':\n",
    "#     print(\"starting with full network!\")\n",
    "#     net = NetFull().to(device)\n",
    "# else:\n",
    "net = NetConv().to(device)\n",
    "for init in inits:\n",
    "    if list(net.parameters()):\n",
    "        # use SGD optimizer\n",
    "        optimizer = optim.SGD(net.parameters(), lr=.1, momentum=.1)\n",
    "        # training and testing loop\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train(args, net, device, train_loader, optimizer, epoch)\n",
    "            test(args, net, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
